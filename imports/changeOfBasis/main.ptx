<?xml version="1.0" encoding="utf-8"?>

<pretext xml:lang="en-US" xmlns:xi="http://www.w3.org/2001/XInclude">
  <!-- we first include a file which contains the docinfo element: -->
  <xi:include href="./docinfo.ptx" />

<book xml:id="a0000000001" >
    <title>Change of basis</title>

    <!-- Include frontmatter -->
    <!-- <xi:include href="./frontmatter.ptx" /> -->

    <abstract>
   Determine how the matrix representation depends on a choice of basis. 
</abstract>
<frontmatter>
<titlepage>
<title>Change of basis</title>
<author>Crichton Ogle</author>
</titlepage>
</frontmatter>
<p>
  Suppose that <m>V</m> is an <m>n</m>-dimensional vector space equipped with two bases <m>S_1 = \{ {\bf v}_1,{\bf v}_2,\dots ,{\bf v}_n\} </m> and <m>S_2 = \{ {\bf w}_1, {\bf w}_2,\dots ,{\bf w}_n\} </m> (as indicated above, any two bases for <m>V</m> must have the same number of elements). Taking <m>L=Id</m>, Theorem ?? yields the equation 
</p>
<men id="eqn:basechange">
    egin{equation} \label{eqn:basechange} {}_{S_2}({\bf v}) = {}_{S_2}(Id*{\bf v}) = {}_{S_2}Id_{S_1}*{}_{S_1}{\bf v} \end{equatio
</men>
<p>
   where 
</p>
<men id="eqn:basechangematrix">
    egin{equation} \label{eqn:basechangematrix} {}_{S_2}Id_{S_1} = [{}_{S_2}{\bf v}_1\  {}_{S_2}{\bf v}_2\  \dots \  {}_{S_2}{\bf v}_n] \end{equatio
</men>
<p>
   The matrix <m>{}_{S_2}Id_{S_1}</m> is referred to as a <!-- \it --><em>base transition matrix</em>, and written as <m>{}_{S_2}T_{S_1}</m>. In words, equations (??) and (??) tells us that <!-- \it --><em>in order to compute the coordinate vector <m>{}_{S_2}{\bf v}</m> from <m>{}_{S_1}{\bf v}</m>, we multiply <m>{}_{S_1}{\bf v}</m> on the left by the <m>n\times n</m> matrix whose <m>i^{th}</m> column is the coordinate vector of <m>{\bf v}_i</m> with respect to the basis <m>S_2</m></em>. 
</p>
<p>
   Suppose <m>S_i,1\le i\le 3</m> are three bases for <m>V</m>. Then one has the following equalities 
</p>
<ol>
  <li><p>
  <m>{}_{S_3}T_{S_1} = {}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}</m> 
</p>
</li>
  <li><p>
  <m>{}_{S_i}T_{S_i} = Id</m> 
</p>
</li>
  <li><p>
  <m>{}_{S_i}T_{S_j} = \left({}_{S_j}T_{S_i}\right)^{-1}</m> 
</p>
</li>
</ol>

<p>
  <proof>
  
</proof> To show that (a) is true, it is enough to observe that for any coordinate vector <m>{}_{S_1}{\bf v}</m> there is an equality 
</p>
<me>
    {}_{S_3}T_{S_1}*{}_{S_1}{\bf v} = {}_{S_3}{\bf v} = {}_{S_3}T_{S_2}*({}_{S_2}{\bf v}) = {}_{S_3}T_{S_2}*\left({}_{S_2}T_{S_1}*{}_{S_1}{\bf v}\right) = \left({}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}\right)*{}_{S_1}{\bf v}  
</me>
<p>
   which implies the equality of matrices <m>{}_{S_3}T_{S_1}*{}_{S_1} = {}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}</m>. The second statement is clear; <m>{}_{S_i}T_{S_i}</m> is the <m>n\times n</m> matrix for which left multiplication by it leaves each coordinate vector <m>{}_{S_1}{\bf v}</m> unchanged, and the only matrix satisfying this property is the <m>n\times n</m> identity matrix. Finally (c) follows from (a) and (b) when <m>S_3 = S_1</m>. <proof>
  
</proof> 
</p>
<p>
  For vector spaces other than <m>\mathbb R^n</m> (such as the function space <m>F[a,b]</m> we looked at earlier) where the vectors do not naturally look like column vectors, we always use the above notation when working with their coordinate representations. However, in the case of <m>\mathbb R^n</m>, the vectors were <!-- \it --><em>defined</em> as column vectors even before discussing coordinate representations. So what should we do here? The answer is that the vectors in <m>\mathbb R^n</m> are, by convention, identified with their coordinate representations in the <!-- \it --><em>standard basis</em> <m>{\bf e} = \{ {\bf e}_1,\dots ,{\bf e}_n\} </m> for <m>\mathbb R^n</m>. So, for example, in <m>\mathbb R^3</m> when we wrote <m>{\bf v} = \begin{bmatrix} \end{bmatrix} 2\\ 3\\ -1\end{bmatrix}</m> what we really meant was that <m>\bf v</m> is the vector in <m>\mathbb R^3</m> with coordinate representation in the standard basis given by <m>{}_{\bf e}{\bf v} = \begin{bmatrix} \end{bmatrix} 2\\ 3\\ -1\end{bmatrix}</m>. Because it is very important to keep track of bases whenever determining base transition matrices and computing new coordinate representations, when doing so we will <!-- \it --><em>always</em> use base-subscript notation when working with coordinate vectors, even when the vectors are in <m>\mathbb R^n</m> and are being represented in the standard basis. 
</p>
<p>
   Suppose <m>S = \{ {\bf u}_1, {\bf u}_2, {\bf u}_3\} </m> are three vectors in <m>\mathbb R^3</m> with 
</p>
<me>
    {}_{\bf e}{\bf u}_1 = \begin{bmatrix} \end{bmatrix} 1\\ 0\\ 2\end{bmatrix},\quad {}_{\bf e}{\bf u}_2 = \begin{bmatrix} \end{bmatrix} -2\\ 3\\ 1\end{bmatrix},\quad {}_{\bf e}{\bf u}_3 = \begin{bmatrix} \end{bmatrix} 0\\ 1\\ -1\end{bmatrix}  
</me>
<p>
   Suppose we want to 
</p>
<ul>
  <li><p>
  Show that the set of vectors <m>S</m> is a basis for <m>\mathbb R^3</m>, 
</p>
</li>
  <li><p>
  compute the base transition matrix <m>{}_{S}T_{\bf e}</m>, 
</p>
</li>
  <li><p>
  for <m>\bf v</m> in <m>\mathbb R^3</m> with <m>{}_{\bf e}{\bf v} = \begin{bmatrix} \end{bmatrix} 2\\ 3\\ -1\end{bmatrix}</m>, compute the coordinate representation of <m>\bf v</m> with repsect to the basis <m>S</m>. 
</p>
</li>
</ul>
<p>
   To perform step 1, since <m>S</m> has the right number of vectors to be a basis for <m>\mathbb R^3</m>, it suffices to show the vectors are linearly independent. And we know how to do this; we form the matrix <m>A = [{}_{\bf e}{\bf u}_1\  {}_{\bf e}{\bf u}_2\  {}_{\bf e}{\bf u}_3]</m> and show that the columns are linearly independent by showing <m>rref(A) = Id^{3\times 3}</m> (exercise: do this, using MATLAB or Octave). This verifies <m>S</m> is a basis. Next, we look at the matrix <m>A</m>. The columns of <m>A</m> are the coordinate representations of the vectors in <m>S</m> with respect to the standard basis <m>\bf e</m>. But <m>S</m> is a basis. So the matrix <m>A</m> identifies as a base-transition matrix. We know it must be either <m>{}_S T_{\bf e}</m> or <m>{}_{\bf e}T_S</m>. But which one? This is where the notation being used helps us. The coordinate vectors of the columns of <m>A</m> have “<m>\bf e</m>" in the lower left. The rule is that this must match what appears in the notation of the transition matrix. So the only possibility is 
</p>
<me>
    {}_{\bf e}T_S = A  
</me>
<p>
   To complete the second step, we then compute 
</p>
<me>
    {}_S T_{\bf e} = \left({}_{\bf e} T_S\right)^{-1}  
</me>
<p>
   Finally, we can use this to compute <m>{}_S {\bf v}</m> as 
</p>
<me>
    {}_S {\bf v} = {}_S T_{\bf e}* {}_{\bf e}{\bf v}  
</me>

<p>
  The following theorem combines base-transition in both the domain and range, together with matrix representations of linear transformations. It amounts to a “base-transition" for matrix representations of linear transformations. 
</p>
<p>
   If <m>S_1,S_1'</m> are two bases for <m>V</m>, <m>S_2,S_2'</m> two bases for <m>W</m>, and <m>L:V\to W</m> a linear transformation from <m>V</m> to <m>W</m>, then 
</p>
<me>
    {}_{S_2'}L_{S_1'} = {}_{S_2'}T_{S_2}*{}_{S_2}L_{S_1}*{}_{S_1}T_{S_1'}  
</me>

<p>
   Let <m>S_1,S_2</m> be two bases for <m>V</m>, and <m>L:V\to V</m> a linear transformation from <m>V</m> to itself. We can consider The representations <m>{}_{S_1}L_{S_1}</m> and <m>{}_{S_2}L_{S_2}</m> of <m>L</m> with respect to the bases <m>S_1</m> and <m>S_2</m>. Using the above identities, show that 
</p>
<me>
    {}_{S_1}L_{S_1} = A*{}_{S_2}L_{S_2}*A^{-1}  
</me>
<p>
   where <m>A = {}_{S_1}T_{S_2}</m> (hint: apply the above theorem.)  
</p>
<p>
  Note: Square matrices <m>B,C</m> which satisfy the equality <m>B = A*C*A^{-1}</m> are called <!-- \it --><em>similar</em>. This is an important relation between square matrices, and plays a prominent role in the theory of eigenvalues and eigenvectors as we will see later on. 
</p>



    <!-- Include backmatter -->
   <!-- <xi:include href="./backmatter.ptx" /> -->

  </book>
</pretext>