<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-Isomorphic-Vector-Spaces" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Isomorphic Vector Spaces</title>



 




<p>
A vector space is defined as a collection of objects together with operations of addition and scalar multiplication that follow certain rules (<xref ref="def-vectorspacegeneral"/>).  In our study of abstract vector spaces, we have encountered spaces that appeared very different from each other.  Just how different are they?  Does <m>\mathbb{L}</m>, a vector space whose elements have the form <m>mx+b</m>, have anything in common with <m>\R^2</m>?  Is <m>\mathbb{P}^3</m> fundamentally different from <m>\mathbb{M}_{2,2}</m>?
</p> 

<p>
To answer these questions, we will have to look beyond the superficial appearance of the elements of a vector space and delve into its structure.  The ``structure" of a vector space is determined by how the elements of the vector space interact with each other through the operations of addition and scalar multiplication.  
</<p>
    

<p>
Let us return to the question of what <m>\mathbb{L}</m> has in common with <m>\R^2</m>.  Consider two typical elements of <m>\mathbb{L}</m>:
<men xml:id="eq-iso1">
    mx+b\quad\text{and}\quad qx+c.
</men>

We can add these elements together
<men xml:id="eq-iso2">
(mx+b)+(qx+c)=(m+q)x+(b+c)
</men>

or multiply each one by a scalar
<men xml:id="eq-iso3">
k(mx+b)=kmx+kb\quad\text{and}\quad t(qx+c)=tqx+tc.
</men>

But suppose we get tired of having to write  <m>x</m> down every time.  Could we leave off the <m>x</m> and represent <m>mx+b</m> by  <m>\begin{bmatrix}m\\b\end{bmatrix}</m>?  If we do this, expressions (<xref ref="eq-iso1"/>), (<xref ref="eq-iso2"/>) and (<xref ref="eq-iso3"/>) would be mimicked by the following expressions involving vectors of <m>\R^2</m>:

<me>
    \begin{bmatrix}m\\b\end{bmatrix}\quad\text{and}\quad\begin{bmatrix}q\\c\end{bmatrix},
</me>


<me>
    \begin{bmatrix}m\\b\end{bmatrix}+\begin{bmatrix}q\\c\end{bmatrix}=\begin{bmatrix}m+q\\b+c\end{bmatrix},
</me>


<me>
    k\begin{bmatrix}m\\b\end{bmatrix}=\begin{bmatrix}km\\kb\end{bmatrix}\quad\text{and}\quad t\begin{bmatrix}q\\c\end{bmatrix}=\begin{bmatrix}tq\\tc\end{bmatrix}.
</me>

It appears that we should be able to switch back and forth between <m>\mathbb{L}</m> and <m>\R^2</m>, translating questions and answers from one space to the other and back again.  
</p>

 <image width="65%">
   <shortdescription>Picture of isomorphism</shortdescription>
    <latex-image>
      \begin{tikzpicture} 
   \fill[blue, opacity=0.3] (0,0) rectangle (5,5);
   \fill[pink, opacity=0.5] (6,0) rectangle (11,5);
   
   \node[] at (0.5, 4.5)  (p2)    {\(\mathbb{L}\)};
   \node[] at (10.5, 4.5)  (r3)    {\(\R^2\)};
   
   \node[] at (2, 3)  (p_1)    {\((mx+b)+(qx+c)\)};
   \node[] at (2, 1)  (p_4)    {\((m+q)x+(b+c)\)};
   \node[] at (2, 2)  (eq1)    {{\rotatebox{90}{\(=\)}}};
   \node[] at (8.5, 2)  (eq2)    {{\rotatebox{90}{\(=\)}}};
     
     \node[] at (8.5, 3)  (v_1)    {\(\begin{bmatrix}m\\b\end{bmatrix}+\begin{bmatrix}q\\c\end{bmatrix}\)};
  \node[] at (8.5, 1.2)  (v_4)    {\(\begin{bmatrix}m+q\\b+c\end{bmatrix}\)};
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (3.6, 3)to[out=10, in=170](7.4,3);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_4.east)to[out=10, in=170](v_4.west);
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (7.4,2.9)to[out=190, in=350](3.6, 2.9);
     \draw [-\gt ,line width=0.5pt,-stealth]  (7.7,1.1)to[out=190, in=350](3.6,0.9 );
     
      \end{tikzpicture}
    </latex-image>
</image>


<p>
We begin to suspect that <m>\mathbb{L}</m> and <m>\R^2</m> have the same ``structure".  Spaces such as <m>\mathbb{L}</m> and <m>\R^2</m> are said to be <term>isomorphic</term>.  This term is derived from the Greek ``iso," meaning ``same," and ``morphe," meaning ``form."  The term captures the idea that isomorphic vector spaces have the same structure.    Before we present a precise definition of the term, we need to better understand what we mean by  ``switching back and forth" between spaces.  The following Exploration will help us formulate this vague notion in terms of transformations.
</p>


<exploration xml:id="init-isomorph">
    <p>
        Recall that the set of all polynomials of degree <m>2</m> or less, together with polynomial addition and scalar multiplication, is a vector space, denoted by <m>\mathbb{P}^2</m>.  Let <m>\mathcal{B}=\{1, x, x^2\}</m>. You should do a quick mental check that <m>\mathcal{B}</m> is a basis of <m>\mathbb{P}^2</m>.

Define a transformation <m>T:\mathbb{P}^2\rightarrow \R^3</m> by
<me>
T(a+bx+cx^2)=\begin{bmatrix}a\\b\\c\end{bmatrix}.
</me>

You may have recognized <m>T</m> as the transformation that maps each element of <m>\mathbb{P}^2</m> to its coordinate vector with respect to the ordered basis <m>\mathcal{B}</m>.  

Transformation <m>T</m> has several nice properties:
  <ol>
      <li>
      <p> By <xref ref="th-coordvectmappinglinear"/>, <m>T</m> is linear. 
    </p>
</li>
      <li>
      <p> It is easy to verify that <m>T</m> is one-to-one and onto (see <xref ref="prob-Tonetooneonto"/>.)
    </p>
</li>
      <li>
      <p> By <xref ref="th-isomeansinvert"/>, <m>T</m> has an inverse. 
      </p>
</li>
  </ol>
  
  Our goal is to investigate and illustrate what these properties mean for transformation <m>T</m>, and for the relationship between <m>\mathbb{P}^2</m> and <m>\R^3</m>.  
  
  First, observe that <m>T</m> being one-to-one and onto establishes ``pairings" between elements of <m>\mathbb{P}^2</m> and <m>\R^3</m> in such a way that every element of one vector space is uniquely matched with exactly one element of the other vector space, as shown in the diagram below.
</p> 



 <image width="65%">
   <shortdescription>The above isomorphism elaborated</shortdescription>
    <latex-image>
      \begin{tikzpicture} 
   \fill[blue, opacity=0.3] (0,0) rectangle (5,5);
   \fill[pink, opacity=0.5] (6,0) rectangle (11,5);
   
   \node[] at (0.5, 4.5)  (p2)    {\(\mathbb{P}^2\)};
   \node[] at (10.5, 4.5)  (r3)    {\(\R^3\)};
   
   \node[] at (2.5, 4.2)  (p_1)    {\(1+2x-3x^2\)};
   \node[] at (1.2, 2.8)  (p_2)    {\(-4\)};
    \node[] at (3, 1.8)  (p_3)    {\(x+5\)};
     \node[] at (2, 0.6)  (p_4)    {\(a+bx+cx^2\)};
     
     \node[] at (8.5, 4.2)  (v_1)    {\(\begin{bmatrix}1\\2\\-3\end{bmatrix}\)};
   \node[] at (7.2, 3.2)  (v_2)    {\(\begin{bmatrix}-4\\0\\0\end{bmatrix}\)};
    \node[] at (9, 1.9)  (v_3)    {\(\begin{bmatrix}5\\1\\0\end{bmatrix}\)};
     \node[] at (7.5, 0.8)  (v_4)    {\(\begin{bmatrix}a\\b\\c\end{bmatrix}\)};
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (3.6, 4.2)to[out=10, in=170](7.9,4.2);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_2.east)to[out=10, in=170](v_2.west);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_3.east)to[out=10, in=170](v_3.west);
     \draw [-\gt ,line width=0.5pt,-stealth]  (p_4.east)to[out=10, in=170](v_4.west);
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (7.9,4.1)to[out=190, in=350](3.6, 4.1);
     \draw [-\gt ,line width=0.5pt,-stealth]  (6.5,3.1)to[out=190, in=350](1.6,2.7 );
     \draw [-\gt ,line width=0.5pt,-stealth]  (8.5,1.8)to[out=190, in=350](3.6,1.7 );
     \draw [-\gt ,line width=0.5pt,-stealth]  (7,0.7)to[out=190, in=350](3,0.5 );
     
     \draw [-\gt ,line width=0.5pt,-stealth]  (8.5,-0.1)to[out=200, in=340](2.5,-0.1 );
     \draw [-\gt ,line width=0.5pt,-stealth]  (2.5,5.1)to[out=20, in=160](8.5, 5.1);
     
     \node[] at (5.5, 6)    {\(T\)};
      \node[] at (5.5, -1)    {\(T^{-1}\)};
      \end{tikzpicture}
    </latex-image>
</image>

<p>
  Second, the fact that <m>T</m> (and <m>T^{-1}</m>) are linear will allow us to translate questions related to linear combinations in one of the vector spaces to equivalent questions in the other vector space, then translate answers back to the original vector space.  To make this statement concrete, consider the following problem:
</p> 

<p>
  Let 
<me>
    p_1(x)=3-x+2x^2\quad\text{and}\quad p_2(x)=-1+3x+x^2.
</me>
 
Find <m>p_1(x)+p_2(x)</m>.  
  
  The answer is, of course
  
<me>
    p_1(x)+p_2(x)=2+2x+3x^2.
</me>

  Easy.  But suppose for a moment that we did not know how to add polynomials, or that we found the process extremely difficult, or maybe instead of <m>\mathbb{P}^2</m> we had another vector space that we did not want to deal with. 
  
  It turns out that we can use <m>T</m> and <m>T^{-1}</m> to answer the addition question.  We will start by applying <m>T</m> to <m>p_1(x)</m> and <m>p_2(x)</m> separately:
  
<me>
    T(p_1(x))=\begin{bmatrix}3\\-1\\2\end{bmatrix},\quad T(p_2(x))=\begin{bmatrix}-1\\3\\1\end{bmatrix}.
</me>

  Next, we add the images of <m>p_1(x)</m> and <m>p_2(x)</m> in <m>\R^3</m>:
  
<me>
    \begin{bmatrix}3\\-1\\2\end{bmatrix}+\begin{bmatrix}-1\\3\\1\end{bmatrix}=\begin{bmatrix}2\\2\\3\end{bmatrix}.
</me>

  This maneuver allows us to avoid the addition question in <m>\mathbb{P}^2</m> and answer the question in <m>\R^3</m> instead.  We use <m>T^{-1}</m> to translate the answer back to <m>\mathbb{P}^2</m>:
  
<me>
    T^{-1}\left(\begin{bmatrix}2\\2\\3\end{bmatrix}\right)=2+2x+3x^2.
</me>

  All of this relies on linearity.  Here is a formal justification for the process.  Try to spot where linearity is used.
  <mdn>
  <mrow number="yes" xml:id="steplin1">    p_1(x)+p_2(x)\amp =(3-x+2x^2)+(-1+3x+x^2) </mrow>
    <mrow number="yes" xml:id="steplin2">     \amp =T^{-1}\left(\begin{bmatrix}3\\-1\\2\end{bmatrix}\right)+T^{-1}\left(\begin{bmatrix}-1\\3\\1\end{bmatrix}\right) </mrow>
        <mrow number="yes" xml:id="steplin3">   \amp =T^{-1}\left(\begin{bmatrix}3\\-1\\2\end{bmatrix}+\begin{bmatrix}-1\\3\\1\end{bmatrix}\right) </mrow>
            <mrow number="yes"  xml:id="steplin4"> \amp =T^{-1}\left(\begin{bmatrix}2\\2\\3\end{bmatrix}\right) </mrow>
                <mrow number="yes" xml:id="steplin5">  \amp =2+2x+3x^2 </mrow>
  </mdn>
</p>

<problem>
<statement>
<p>
    Which transition in the above calculation requires linearity?
</p>
</problem>

  <choices>
  <choice>
      <statement>
          <p> From <xref ref="steplin1"/> to <xref ref="steplin2"/>. </p>
      </statement>
  </choice>
    <choice correct="yes">
      <statement>
          <p> From <xref ref="steplin2"/> to <xref ref="steplin3"/>. </p>
      </statement>
  </choice>
    <choice>
      <statement>
          <p> From <xref ref="steplin3"/> to <xref ref="steplin4"/>. </p>
      </statement>
  </choice>
    <choice>
      <statement>
          <p> From <xref ref="steplin4"/> to <xref ref="steplin5"/>. </p>
      </statement>
  </choice>
</choices>
</problem> 
</exploration>



<p>
Invertible linear transformations, such as transformation <m>T</m> of <xref ref="init-isomorph"/>, are useful because they preserve the structure of interactions between elements as we move back and forth between two vector spaces, allowing us to answer questions about one vector space in a different vector space.  In particular, any question related to linear combinations can be addressed in this fashion. This includes questions concerning linear independence, span, basis and dimension. 
</p>

<definition xml:id="def-isomorphism">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be vector spaces.  If there exists an invertible linear transformation <m>T:V\rightarrow W</m> we say that <m>V</m> and <m>W</m> are <term>isomorphic</term> and write <m>V\cong W</m>.  The invertible linear transformation <m>T</m> is called an <term>isomorphism</term>.
        </p>
    </statement>
</definition>

<p>
It is worth pointing out that if <m>T:V\rightarrow W</m> is an isomorphism, then <m>T^{-1}:W\rightarrow V</m>, being linear and invertible, is also an isomorphism.
</p>


<example xml:id="ex-lisor2">
    <p>
        Our earlier discussion suggests that <m>\mathbb{L}\cong\R^2</m>.  We postpone the proof until <xref ref="ex-coordmapiso"/>.
    </p>
</example>

<example xml:id="ex-p2isor3">
    <p>
     <xref ref="init-isomorph"/> shows that <m>\mathbb{P}^2\cong\R^3</m>.
    </p>
</example>



<example xml:id="ex-isomorphexample1">
    <statement>
        <p>
            Show that <m>\mathbb{M}_{2,2}</m> and <m>\mathbb{P}^3</m> are isomorphic.
       </p>
    </statement>
    <answer>
        <p>
            We will start by finding a plausible candidate for an isomorphism.  Define <m>T:\mathbb{M}_{2,2}\rightarrow \mathbb{P}^3</m> by

<me>
    T\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a+bx+cx^2+dx^3.
</me>


We will first show that <m>T</m> is a linear transformation,  then verify that <m>T</m> is invertible.
<md>
    T\left(k\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)\amp =T\left(\begin{bmatrix}ka\amp kb\\kc\amp kd\end{bmatrix}\right)\\
    \amp =ka+kbx+kcx^2+kdx^3=k(a+bx+cx^2+dx^3)\\
    \amp =kT\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)
</md>

<md>
    T\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}+\begin{bmatrix}m\amp n\\p\amp q\end{bmatrix}\right)\amp =T\left(\begin{bmatrix}a+m\amp b+n\\c+p\amp d+q\end{bmatrix}\right)\\
    \amp =(a+m)+(b+n)x+(c+p)x^2+(d+q)x^3\\
    \amp =(a+bx+cx^2+dx^3)+(m+nx+px^2+qx^3)\\
    \amp =T\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)+T\left(\begin{bmatrix}m\amp n\\p\amp q\end{bmatrix}\right)
</md>
We can show that <m>T</m> is one-to-one and onto, and therefore has an inverse.  We can also observe directly that <m>T^{-1}</m> is given by 

<me>
    T^{-1}(a+bx+cx^2+dx^3)=\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}
</me>

We conclude that <m>T</m> is an isomorphism, and <m>\mathbb{M}_{2,2}\cong\mathbb{P}^3</m>.
       </p>
    </answer>
</example>
Isomorphism <m>T</m> in Example <xref ref="ex-isomorphexample1"/> establishes the fact that <m>\mathbb{M}_{2,2}\cong\mathbb{P}^3</m>. However, there is nothing special about <m>T</m>, as there are many other isomorphisms from <m>\mathbb{M}_{2,2}</m> to <m>\mathbb{P}^3</m>.  Just for fun, try to verify that each of the following is an isomorphism.

<men>\label{eq:justforfuniso1}\tau_1:\mathbb{M}_{2,2}\rightarrow\mathbb{P}^3</men>

<me>
    \tau_1\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a-bx-cx^2+dx^3
</me>


<men>\label{eq:justforfuniso2}\tau_2:\mathbb{M}_{2,2}\rightarrow\mathbb{P}^3</men>

<me>
    \tau_2\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a+(a+c)x+(b-c)x^2+dx^3
</me>








\subsubsection*{The Coordinate Vector Isomorphism}
In Exploration <xref ref="init-isomorph"/> we made good use of a transformation that maps every element of <m>\mathbb{P}^2</m> to its coordinate vector in <m>\R^3</m>.  We observed that this transformation is linear and invertible, therefore it is an isomorphism.  The following example generalizes this result.

<theorem xml:id="ex-coordmapiso">

    <statement>
        <p>
            Let <m>V</m> be an <m>n</m>-dimensional vector space, and let <m>\mathcal{B}</m> be an ordered basis for <m>V</m>.  Then  <m>T:V\rightarrow \R^n</m> given by <m>T(\mathbf{v})=[\mathbf{v}]_{\mathcal{B}}</m> is an isomorphism.
        </p>
    </statement>

<proof>
    <p>
We leave the proof of this result to the reader (see <xref ref="prob-verifyisomorphism"/>.)
    </p>
</proof>
</theorem>
</subsubsection>










<subsection xml:id="Subsection-Properties-of-Isomorphic-Vector-Spaces-and-Isomorphisms">
    <title>Properties of Isomorphic Vector Spaces and Isomorphisms</title>
</subsection>  In this section we will illustrate properties of isomorphisms with specific examples.  Formal proofs of properties will be presented in the next section.

<exploration xml:id="init-basestobasesiso">
    <p>
        In Exploration <xref ref="init-isomorph"/> we defined a transformation <m>T:\mathbb{P}^2\rightarrow \R^3</m> by
<m>T(a+bx+cx^2)=\begin{bmatrix}a\\b\\c\end{bmatrix}</m>. We later observed that  <m>T</m> is an isomorphism.  We will now examine the effect of <m>T</m> on two different bases of <m>\mathbb{P}^2</m>.

Let 
<m>\mathcal{B}_1=\{1, x, x^2\}</m> and <m>\mathcal{B}_2=\{x, 1+x, x+x^2\}</m>.  (Recall that <m>\mathcal{B}_2</m> is a basis of <m>\mathbb{P}^2</m> by Example <xref ref="ex-coordvectorinpolyvectspace2"/> of <url href="https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0060/main">Bases and Dimension of Abstract Vector Spaces</url>.)

First,

<me>
    T(\mathcal{B}_1)=\{T(1), T(x), T(x^2)\}=\left\{\begin{bmatrix}1\\0\\0\end{bmatrix}, \begin{bmatrix}0\\1\\0\end{bmatrix}, \begin{bmatrix}0\\0\\1\end{bmatrix}\right\}
</me>

Clearly, the images of the elements of <m>\mathcal{B}_1</m> form a basis of <m>\R^3</m>.

Now we consider <m>\mathcal{B}_2</m>.

<me>
    T(\mathcal{B}_2)=\{T(x), T(1+x), T(x+x^2)\}=\left\{\begin{bmatrix}0\\1\\0\end{bmatrix}, \begin{bmatrix}1\\1\\0\end{bmatrix}, \begin{bmatrix}0\\1\\1\end{bmatrix}\right\}
</me>

It is easy to verify that <m>\begin{bmatrix}0\\1\\0\end{bmatrix}, \begin{bmatrix}1\\1\\0\end{bmatrix}, \begin{bmatrix}0\\1\\1\end{bmatrix}</m> are linearly independent and span <m>\R^3</m>, therefore the images of the elements of <m>\mathcal{B}_2</m> from a basis of <m>\R^3</m>.
    </p>
</exploration>

We can try any number of bases of <m>\mathbb{P}^2</m> and we will find that the image of each basis of <m>\mathbb{P}^2</m> is a basis of <m>\R^3</m>.  In general, we have the following result:
<{pretext}>

    <statement>
        <p>
            An isomorphism maps a basis of the domain to a basis of the codomain. (We will state this result more formally as Theorem <xref ref="th-bijectionsbasis"/> in the next section.)
        </p>
    </statement>
</fact>

Isomorphisms preserve bases, but more generally, they preserve linear independence.  
<{pretext}>

    <statement>
        <p>
            If <m>T:V\rightarrow W</m> is an isomorphism, then the subset <m>\{\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n\}</m> of <m>V</m> is linearly independent if and only if <m>\{T(\mathbf{v_1}), T(\mathbf{v}_2),\ldots ,T(\mathbf{v}_n)\}</m> is linearly independent in <m>W</m>.  (We will state and prove this result as Theorem <xref ref="th-linindtolinindiso"/>.)
        </p>
    </statement>
</fact>

<example xml:id="ex-inverseimageoflinind">
    <statement>
        <p>
            Let <m>V</m> be a vector space, and let <m>\mathcal{B}=\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \mathbf{v}_4\}</m> be an ordered basis of <m>V</m>.  Let 

<me>
    \mathbf{w}_1=2\mathbf{v}_1+3\mathbf{v}_2-5\mathbf{v}_3-2\mathbf{v}_4
</me>


<me>
    \mathbf{w}_2=-\mathbf{v}_1+4\mathbf{v}_2-3\mathbf{v}_3+\mathbf{v}_4
</me>


<me>
    \mathbf{w}_3=-3\mathbf{v}_1-\mathbf{v}_2+4\mathbf{v}_3+3\mathbf{v}_4
</me>


Are <m>\mathbf{w}_1, \mathbf{w}_2, \mathbf{w}_3</m> linearly independent?
       </p>
    </statement>
    <answer>
        <p>
            We could approach this question head-on by considering the vector equation

<me>
    a\mathbf{w}_1+b\mathbf{w}_2+c\mathbf{w}_3=\mathbf{0}
</me>

to see if the only solution is the trivial one. (See Practice Problem <xref ref="prob-noiso"/>.)

Instead, we will use isomorphisms.  Observe that we do not know anything about <m>V</m> aside from the fact that it has four basis vectors.  Vectors <m>\mathbf{w}_1</m>, <m>\mathbf{w}_2</m>, <m>\mathbf{w}_3</m> are given in terms of these basis vectors.  This should give us an idea for constructing an isomorphism between <m>V</m> and <m>\R^4</m>.  Consider <m>T:V\rightarrow\R^4</m> such that <m>T(\mathbf{w})=[\mathbf{w}]_{\mathcal{B}}</m>.
Then 
<me>
    T(\mathbf{w}_1)=[\mathbf{w}_1]_{\mathcal{B}}=\begin{bmatrix}2\\3\\-5\\-2\end{bmatrix},\quad T(\mathbf{w}_2)=[\mathbf{w}_2]_{\mathcal{B}}=\begin{bmatrix}-1\\4\\-3\\1\end{bmatrix},\quad T(\mathbf{w}_3)=[\mathbf{w}_3]_{\mathcal{B}}=\begin{bmatrix}-3\\-1\\4\\3\end{bmatrix}
</me>

By Theorem <xref ref="ex-coordmapiso"/>, <m>T</m>   is an isomorphism.  This means that <m>\mathbf{w}_1</m>, <m>\mathbf{w}_2</m>, <m>\mathbf{w}_3</m> are linearly independent if and only if their coordinate vectors are linearly independent.  There are multiple ways of determining whether 

<me>
    \begin{bmatrix}2\\3\\-5\\-2\end{bmatrix}, \begin{bmatrix}-1\\4\\-3\\1\end{bmatrix}, \begin{bmatrix}-3\\-1\\4\\3\end{bmatrix}
</me>

are linearly independent.  One way is to find the reduced row echelon form of 

<me>
    \begin{bmatrix}2\amp -1\amp -3\\3\amp 4\amp -1\\-5\amp -3\amp 4\\-2\amp 1\amp 3\end{bmatrix}
</me>

The matrix reduces as follows:

<me>
    \begin{bmatrix}2\amp -1\amp -3\\3\amp 4\amp -1\\-5\amp -3\amp 4\\-2\amp 1\amp 3\end{bmatrix}\rightsquigarrow\begin{bmatrix}1\amp 0\amp -13/11\\0\amp 1\amp 7/11\\0\amp 0\amp 0\\0\amp 0\amp 0\end{bmatrix}
</me>

We see that the rank of the matrix is <m>2</m>.  We conclude that the column vectors are not linearly independent.   Thus, the vectors <m>\mathbf{w}_1</m>, <m>\mathbf{w}_2</m> and <m>\mathbf{w}_3</m> are not linearly independent.
       </p>
    </answer>
</example>



<subsection xml:id="Subsection-Proofs-of-Isomorphism-Properties">
    <title>Proofs of Isomorphism Properties</title>
</subsection>

Recall that a transformation <m>T</m> is <term>one-to-one</term> provided that 
<me>
    T(\mathbf{v}_1)=T(\mathbf{v}_2)
</me>
 implies that 
<me>
    \mathbf{v}_1=\mathbf{v}_2
</me>


 We will show that images of linearly independent vectors under one-to-one linear transformations are linearly independent.  

<theorem xml:id="th-onetoonelinind">

    <statement>
        <p>
            Let <m>T:V\rightarrow W</m> be a one-to-one linear transformation.  Suppose <m>\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}</m> is linearly independent in <m>V</m>.  Then <m>\{T(\mathbf{v}_1),\ldots,T(\mathbf{v}_n)\}</m> is linearly independent in <m>W</m>.
        </p>
    </statement>
</theorem>

<proof>
    <p>
Suppose <m>a_1, \ldots, a_n</m> satisfy
<mdn>\label{onlysolution}a_1T(\mathbf{v}_1)+\ldots +a_nT(\mathbf{v}_n)=\mathbf{0}</mdn>
We will show that for each <m>i</m>, we must have <m>a_i=0</m>.

By linearity, we have:
<md>\mathbf{0}\amp =a_1T(\mathbf{v}_1)+\ldots +a_nT(\mathbf{v}_n)\\
\amp =T(a_1\mathbf{v}_1)+\ldots +T(a_n\mathbf{v}_n)\\
\amp =T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n)
</md>
By Theorem <xref ref="th-zerotozero"/>, <m>T(\mathbf{0})=\mathbf{0}</m>.  Therefore,

<me>
    T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n)=T(\mathbf{0})
</me>


Because <m>T</m> is one-to-one, we conclude that 
<mdn>\label{onlytrivial}a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n=\mathbf{0}</mdn>


By assumption, <m>\{\mathbf{v}_1,\ldots,\mathbf{v}_n\}</m> is linearly independent.  Therefore <m>a_i=0</m> for <m>1\leq i\leq n</m>.  
    </p>
</proof>

Recall that a transformation <m>T</m> is <term>onto</term> provided that every vector of the codomain of <m>T</m> is the image of some vector in the domain of <m>T</m>.  

We will show that an onto linear transformation maps sets that span the domain to sets that span the codomain. 

<theorem xml:id="th-ontospan">

    <statement>
        <p>
            Let <m>T:V\rightarrow W</m> be an onto linear transformation.  Suppose <m>V=\mbox{span}(\mathbf{v}_1,\ldots ,\mathbf{v}_n)</m>.  Then <m>W=\mbox{span}(T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n))</m>.
        </p>
    </statement>
</theorem>
<proof>
    <p>
Suppose <m>\mathbf{w}</m> is an element of <m>W</m>. To show that <m>\{T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)\}</m> spans <m>W</m>, we will express <m>\mathbf{w}</m> as a linear combination of <m>T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)</m>.

Because <m>T</m> is onto, <m>\mathbf{w}=T(\mathbf{v})</m> for some <m>\mathbf{v}</m> in <m>V</m>.  But <m>V=\mbox{span}(\mathbf{v}_1,\ldots ,\mathbf{v}_n)</m>.  Therefore, <m>\mathbf{v}=a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n</m> for some scalar coefficients <m>a_1,\ldots ,a_n</m>.
By linearity, we have:
<md>
\mathbf{w}=T(\mathbf{v})\amp =T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n)\\
\amp =a_1T(\mathbf{v}_1)+\ldots +a_nT(\mathbf{v}_n)
</md>
Thus, <m>\mathbf{w}</m> is in the span of <m>T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)</m>.
    </p>
</proof>
 
We will now combine the results of Theorem <xref ref="th-onetoonelinind"/> and Theorem <xref ref="th-ontospan"/> to obtain a result about the effect of isomorphisms on a basis.

<theorem xml:id="th-bijectionsbasis">

    <statement>
        <p>
            Let <m>T:V\rightarrow W</m> be an isomorphism.  Suppose <m>\mathcal{B}_V=\{\mathbf{v}_1,\ldots ,\mathbf{v}_n\}</m> is a basis for <m>V</m>.  Then <m>\{T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)\}</m> is a basis for <m>W</m>.
        </p>
    </statement>
</theorem>
<proof>
    <p>
Left to the reader.  (See Practice Problem <xref ref="prob-bijectionsbasisproof"/>) 
    </p>
</proof>

<theorem xml:id="th-linindtolinindiso">

    <statement>
        <p>
            Suppose <m>T:V\rightarrow W</m> is an isomorphism, then the subset <m>\{\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n\}</m> of <m>V</m> is linearly independent if and only if <m>\{T(\mathbf{v_1}), T(\mathbf{v}_2),\ldots ,T(\mathbf{v}_n)\}</m> is linearly independent in <m>W</m>.
        </p>
    </statement>
</theorem> 
<proof>
    <p>
We have already proved one direction of this this ``if and only if" statement as Theorem <xref ref="th-onetoonelinind"/>.  To prove the other direction, suppose that <m>T(\mathbf{v_1}), T(\mathbf{v}_2),\ldots ,T(\mathbf{v}_n)</m> are linearly independent vectors in <m>W</m>.  We need to show that this implies that <m>\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n</m> are linearly independent in <m>V</m>.  Observe that if <m>T</m> is an isomorphism, then <m>T^{-1}:W\rightarrow V</m> is also an isomorphism.  Thus, by Theorem <xref ref="th-onetoonelinind"/>, <m>T^{-1}(T(\mathbf{v_1})), T^{-1}(T(\mathbf{v}_2)),\ldots ,T^{-1}(T(\mathbf{v}_n))</m> are linearly independent.  But this means that <m>\mathbf{v_1}, \mathbf{v}_2,\ldots ,\mathbf{v}_n</m> are linearly independent.
    </p>
</proof>

<theorem xml:id="th-isocompisiso">

    <statement>
        <p>
            Let <m>U</m>, <m>V</m> and <m>W</m> be vector spaces.  Suppose that <m>T_1:U\rightarrow V</m> and <m>T_2:V\rightarrow W</m> are isomorphisms.  Then <m>T_2\circ T_1:U\rightarrow W</m> is an isomorphism.
        </p>
    </statement>
</theorem>
<proof>
    <p>
The proof is left to the reader.  (See Practice Problem <xref ref="prob-isocompisisoproof"/>.)
    </p>
</proof>
 
%%%% Moved to Exercises %%%%%%%%%%%%%% 
%<{pretext}>

    <statement>
        <p>
            A linear transformation <m>T:V\rightarrow W</m> is one-to-one if and only if <m>\text{ker}(T)=\{\mathbf{0}\}</m>.
%
        </p>
    </statement>
</theorem>
%<proof>
    <p>
%First assume that <m>T</m> is one-to-one.  Suppose <m>\mathbf{v}</m> is in <m>\text{ker}(T)</m>.  Then <m>T(\mathbf{v})=\mathbf{0}=T(\mathbf{0})</m>.  But then <m>\mathbf{v}=\mathbf{0}</m>.

%Next, assume that <m>\text{ker}(T)=\{\mathbf{0}\}</m>.  To show that <m>T</m> is one-to-one, suppose <m>T(\mathbf{v}_1)=T(\mathbf{v}_2)</m>, but then
%<md>
%T(\mathbf{v}_1)-T(\mathbf{v}_2)\amp =\mathbf{0}\\
%T(\mathbf{v}_1-\mathbf{v}_2)\amp =\mathbf{0}
%</md>
%Since the kernel only contains the zero vector, we conclude that
%
<me>
    \mathbf{v}_1-\mathbf{v}_2=\mathbf{0}
</me>

%Therefore
%
<me>
    \mathbf{v}_1=\mathbf{v}_2
</me>

%    </p>
</proof>
 
<subsection xml:id="Subsection-Finite-dimensional-Vector-Spaces">
    <title>Finite-dimensional Vector Spaces</title>
</subsection>

<theorem xml:id="th-ndimspacesisorn">

    <statement>
        <p>
            Let <m>V</m> and <m>W</m> be finite-dimensional vector spaces. Then

<me>
    V\cong W\quad\text{if and only if}\quad \mbox{dim}(V)=\mbox{dim}(W)
</me>
        </p>
    </statement>
</theorem>
<proof>
    <p>
First, assume that <m>V\cong W</m>.  Then there exists an isomorphism <m>T:V\rightarrow W</m>.  Suppose <m>\mbox{dim}(V)=n</m> and let <m>\{\mathbf{v}_1,\mathbf{v}_2,\ldots ,\mathbf{v}_n\}</m> be a basis for <m>V</m>. By Theorem <xref ref="th-bijectionsbasis"/> <m>\{T(\mathbf{v}_1),\ldots ,T(\mathbf{v}_n)\}</m> is a basis for <m>W</m>. Therefore <m>\mbox{dim}(W)=n</m>.

Conversely, suppose <m>\mbox{dim}(V)=\mbox{dim}(W)=n</m>, and let <m>\mathcal{B}=\{\mathbf{v}_1,\mathbf{v}_2,\ldots ,\mathbf{v}_n\}</m>, <m>\mathcal{C}=\{\mathbf{w}_1,\mathbf{w}_2,\ldots ,\mathbf{w}_n\}</m> be bases for <m>V</m> and <m>W</m>, respectively.

Define a linear transformation <m>T:V\rightarrow W</m> by <m>T(\mathbf{v}_i)=\mathbf{w}_i</m> for <m>1\leq i\leq n</m>.  To show that <m>T</m> is an isomorphism, we need to prove that <m>T</m> is one-to-one and onto.

Suppose <m>T(\mathbf{u}_1)=T(\mathbf{u}_2)</m> for some vectors <m>\mathbf{u}_1</m>, <m>\mathbf{u}_2</m> in <m>V</m>.  We know that

<me>
    \mathbf{u}_1=a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n
</me>


<me>
    \mathbf{u}_2=b_1\mathbf{v}_1+\ldots +b_n\mathbf{v}_n
</me>

for some scalars <m>a_i</m>'s and <m>b_i</m>'s.  Thus,

<me>
    T(a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n)=T(b_1\mathbf{v}_1+\ldots +b_n\mathbf{v}_n)
</me>

By linearity of <m>T</m>,

<me>
    a_1\mathbf{w}_1+\ldots +a_n\mathbf{w}_n=b_1\mathbf{w}_1+\ldots +b_n\mathbf{w}_n
</me>


<me>
    (a_1-b_1)\mathbf{w}_1+\ldots +(a_n-b_n)\mathbf{w}_n=\mathbf{0}
</me>

But <m>\mathbf{w}_1,\mathbf{w}_2,\ldots ,\mathbf{w}_n</m> are linearly independent, so <m>a_i-b_i=0</m> for all <m>1\leq i\leq n</m>.  Therefore <m>a_i=b_i</m> for all <m>1\leq i\leq n</m>.  We conclude that <m>\mathbf{u}_1=\mathbf{u}_2</m>.

We now show that <m>T</m> is onto. Suppose that <m>\mathbf{w}</m> is an element of <m>W</m>.  Then <m>\mathbf{w}=c_1\mathbf{w}_1+\ldots +c_n\mathbf{w}_n</m> for some scalars <m>c_i</m>'s.  But then

<me>
    \mathbf{w}=c_1T(\mathbf{v}_1)+\ldots +c_nT(\mathbf{v}_n)=T(c_1\mathbf{v}_1+\ldots +c_n\mathbf{v}_n)
</me>

We conclude that <m>\mathbf{w}</m> is an image of an element of <m>V</m>, so <m>T</m> is onto.
    </p>
</proof>

From this theorem follows an important corollary that shows why we spent so much time trying to understand <m>\R^n</m> in this course.

<corollary xml:id="cor-ndimisotorn">

    <statement>
        <p>
            Every <m>n</m>-dimensional vector space is isomorphic to <m>\R^n</m>.
        </p>
    </statement>
</corollary>

<example xml:id="ex-planeisoplane">
    <p>
        The span of any two linearly independent vectors in <m>\R^3</m> is isomorphic to <m>\R^2</m>. \begin{center}
\tdplotsetmaincoords{70}{130}
<image width="100%">
   <shortdescription></shortdescription>
    <latex-image>
      \begin{tikzpicture}[scale=0.8]
	\draw[-\gt ](-2,0,0)--(3,0,0) ;
    \draw[-\gt ](0,-2,0)--(0,3,0) ;
    \draw[-\gt ](0,0,-2)--(0,0,5) ;
     \filldraw[blue, opacity=0.3] (2,3,0)--(0,2,2.5)--(-2,-3,0)--(0,-2,-2.5)--cycle;
     
    
    \draw[-\gt , line width=2pt,red, -stealth](0,0,0)--(2,3,0);
     \draw[-\gt , line width=2pt,blue, -stealth](0,0,0)--(0,2,2.5);
    
    \end{tikzpicture}
    </latex-image>
</image>\quad\quad
<image width="100%">
   <shortdescription></shortdescription>
    <latex-image>
      \begin{tikzpicture}[scale=0.5]

  \draw[\lt -\gt ] (-4,0)--(4,0);
  \draw[\lt -\gt ] (0,-4)--(0,4);
   \filldraw[blue, opacity=0.3] (-3,-3)--(-3,3)--(3,3)--(3,-3)--cycle;
     \end{tikzpicture}
    </latex-image>
</image>
\end{center}
    </p>
</example>

<example xml:id="ex-p2isor3b">
    <statement>
        <p>
            <m>\mathbb{P}^2\ncong \R^2</m>
       </p>
    </statement>
    <answer>
        <p>
            Recall that <m>\mbox{dim}(\mathbb{P}^2)=3</m>.  Since <m>\mbox{dim}(\mathbb{P}^2)=3\neq 2=\mbox{dim}(\R^2)</m>, we conclude that <m>\mathbb{P}^2</m> is not isomorphic to <m>\R^2</m>.
       </p>
    </answer>
</example>

<subsection xml:id="Subsection-Practice-Problems">
    <title>Practice Problems</title>
</subsection>

<exercise xml:id="prob-Tonetooneonto">
    <statement>
        <p>
            Prove that transformation <m>T</m> of Exploration <xref ref="init-isomorph"/> is one-to-one and onto.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-tauone">
    <statement>
        <p>
            Verify that 
<me>
    \tau_1:\mathbb{M}_{2,2}\rightarrow\mathbb{P}^3
</me>

given by

<me>
    \tau_1\left(\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\right)=a-bx-cx^2+dx^3
</me>

of Expression <xref ref="eq-justforfuniso1"/> is an isomorphism.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-noiso">
    <statement>
        <p>
            Do Example <xref ref="ex-inverseimageoflinind"/> without using isomorphisms.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-useisoshowlinind">
    <statement>
        <p>
            Let 

<me>
    \mathcal{S}=\left\{\begin{bmatrix}1\amp -3\\-2\amp 2\end{bmatrix}, \begin{bmatrix}4\amp -2\\1\amp 5\end{bmatrix}, \begin{bmatrix}5\amp 5\\8\amp 4\end{bmatrix}\right\}
</me>

Is <m>\mathcal{S}</m> linearly independent in <m>\mathbb{M}_{2,2}</m>?  \wordChoice{\choice[correct]{Yes}\choice{No}}
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-basism22iso">
    <statement>
        <p>
            Let 

<me>
    \mathcal{S}=\left\{\begin{bmatrix}1\amp 2\\3\amp 4\end{bmatrix}, \begin{bmatrix}5\amp 6\\7\amp 8\end{bmatrix}, \begin{bmatrix}9\amp 10\\11\amp 12\end{bmatrix}, \begin{bmatrix}13\amp 14\\15\amp 16\end{bmatrix}\right\}
</me>

Is <m>\mathcal{S}</m> a basis for <m>\mathbb{M}_{2,2}</m>? \wordChoice{\choice{Yes}\choice[correct]{No}}
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-bijectionsbasisproof">
    <statement>
        <p>
            Prove Theorem <xref ref="th-bijectionsbasis"/>.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-chooseisospace">
    <statement>
        <p>
            Let <m>V</m> be a vector space, and suppose <m>\mathcal{B}=\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \mathbf{v}_4, \mathbf{v}_5\}</m> is a basis for <m>V</m>.  What can we conclude about <m>V</m>?  Check ALL that apply.
<choices>
  <choice>
      <statement>
          <p> We cannot conclude anything about <m>V</m> because we don't know what <m>V</m> is. </p>
      </statement>
  </choice>
      <choice correct="yes">
      <statement>
          <p> <m>V\cong \R^5</m> </p>
      </statement>
  </choice>
      <choice>
      <statement>
          <p> <m>V\cong \mathbb{P </p>
      </statement>
  </choice>^5</m>}
      <choice>
      <statement>
          <p> <m>V\cong \R^4</m> </p>
      </statement>
  </choice>
      <choice correct="yes">
      <statement>
          <p> <m>V\cong \mathbb{P </p>
      </statement>
  </choice>^4</m>}
      <choice>
      <statement>
          <p> <m>V\cong \mathbb{M </p>
      </statement>
  </choice>_{5,5}</m>}
</choices>
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-pickisospaces">
    <statement>
        <p>
            Which of the followng statements are true? Check ALL that apply.
<choices>
  <choice>
      <statement>
          <p> <m>\mathbb{M </p>
      </statement>
  </choice>_{3,3}\cong \R^6</m>}
      <choice correct="yes">
      <statement>
          <p> <m>\mathbb{M </p>
      </statement>
  </choice>_{3,3}\cong \R^9</m>}
      <choice>
      <statement>
          <p> <m>\mathbb{M </p>
      </statement>
  </choice>_{4,4}\cong \mathbb{P}^{16}</m>}
      <choice correct="yes">
      <statement>
          <p> <m>\mathbb{M </p>
      </statement>
  </choice>_{4,4}\cong \mathbb{P}^{15}</m>}
      <choice correct="yes">
      <statement>
          <p> <m>\mathbb{L </p>
      </statement>
  </choice>\cong \R^2</m>}
</choices>
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-verifyisomorphism">
    <statement>
        <p>
            Verify that <m>T:V\rightarrow \R^n</m> of Theorem <xref ref="ex-coordmapiso"/> is an isomorphism.
\begin{hint}
You may find the proof of Theorem <xref ref="th-ndimspacesisorn"/> helpful.
\end{hint}
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-isocompisisoproof">
    <statement>
        <p>
            Prove that the composition of two isomorphisms is an isomorphism.  (Theorem <xref ref="th-isocompisiso"/>.)
        </p>
    </statement>
</exercise>

<exercise>
    <statement>
        <p>
            \label{prob:kerneliszero}
Prove that a linear transformation <m>T:V\rightarrow W</m> is one-to-one if and only if <m>\text{ker}(T)=\{\mathbf{0}\}</m>.
        </p>
    </statement>
</exercise>




</section>