<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-Abstract-Vector-Spaces" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Abstract Vector Spaces</title>



 



<p>
When we examined subspaces of <m>\R^n</m> we discussed <m>\R^n</m> as a vector space and introduced the notion of a subspace of <m>\R^n</m>.  
In this section we will consider sets other than <m>\R^n</m> that have two operations and satisfy the same properties as <m>\R^n</m>.  Such sets, together with the operations of addition and scalar multiplication, will also be called vector spaces.
</p> 


<subsection xml:id="Subsection-Properties-of-Vector-Spaces">
    <title>Properties of Vector Spaces</title>

<p>
Recall that <m>\R^n</m> is said to be a vector space because
<ul>
    <li>
      <p> <m>\R^n</m> is closed under vector addition, </p>
</li>
    <li>
      <p> <m>\R^n</m> is closed under scalar multiplication, </p>
</li>
</ul>
and satisfies the following properties:

  <ol>
  <li>
      <p> Commutative Property of Addition:
  <m>\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}.</m> </p>
</li>
  <li>
      <p> Associative Property of Addition:
  <m>(\mathbf{u}+\mathbf{v})+\mathbf{w}=\mathbf{u}+(\mathbf{v}+\mathbf{w}).</m> </p>
</li>
  <li>
      <p> Existence of Additive Identity:
  <m>\mathbf{u}+\mathbf{0}=\mathbf{u}.</m> </p>
</li>
  <li>
      <p> Existence of Additive Inverse:
  <m>\mathbf{u}+(-\mathbf{u})=\mathbf{0}.</m> </p>
</li>
  <li>
      <p> Distributive Property over Vector Addition:
  <m>k(\mathbf{u}+\mathbf{v})=k\mathbf{u}+k\mathbf{v}.</m> </p>
</li>
  <li>
      <p> Distributive Property over Scalar Addition:
  <m>(k+p)\mathbf{u}=k\mathbf{u}+p\mathbf{u}.</m> </p>
</li>
  <li>
      <p> Associative Property for Scalar Multiplication:
  <m>k(p\mathbf{u})=(kp)\mathbf{u}.</m> </p>
</li>
  <li>
      <p> Multiplication by <m>1</m>:
  <m>1\mathbf{u}=\mathbf{u}.</m> </p>
</li>
  </ol>
</p> 


<remark>
<statement>
<p>
      All scalars in this chapter are assumed to be real numbers.  Complex scalars are considered later.
</p>
</statement>
 </remark>

 <p>
In the next two examples we will explore two sets other than <m>\R^n</m> endowed with addition and scalar multiplication and satisfying the same properties.
 </p> 


<example xml:id="ex-setofmatricesvectorspace">
    <p>
        Let <m>\mathbb{M}_{m,n}</m> be the set of all <m>m\times n</m> matrices.  Matrix addition and scalar multiplication were defined in chapter <m>4</m>.

Observe that the sum of two <m>m\times n</m> matrices is also an <m>m\times n</m> matrix. Likewise, a scalar multiple of an <m>m\times n</m> matrix is an <m>m\times n</m> matrix.  Thus 
<ul>
    <li>
      <p> <m>\mathbb{M}_{m,n}</m> is closed under matrix addition; </p>
</li>
    <li>
      <p> <m>\mathbb{M}_{m,n}</m> is closed under scalar multiplication. </p>
</li>
</ul>

In addition, <xref ref="th-propertiesofaddition"/> and <xref ref="th-propertiesscalarmult"/> give us the following properties of matrix addition and scalar multiplication.  Note that these properties are analogous to the eight vector properties above.
<ol>
  <li>
      <p> Commutative Property of Addition:  <m>\quad A+B=B+A</m>. </p>
</li>
  <li>
      <p> Associative Property of Addition: <m>\quad (A+B)+C=A+(B+C)</m>. </p>
</li>
  <li>
      <p> Existence of Additive Identity:  <m>\quad A+O=A \ </m> where <m>O</m> is the <m>m \times n</m> zero matrix. </p>
</li>
  <li>
      <p> Existence of Additive Inverse:  <m>\quad A+(-A)=O</m>. </p>
</li>
  <li>
      <p> Distributive Property over Matrix Addition:  <m>\quad k(A+B)=kA+kB</m>. </p>
</li>
  <li>
      <p> Distributive Property over Scalar Addition:  <m>\quad (k+p)A=kA+pA</m>. </p>
</li>
  <li>
      <p> Associative Property for Scalar Multiplication: <m>\quad k(pA)=(kp)A</m>. </p>
</li>
  <li>
      <p> Multiplication by <m>1</m>: <m>\quad 1A=A</m>. </p>
</li>
  </ol>
    </p>
</example>

<example xml:id="ex-linfunctionsvectspace">
    <statement>
        <p>
            Consider the set <m>\mathbb{L}</m> of all linear functions.  This set includes all polynomials of degree <m>1</m> and degree <m>0</m>.  We will use addition and scalar multiplication of polynomials as the two operations, and show that <m>\mathbb{L}</m> is closed under those operations and satisfies eight properties analogous to those of vectors of <m>\R^n</m>.
       </p>
    </statement>
    <answer>
        <p>
            Elements of <m>\mathbb{L}</m> are functions <m>f</m> given by

<me>
    f(x)=mx+b.
</me>

(Note that <m>m</m> and <m>b</m> can be equal to zero.)
</p> 

<p>
Given <m>f_1</m> and <m>f_2</m> in <m>\mathbb{L}</m>, it is easy to verify that <m>f_1+f_2</m> is also in <m>\mathbb{L}</m>.  This gives us closure under function addition.

For any scalar <m>k</m>, we have

<me>
    kf(x)=k(mx+b)=(km)x+(kb).
</me>

Therefore <m>kf</m> is in <m>\mathbb{L}</m>, and <m>\mathbb{L}</m> is closed under scalar multiplication.

We now proceed to formulate eight properties analogous to those of vectors of <m>\R^n</m>.
</p> 

<p> 
Let <m>f_1</m>, <m>f_2</m> and <m>f_3</m> be elements of <m>\mathbb{L}</m> given by <m>f_1(x)=m_1 x + b_1</m>, <m>f_2(x)=m_2 x + b_2</m>, and <m>f_3(x)=m_3 x + b_3</m>. Let <m>k</m> and <m>p</m> be scalars.  
  <ol>
  <li>
      <p> Commutative Property of Addition:  

   <m>f_1+f_2=f_2+f_1.</m>
</p>

<p>
  This property holds because
  <md>
<mrow> f_1(x) + f_2(x) \amp = (m_1 x + b_1) + (m_2 x + b_2) </mrow> 
<mrow> \amp = (m_2 x + b_2) + (m_1 x + b_1) </mrow>
<mrow> \amp = f_2(x) + f_1(x).  </mrow>
  </md>
</p>
</li>

  <li>
      <p> Associative property of Addition:
  <me>(f_1 + f_2) + f_3 = f_1 + (f_2 + f_3).</me>
      </p>

      <p>
  This property is easy to verify and is left to the reader. </p>
</li>

  <li>
      <p> Existence of additive identity:
  <me>f_1 + f_0 = f_1</me>
      </p>
  
    <p>
  The additive identity <m>f_0</m> is given by <m>f_0(x)=0</m>.  Note that <m>f_0</m> is a vector in the space <m>\mathbb{L}</m>.
</p>
</li>

  <li>
      <p> Existence of additive inverse:
    <me>f_1 + (-f_1) = f_0.</me> 
      </p>
    
      <p>
The additive inverse of <m>f_1</m> is a function <m>-f_1</m> given by <m>-f_1(x)=-mx+(-b)</m>.    Note that <m>-f_1</m> is in <m>\mathbb{L}</m>.
    </p>
</li>

  <li>
      <p> Distributive Property over Vector Addition:
  <me>k(f_1+f_2)=kf_1+kf_2.</me>
      </p>

      <p>
  This property holds because
 <md>
  <mrow> k(f_1(x) + f_2(x)) \amp = k((m_1 x + b_1) + (m_2 x + b_2)) </mrow>
  <mrow>  \amp = k(m_1 x + b_1) + k(m_2 x + b_2) </mrow> 
  <mrow> \amp = k f_1(x) + k f_2(x). </mrow>
  </md>
</p>
</li>
  
  <li>
      <p> Distributive property over scalar addition: <me>(k+p)f_1=kf_1+pf_1.</me>  
      </p>

      <p>
  This property holds because
  <md>
 <mrow> (k+p)f_1(x)\amp = (k+p)(m_1 x + b_1) </mrow> 
<mrow>  \amp =k(m_1 x + b_1) + p(m_1 x + b_1) </mrow>
<mrow> \amp = k f_1(x) + p f_1(x). </mrow>
 </md>
     </p>
</li>
 
  <li>
      <p> Associative property for scalar multiplication: <m>(k(pf_1))=(kp)f_1.</m> 
      </p> 

  <p> 
  This property holds because
  <md>
<mrow>  k(p(f_1(x)))\amp =k(p(m_1 x + b_1)) </mrow>
<mrow> \amp =k(p m_1 x +p b_1)  </mrow>
<mrow>  \amp =kp m_1 x +kp b_1  </mrow>
<mrow> \amp = (kp) m_1 x + (kp) b_1 </mrow>
<mrow>  \amp = (kp)(m_1 x + b_1)  </mrow>
<mrow>  \amp =(kp)f_1(x). </mrow>
</md>
</p>
</li>

  <li>
      <p> Multiplication by <m>1</m> 
      </p>

    <p>
    This follows from 
<me>
    1 f_1=f_1.
</me>
</p>
</li>
  </ol>
</p>
    </answer>
</example>
</subsection> 















<subsection xml:id="Subsection-Definition-of-a-Vector-Space">
    <title>Definition of a Vector Space</title>

<p>
During <xref ref="ex-setofmatricesvectorspace"/> and <xref ref="ex-linfunctionsvectspace"/> show us that there are many times in mathematics when we encounter a set with two operations (that we call addition and scalar multiplication) such that the set is closed under the two operations, and satisfies the same eight properties as <m>\R^n</m>.  We will refer to such sets as <term>vector spaces</term>.
</p> 


  <definition xml:id="def-vectorspacegeneral">

    <statement>
        <p>
            Let <m>V</m> be a nonempty set.  Suppose that elements of <m>V</m> can be added together and multiplied by scalars.  The set <m>V</m>, together with operations of addition and scalar multiplication, is called a <term>vector space</term> provided that 
  <ul>
  <li>
      <p> <m>V</m> is closed under addition, </p>
</li>
  <li>
      <p> <m>V</m> is closed under scalar multiplication </p>
</li>
  </ul>
  and the following properties hold for <m>\mathbf{u}</m>, <m>\mathbf{v}</m> and <m>\mathbf{w}</m> in <m>V</m> and scalars <m>k</m> and <m>p</m>:
  
  <ol>
   <li xml:id="item-commaddvectspdef">
  <p> 
  Commutative Property of Addition:
  <m>\mathbf{u}+\mathbf{v}=\mathbf{v}+\mathbf{u}.</m> </p>
</li>

  <li xml:id="item-assaddvectspdef">
  <p>
  Associative Property of Addition:\quad
  <m>(\mathbf{u}+\mathbf{v})+\mathbf{w}=\mathbf{u}+(\mathbf{v}+\mathbf{w}).</m> </p>
</li>

  <li xml:id="item-idaddvectspdef">
  <p> 
  Existence of Additive Identity:
  <m>\mathbf{u}+\mathbf{0}=\mathbf{u}.</m> </p>

</li>
  <li xml:id="item-invaddvectspdef">
  <p>
  Existence of Additive Inverse:
  <m>\mathbf{u}+(-\mathbf{u})=\mathbf{0}.</m> </p>
</li>

  <li xml:id="item-distvectaddvectspdef">
  <p> 
  Distributive Property over Vector Addition:
  <m>k(\mathbf{u}+\mathbf{v})=k\mathbf{u}+k\mathbf{v}.</m> </p>
</li>
  <li xml:id="item-distscalaraddvectspdef">
  <p> 
  Distributive Property over Scalar Addition:
  <m>(k+p)\mathbf{u}=k\mathbf{u}+p\mathbf{u}.</m> </p>
</li>

  <li xml:id="item-assmultvectspdef">
  <p> 
  Associative Property for Scalar Multiplication:
  <m>k(p\mathbf{u})=(kp)\mathbf{u}.</m> </p>
</li>
  <li xml:id="item-idmultvectspdef">
  <p> 
  Multiplication by <m>1</m>:
  <m>1\mathbf{u}=\mathbf{u}.</m> </p>
</li>
  </ol>

We will refer to elements of <m>V</m> as <term>vectors</term>.
        </p>
    </statement>
</definition>

<p>
When scalars <m>k</m> and <m>p</m> in the above definition are restricted to real numbers, as they are in this chapter, vector space <m>V</m> may be referred to as  a <term>vector space over the real numbers</term>.
</p> 

<p>
We have already encountered two abstract vectors spaces before, viz.:
</p>


<example xml:id="ex-MLexamplesofvectspaces">
    <p>
        <m>\mathbb{M}_{m,n}</m> and <m>\mathbb{L}</m> are vector spaces (eee <xref ref="ex-setofmatricesvectorspace"/> and <xref ref="ex-linfunctionsvectspace"/>).
    </p>
</example>

<p>
Sets of polynomials provide an important source of examples, so we review some basic facts. A <term>polynomial with real coefficients</term> in <m>x</m> is an expression
<me>
p(x) = a_0 + a_1x + a_2x^2 + \ldots + a_nx^n
</me>
where <m>a_{0}, a_{1}, a_{2}, \ldots, a_{n}</m> are real numbers called the <term>coefficients</term> of the polynomial. 
</p>

<p>
If all the coefficients are zero, the polynomial is called the <term>zero polynomial</term> and is denoted simply as <m>0</m>. 
</p> 

<p> 
If <m>p(x) \neq 0</m>, the highest power of <m>x</m> with a nonzero coefficient is called the <term>degree</term> of <m>p(x)</m> denoted as <m>\mbox{deg}(p(x))</m>. The degree of the zero polynomial is not defined.
</p>

<p>
The coefficient itself is called the <term>leading coefficient</term> of <m>p(x)</m>. Hence <m>\mbox{deg}(3 + 5x) = 1</m>, <m>\mbox{deg}(1 + x + x^{2}) = 2</m>, and <m>\mbox{deg}(4) = 0</m>. 
</p> 


<p>
Let <m>\mathbb{P}</m> denote the set of all polynomials and suppose that
<md>
<mrow> p(x) \amp = a_0 + a_1x + a_2x^2 + \ldots </mrow>
<mrow> q(x) \amp = b_0 + b_1x + b_2x^2 + \ldots </mrow>
</md>
are two polynomials in <m>\mathbb{P}</m> (possibly of different degrees). Then <m>p(x)</m> and <m>q(x)</m> are called <term>equal</term> (written <m>p(x) = q(x)</m>) if and only if all the corresponding coefficients are equal---
that is, one has <m>a_{0} = b_{0}</m>, <m>a_{1} = b_{1}</m>, <m>a_{2} = b_{2}</m>, and so on. 
In particular, <m>a_{0} + a_{1}x + a_{2}x^{2} + \ldots = 0</m> means <m>a_{0} = 0</m>, <m>a_{1} = 0</m>, <m>a_{2} = 0</m>, <m>\ldots</m>. 
</p> 

<p>
The set <m>\mathbb{P}</m> has an addition and scalar multiplication defined on it as follows: if <m>p(x)</m> and <m>q(x)</m> are as before and <m>k</m> is a real number,
<md>
<mrow> p(x) + q(x) \amp = (a_0 + b_0) + (a_1 + b_1)x + (a_2 + b_2)x^2 + \ldots </mrow>
<mrow> kp(x) \amp = ka_0 + (ka_1)x + (ka_2)x^2 + \ldots </mrow>
</md> 
</p>


<p> 
A ton of terminology was just introduced. They are underlined in the example below.
</p> 

<example xml:id="ex-pisavectorspace">
    <statement>
        <p>
            <m>\mathbb{P}</m> is a vector space.
       </p>
    </statement>
    <answer>
        <p>
            It is easy to see that the sum of two polynomials is again a polynomial, and that a scalar multiple of a polynomial is a polynomial.  Thus, <m>\mathbb{P}</m> is closed under addition and scalar multiplication.  The other eight vector space properties are easily verified, and we conclude that <m>\mathbb{P}</m> is a vector space.
       </p>
    </answer>
</example>


<example xml:id="ex-deg2onlynotavecspace">
    <statement>
        <p>
            Let <m>Y</m> be the set of all degree two polynomials in <m>x</m>.  In other words,

<me>
    Y=\left \lbrace ax^2+bx+c : a \ne 0 \right \rbrace.
</me>

We claim that <m>Y</m> is not a vector space.
       </p>
    </statement>
    <answer>
        <p>
            Observe that <m>Y</m> is not closed under addition.  To see this, let <m>y_1 = 2x^2+3x+4</m> and let <m>y_2=-2x^2</m>.  Then <m>y_1</m> and <m>y_2</m> are both elements of <m>Y</m>.  However, <m>y_1+y_2 = 3x+4</m> is not an element of <m>Y</m>, as it is only a degree one polynomial.  We require the coefficient <m>a</m> of <m>x^2</m> to be nonzero for a polynomial to be in <m>Y</m>, and this is not the case for <m>y_1+y_2</m>.

As an exercise, check the remaining vector space properties one-by-one to see which properties hold and which do not.  
        </p>
    </answer> 
</example>

<p>
Set <m>Y</m> in <xref ref="ex-deg2onlynotavecspace"/> is not a vector space, but if we make a slight modification, we can make it into a vector space.  
</p>


<example xml:id="ex-deg-le-2vectorspace">
<p>
Let  <m>\mathbb{P}^2</m> be the set of polynomials of degree two or less.  In other words,

<me>
    \mathbb{P}^2=\left \lbrace ax^2+bx+c : a,b,c \in \mathbb{R} \right \rbrace.
</me>


Note that <m>\mathbb{P}^2</m> contains the zero polynomial (let <m>a=b=c=0</m>).  

Unlike set <m>Y</m> in <xref ref="ex-deg2onlynotavecspace"/>, <m>\mathbb{P}^2</m> is closed under polynomial addition and scalar multiplication.  It is easy to verify that all vector space properties hold, so <m>\mathbb{P}^2</m> is a vector space.
</p>
</example>

<example xml:id="ex-pnisavectorspace">
<p>
Let <m>n</m> be a natural number.  Define <m>\mathbb{P}^n</m> to be the set of polynomials of degree <m>n</m> or less than <m>n</m>, then by reasoning similar to <xref ref="ex-deg-le-2vectorspace"/>, <m>\mathbb{P}^n</m> is a vector space.
</p>
</example>
</subsection>











<subsection xml:id="Subsection-Subspaces">
    <title>Subspaces</title>

<definition xml:id="def-subspaceabstract">

    <statement>
        <p>
            A nonempty subset <m>U</m> of a vector space <m>V</m> is called a <term>subspace</term> of <m>V</m>, provided that <m>U</m> is itself a vector space when given the same addition and scalar multiplication as <m>V</m>.
        </p>
    </statement>
</definition>

<p> 
An example to showcase this is in order.
</p> 

<example xml:id="ex-subspaceabstract1">
<p>
In <xref ref="ex-deg-le-2vectorspace"/> we demonstrated that <m>\mathbb{P}^2</m> is a vector space.  From <xref ref="ex-pisavectorspace"/> we know that <m>\mathbb{P}</m> is a vector space. But <m>\mathbb{P}^2</m> is a subset of <m>\mathbb{P}</m>, and uses the same operations of polynomial addition and scalar multiplication.  Therefore <m>\mathbb{P}^2</m> is a subspace of <m>\mathbb{P}</m>.
</p>
</example>

<p>
Checking all ten properties to verify that a subset of a vector space is a subspace can be cumbersome.  Fortunately we have the following theorem.
</p> 

<theorem xml:id="th-subspacetestabstract">
    <title>Subspace Test</title>
    <statement>
        <p>
            Let <m>U</m> be a nonempty subset of a vector space <m>V</m>.  If <m>U</m> is closed under the operations of addition and scalar multiplication of <m>V</m>, then <m>U</m> is a subspace of <m>V</m>.
        </p>
    </statement>

<proof>
    <p>
To prove that closure is a sufficient condition for <m>U</m> to be a subspace, we will need to show that closure under addition and scalar multiplication of <m>V</m> guarantees that the remaining eight properties are satisfied automatically.  
    </p>

    <p>
Observe that <xref ref="item-commaddvectspdef"/>, <xref ref="item-assaddvectspdef"/>, <xref ref="item-distvectaddvectspdef"/>, <xref ref="item-distscalaraddvectspdef"/>, <xref ref="item-assmultvectspdef"/> and <xref ref="item-idmultvectspdef"/> hold for all elements of <m>V</m>.  Thus, these properties will hold for all elements of <m>U</m>.  We say that these properties are <term>inherited</term> from <m>V</m>.
    </p> 

    <p>
To prove <xref ref="item-idaddvectspdef"/> we need to show that <m>\mathbf{0}</m>, which we know to be an element of <m>V</m>, is contained in <m>U</m>.  Let <m>\mathbf{u}</m> be an element of <m>U</m> (recall that <m>U</m> is nonempty).  We will show that <m>0\mathbf{u}=\mathbf{0}</m> in <m>V</m>.  Then, by closure under scalar multiplication, we will be able to conclude that <m>0\mathbf{u}=\mathbf{0}</m> must be in <m>U</m>.

<me>
    0\mathbf{u}=(0+0)\mathbf{u}=0\mathbf{u}+0\mathbf{u}.
</me>

Adding the additive inverse of <m>0\mathbf{u}</m> to both sides gives us

<me>
    0\mathbf{u}+(-0\mathbf{u})=(0\mathbf{u}+0\mathbf{u})+(-0\mathbf{u}).
</me>

Thanks to <xref ref="item-assaddvectspdef"/> and <xref ref="item-invaddvectspdef"/>.


<me>
    \mathbf{0}=0\mathbf{u}+(0\mathbf{u}+(-0\mathbf{u})).
</me>

By <xref ref="item-idaddvectspdef"/> and <xref ref="item-invaddvectspdef"/>

<me>
    \mathbf{0}=0\mathbf{u}+\mathbf{0}=0\mathbf{u}.
</me>


Because <m>U</m> is closed under scalar multiplication <m>0\mathbf{u}=\mathbf{0}</m> is in <m>U</m>.

We know that every element of <m>U</m>, being an element of <m>V</m>, has an additive inverse  in <m>V</m>.  We need to show that the additive inverse of every element of <m>U</m> is contained in <m>U</m>. Let <m>\mathbf{u}</m> be any element of <m>U</m>.  We will show that <m>(-1)\mathbf{u}</m> is the additive inverse of <m>\mathbf{u}</m>.  Then by closure, <m>(-1)\mathbf{u}</m> will have to be contained in <m>U</m>.  To show that <m>(-1)\mathbf{u}</m> is the additive inverse of <m>\mathbf{u}</m>, we must show that <m>\mathbf{u}+(-1)\mathbf{u}=\mathbf{0}</m>.  We compute:

<me>
    \mathbf{u}+(-1)\mathbf{u}=1\mathbf{u}+(-1)\mathbf{u}=(1+(-1))\mathbf{u}=0\mathbf{u}=\mathbf{0}.
</me>

Thus <m>(-1)\mathbf{u}</m> is the additive inverse of <m>\mathbf{u}</m>. By closure, <m>(-1)\mathbf{u}</m> is in <m>U</m>.  
    </p>
</proof>
</theorem>

<p>
Let us activate the theorem in an example.
</p>


<example xml:id="ex-centralizerofA">
<p>
Let <m>A</m> be a fixed matrix in <m>\mathbb{M}_{n,n}</m>. Show that the set <m>C_A</m> of all <m>n\times n</m> matrices that commute with <m>A</m> under matrix multiplication  is a subspace of <m>\mathbb{M}_{n,n}</m>.  
</p> 

<answer>
<p>
The set <m>C_A</m> consists of all <m>n\times n</m> matrices <m>X</m> such that <m>AX=XA</m>.  First, observe that <m>C_A</m> is not empty because <m>I_n</m> is an element.  Now we need to show that <m>C_A</m> is closed under matrix addition and scalar multiplication.

Suppose that <m>X_1</m> and <m>X_{2}</m> lie in <m>C_A</m>.  Then <m>AX_1 = X_1A</m> and <m>AX_{2} = X_{2}A</m>. Then

<me>
    
A(X_1 + X_2) 	= AX_1 + AX_2 = X_1A + X_2A + (X_1 + X_2)A.
</me>

Therefore <m>(X_1+X_2)</m> commutes with <m>A</m>.  Thus <m>(X_1+X_2)</m> is in <m>C_A</m>.  We conclude that <m>C_A</m> is closed under matrix addition.

Now suppose <m>X</m> is in <m>C_A</m>.  Let <m>k</m> be a scalar, then

<me>
    
A(kX)= k(AX) = k(XA) = (kX)A.

</me>

Therefore <m>(kX)</m> commutes with <m>A</m>.  We conclude that <m>(kX)</m> is in <m>C_A</m>, and <m>C_A</m> is closed under scalar multiplication.
 Hence <m>C_A</m> is a subspace of <m>\mathbb{M}_{n,n}</m>.
       </p>
    </answer>
</example>

<remark>
<statement>
<p>
Suppose <m>p(x)</m> is a polynomial and <m>a</m> is a number. Then the number <m>p(a)</m> obtained by replacing <m>x</m> by <m>a</m> in the expression for <m>p(x)</m> is called the <term>evaluation</term> of <m>p(x)</m> at <m>a</m>. For example, if <m>p(x) = 5 - 6x + 2x^{2}</m>, then the evaluation of <m>p(x)</m> at <m>a = 2</m> is 
<me>
    p(2) = 5 - 12 + 8 = 1.
</me>

If <m>p(a) = 0</m>, the number <m>a</m> is called a <term>root</term> of <m>p(x)</m>.
</p>
</statement>
</remark> 



<p>
To get used to the new terminology, let us look at an example in the context of polynomials.
</p> 


<example xml:id="ex-root3">
    <statement>
        <p>
            Consider the set <m>U</m> of all polynomials in <m>\mathbb{P}</m> that have <m>3</m> as a root:
<me>
U = \lbrace p(x) \in \mathbb{P} : p(3) = 0 \rbrace.
</me>
Show that <m>U</m> is a subspace of <m>\mathbb{P}</m>.
       </p>
    </statement>

    <answer>
        <p>
            Observe that <m>U</m> is not empty because <m>r(x)=x-3</m> is an element of <m>U</m>.  Suppose <m>p(x)</m> and <m>q(x)</m> lie in <m>U</m>.  Then <m>p(3) = 0</m> and <m>q(3) = 0</m>. We have 
            <me>
            (p + q)(x) = p(x) + q(x)
            </me> 
            
            for all <m>x</m>, so 
            <me>
            (p + q)(3) = p(3) + q(3) = 0 + 0 = 0,
            </me>
            
            and <m>U</m> is closed under addition. The verification that <m>U</m> is closed under scalar multiplication is similar.
       </p>
    </answer>
</example>
</subsection>














<subsection xml:id="Subsection-Linear-Combinations-and-Span">
    <title>Linear Combinations and Span</title>


<definition xml:id="def-lincombabstract">

    <statement>
        <p>
            Let <m>V</m> be a vector space and let <m>\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_n</m> be vectors in <m>V</m>.  A vector <m>\mathbf{v}</m> is said to be a <term>linear combination</term> of vectors <m>\mathbf{v}_1, \mathbf{v}_2,\ldots, \mathbf{v}_n</m> if 

<me>
    \mathbf{v}=a_1\mathbf{v}_1+ a_2\mathbf{v}_2+\ldots + a_n\mathbf{v}_n,
</me>

for some scalars <m>a_1, a_2, \ldots ,a_n</m>.
        </p>
    </statement>
</definition>

<definition xml:id="def-spanabstract">

    <statement>
        <p>
            Let <m>V</m> be a vector space and let <m>\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p</m> be vectors in <m>V</m>.  The set <m>S</m> of all linear combinations of
            <me>
            \mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p
            </me>
            
            is called the <term>span</term> of <m>\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p</m>.  We write 

<me>
    S=\mbox{span}(\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p)
</me>

and we say that vectors <m>\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p</m> <term>span</term> <m>S</m>.  Any vector in <m>S</m> is said to be <term>in the span</term> of <m>\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p</m>.  The set 
<me>
\{\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p\}
</me>

is called a <term>spanning set</term> for <m>S</m>.
        </p>
    </statement>
</definition>


<p>
We revisit the situation for some specific polynomials.
</p> 



<example xml:id="ex-inthespanpoly">
    <statement>
        <p>
            Consider <m>p_{1} = 1 + x + 4x^{2}</m> and <m>p_{2} = 1 + 5x + x^{2}</m> in <m>\mathbb{P}^{2}</m>. 
            Determine whether <m>p_{1}</m> and <m>p_{2}</m> lie in 
            <me>
                \mbox{span}\{1 + 2x - x^{2}, 3 + 5x + 2x^{2}\}.
            </me>
       </p>
    </statement>
    <answer>
        <p>
            For <m>p_{1}</m>, we want to determine if <m>a</m> and <m>b</m> exist such that
<me>
p_1 = a(1 + 2x - x^2) + b(3 + 5x + 2x^2).
</me>

Expanding the right hand side gives us:

<me>
    a+2ax-ax^2+3b+5bx+2bx^2.
</me>

Combining like terms, we get:

<me>
    (a+3b)+(2a+5b)x+(-a+2b)x^2.
</me>

Setting this equal to <m>p_{1} = 1 + x + 4x^{2}</m> and
equating coefficients of powers of <m>x</m> gives us a system of equations
<me>
1 = a + 3b,\quad 1 = 2a + 5b, \quad \mbox{ and } \quad 4 = -a + 2b.
</me>
This system has the solution <m>a = -2</m> and <m>b = 1</m>, so <m>p_{1}</m> is indeed in <m>\mbox{span}\{1 + 2x - x^{2}, 3 + 5x + 2x^{2}\}</m>.

Turning to <m>p_{2} = 1 + 5x + x^{2}</m>, we are looking for <m>a</m> and <m>b</m> such that 
<me>
p_{2} = a(1 + 2x - x^{2}) + b(3 + 5x + 2x^{2}).
</me>
 Again equating coefficients of powers of <m>x</m> gives equations <m>1 = a + 3b</m>, <m>5 = 2a + 5b</m>, and <m>1 = -a + 2b</m>. But in this case there is no solution, so <m>p_{2}</m> is not in <m>\mbox{span}\{1 + 2x - x^{2}, 3 + 5x + 2x^{2}\}</m>.
       </p>
    </answer>
</example>

<theorem xml:id="th-spanisasubspaceabstract">

    <statement>
        <p>
            Let <m>V</m> be a vector space.  Let <m>S</m> be any subset of <m>V</m>.  Then <m>U=\mbox{span}(S)</m> is a subspace of <m>V</m>.
        </p>
    </statement>

<proof>
    <p>
See <xref ref="prob-spanisasubspaceabstract"/>.
    </p>
</proof>
</theorem>
</subsection>





























<subsection xml:id="Section-Bases-and-Dimension-of-Abstract-Vector-Spaces">
    <title>Bases and Dimension of Abstract Vector Spaces</title>



 

<!--
<subsection xml:id="Subsection-Linear-Independence">
    <title>Linear Independence</title>
-->

<p>
When working with <m>\R^n</m> and subspaces of <m>\R^n</m> we developed several fundamental ideas including <term>span</term>, <term>linear independence</term>, <term>bases</term> and <term>dimension</term>.  We will find that these concepts generalize easily to abstract vector spaces and that analogous results hold in these new settings.
</p>



<definition xml:id="def-linearindependenceabstract">
    <title>Linear Independence</title>
    <statement>
        <p>
            Let <m>V</m> be a vector space.  Let <m>\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p</m> be vectors of <m>V</m>.  
            We say that the set <m>\{\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p\}</m> is <term>linearly independent</term> if the only solution to 

<me>
    a_1\mathbf{v}_1+a_2\mathbf{v}_2+\ldots +a_p\mathbf{v}_p=\mathbf{0}
</me>

is the <term>trivial solution</term> <m>a_1=a_2=\ldots =a_p=0</m>.
</p> 

        <p> 
If, in addition to the trivial solution, a <term>non-trivial solution</term> (not all <m>a_1, a_2,\ldots ,a_p</m> are zero) exists, 
then we say that the set <m>\{\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_p\}</m> is <term>linearly dependent</term>.
        </p>
    </statement>
</definition>


<p> 
Let us examine this abstract version of bases in the context of polynomials, to get a feeling for these concepts.
</p> 

<example xml:id="ex-polyindset">
    <statement>
        <p>
            Show that <m>P=\{1 + x, 3x + x^{2}, 2 + x - x^{2}\}</m> is linearly independent in <m>\mathbb{P}^{2}</m>.
       </p>
    </statement>
    <answer>
        <p>
            Consider the linear combination equation
<md>
<mrow> a(1 + x) + b(3x + x^2) + c(2 + x - x^2) \amp = 0 </mrow>
<mrow> a+ax+3bx+bx^2+2c+cx-cx^2\amp =0 </mrow> 
<mrow> (a+2c)+(a+3b+c)x+(b-c)x^2\amp =0 </mrow>
</md>

The constant term, as well as the coefficients in front of <m>x</m> and <m>x^2</m>, must be equal to <m>0</m>.  This gives us the following system of equations.
<me>
\begin{array}{rlrlrcr}
	a \amp  + \amp       \amp  + \amp  2c \amp  = \amp  0 \\
	a \amp  + \amp  3b \amp  + \amp   c \amp  = \amp  0 \\
	    \amp    \amp   b \amp  - \amp   c \amp  = \amp  0 \\
\end{array}
</me>
The only solution is <m>a = b = c = 0</m>.  We conclude that <m>P</m> is linearly independent in <m>\mathbb{P}^2</m>.
       </p>
    </answer>
</example>
</subsection>











<subsection xml:id="Subsection-Bases-and-Dimension">
    <title>Bases and Dimension</title>

<p>
Recall that our motivation for defining a basis of a subspace of <m>\R^n</m> was to have a collection of vectors such that every vector of the subspace can be expressed as a unique linear combination of the vectors in that collection.  Definition of a basis (<xref ref="def-basis"/>) generalizes to abstract vector spaces as follows.
</p>


<definition xml:id="def-basisabstract">

    <statement>
        <p>
            Let <m>V</m> be a vector space.  A set <m>\mathcal{B}</m> of vectors of <m>V</m> is called a <term>basis</term> of <m>V</m> provided that 
<ol>
<li xml:id="item-defbasis1abstract">
  <p> 
<m>\mbox{span}(\mathcal{B})=V</m>  </p>
</li>
<li xml:id="item-defbasis2abstract">
  <p> 
<m>\mathcal{B}</m> is linearly independent. </p>
</li>
</ol>
        </p>
    </statement>
</definition>

<theorem xml:id="th-uniquerep">

    <statement>
        <p>
            Let <m>V</m> be a vector space, and let <m>\mathcal{B}=\{\mathbf{v}_1, \mathbf{v}_2,\ldots,\mathbf{v}_n\}</m> be a basis for <m>V</m>.  Then every element <m>\mathbf{v}</m> of <m>V</m> has a unique representation as linear combination of the elements of <m>\mathcal{B}</m>.
        </p>
    </statement>

<proof>
    <p>
By the definition of a basis, we know that <m>\mathbf{v}</m> can be written as a linear combination of <m>\mathbf{v}_1, \mathbf{v}_2,\ldots,\mathbf{v}_n</m>.  Suppose there are two such representations.  Then,

<me>
    \mathbf{v}=a_1\mathbf{v}_1+ a_2\mathbf{v}_2+\ldots+a_n\mathbf{v}_n
</me>


<me>
    \mathbf{v}=b_1\mathbf{v}_1+ b_2\mathbf{v}_2+\ldots+b_n\mathbf{v}_n
</me>

But then we have:
<md>
<mrow> a_1\mathbf{v}_1+ a_2\mathbf{v}_2+\ldots+a_n\mathbf{v}_n =b_1\mathbf{v}_1+ b_2\mathbf{v}_2+\ldots+b_n\mathbf{v}_n </mrow> 
<mrow> a_1\mathbf{v}_1+ a_2\mathbf{v}_2+\ldots+a_n\mathbf{v}_n-(b_1\mathbf{v}_1+ b_2\mathbf{v}_2+\ldots+b_n\mathbf{v}_n)\amp =\mathbf{0} </mrow>
<mrow> (a_1-b_1)\mathbf{v}_1+ (a_2-b_2)\mathbf{v}_2+\ldots+(a_n-b_n)\mathbf{v}_n\amp =\mathbf{0}  </mrow>
</md>
Because <m>\mathbf{v}_1, \mathbf{v}_2,\ldots,\mathbf{v}_n</m> are linearly independent, we have <m>a_i-b_i=0</m> for <m>1\leq i\leq n</m>. Consequently <m>a_i=b_i</m> for <m>1\leq i\leq n</m>.
    </p>
</proof>
</theorem>

<p>
In chapter <m>5</m>, we defined the dimension of a subspace of <m>\R^n</m> to be the number of elements in a basis (<xref ref="def-dimension"/>).  We will adopt this definition for abstract vector spaces.  As before, to ensure that <term>dimension</term> is well-defined we need to establish that this definition is independent of our choice of a basis.  The proof of the following theorem is identical to the proof of its counterpart in <m>\R^n</m>  (<xref ref="th-dimwelldefined"/>).
</p>

<theorem xml:id="th-dimwelldefinedabstract">

    <statement>
        <p>
            Let <m>V</m> be a vector space.  Suppose <m>\mathcal{B}=\{\mathbf{v}_1, \mathbf{v}_2,\ldots ,\mathbf{v}_t\}</m> and <m>\mathcal{C}=\{\mathbf{w}_1, \mathbf{w}_2,\ldots ,\mathbf{w}_s\}</m> be two bases of <m>V</m>.  Then <m>s=t</m>.
        </p>
    </statement>
</theorem>

<p>
Now we can state the definition of dimensions for abstract vector spaces.
</p> 


<definition xml:id="def-dimensionabstract">

    <statement>
        <p>
            Let <m>V</m> be a subspace of a vector space <m>W</m>.  The <term>dimension</term> of <m>V</m> is the number, <m>m</m>, of elements in any basis of <m>V</m>.  We write

<me>
    \mbox{dim}(V)=m.
</me>
        </p>
    </statement>
</definition>

<p>
In our discussions up to this point, we have always assumed that a basis is nonempty and hence that the dimension of the space is at least <m>1</m>. However, the zero space <m>\{\mathbf{0}\}</m> has <em>no</em> basis.  To accommodate for this, we will say that the zero vector space <m>\{\mathbf{0}\}</m> is defined to have dimension <m>0</m>:
<me>
\mbox{dim }\{\mathbf{0}\} = 0.
</me>

Our insistence that <m>\mbox{dim}\{\mathbf{0}\} = 0</m> amounts to saying that the empty set of vectors is a basis of <m>\{\mathbf{0}\}</m>. Thus the statement that ``the dimension of a vector space is the number of vectors in any basis'' holds even for the zero space. 
</p>


<example xml:id="ex-dimofM">
    <statement>
        <p>
            Recall that the vector space <m>\mathbb{M}_{m,n}</m> consists of all <m>m\times n</m> matrices (see <xref ref="ex-MLexamplesofvectspaces"/>.  Find a basis and the dimension of <m>\mathbb{M}_{m,n}</m>.
       </p>
    </statement>
    <answer>
        <p>
            Let <m>\mathcal{B}</m> consist of <m>m\times n</m> matrices 
 with exactly one entry equal to <m>1</m> and all other entries equal to <m>0</m>. It is clear that every <m>m\times n</m> matrix can be written as a linear combination of elements of <m>\mathcal{B}</m>.  It is also easy to see that the elements of <m>\mathcal{B}</m> are linearly independent.  Thus <m>\mathcal{B}</m> is a basis for <m>\mathbb{M}_{m,n}</m>.  The set <m>\mathcal{B}</m> contains <m>mn</m> elements, so <m>\mbox{dim}(\mathbb{M}_{m,n})=mn</m>.
       </p>
    </answer>
</example>

<example xml:id="ex-dimofP">
    <statement>
        <p>
            Recall that <m>\mathbb{P}^n</m> is the set of all polynomials of degree <m>n</m> or less (see <xref ref="ex-pnisavectorspace"/>. Show that <m>\mbox{dim}( \mathbb{P}^{n}) = n + 1</m> and that 
            <me>
            \lbrace 1, x, x^{2}, \dots, x^{n} \rbrace
            </me>
            
            is a basis of <m>\mathbb{P}^{n}</m>.
       </p>
    </statement>
    <answer>
        <p>
            Each polynomial
            <me>
            p(x) = a_{0} + a_{1}x + \ldots + a_{n}x^{n}, \quad \text{in } \mathbb{P}^{n},
            </me>
            
            is clearly a linear combination of <m>1, x, \dots, x^{n}</m>, so 
            <me>
            \mathbb{P}^{n} = \mbox{span} \lbrace 1, x, \dots, x^{n} \rbrace.
            </me>. 

Suppose <m>a_{0}1 + a_{1}x + \dots + a_{n}x^{n} = 0</m>, then <m>a_{0} = a_{1} = \ldots = a_{n} = 0</m>. So <m>\{1, x, \dots, x^{n}\}</m> is linearly independent and is therefore a basis containing <m>n + 1</m> vectors. Thus, <m>\mbox{dim}(\mathbb{P}^{n}) = n + 1</m>.
       </p>
    </answer>
</example>

<example xml:id="ex-CAbasis">
    <statement>
        <p>
            Consider the subset
<me>
C_A = \lbrace X \in\mathbb{M}_{2,2} : AX = XA \rbrace.
</me>
of <m>\mathbb{M}_{2,2}</m>. 

It was shown in <xref ref="ex-centralizerofA"/> of  that <m>C_A</m> is a subspace for any choice of the matrix <m>A</m>.

Let <me>
A = 
\begin{bmatrix}
1 \amp  1 \\
0 \amp  0
\end{bmatrix}.
</me>

Show that <m>\mbox{dim}(C_A) = 2</m> and find a basis of <m>C_A</m>.
       </p>
    </statement>
    <answer>
        <p>
            Suppose 
<me>X = 
\begin{bmatrix}
a \amp  b \\
c \amp  d
\end{bmatrix}
</me>
 
is in <m>C_A</m>. Then
 
<me>
    \begin{bmatrix}1\amp 1\\0\amp 0\end{bmatrix}\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}=\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix}\begin{bmatrix}1\amp 1\\0\amp 0\end{bmatrix}.
</me>

 
<me>
    \begin{bmatrix}a+c\amp b+d\\0\amp 0\end{bmatrix}=\begin{bmatrix}a\amp a\\c\amp c\end{bmatrix}.
</me>

 This gives us two relationships:  
 
<me>
    b+d=a\quad\text{and}\quad c=0.
</me>

 We can now express a generic element <m>X</m> of <m>C_A</m> as
 
<md>
<mrow>    X=\begin{bmatrix}a\amp b\\c\amp d\end{bmatrix} \amp = \begin{bmatrix}b+d\amp b\\0\amp d\end{bmatrix}  </mrow>
<mrow>    \amp =\begin{bmatrix}b\amp b\\0\amp 0\end{bmatrix}+\begin{bmatrix}d\amp 0\\0\amp d\end{bmatrix}  </mrow>
<mrow>    \amp =b\begin{bmatrix}1\amp 1\\0\amp 0\end{bmatrix}+d\begin{bmatrix}1\amp 0\\0\amp 1\end{bmatrix}. </mrow>
</md>

 
Let 

<me>
    \mathcal{B}=\left \lbrace \begin{bmatrix}1\amp 1\\0\amp 0\end{bmatrix},\begin{bmatrix}1\amp 0\\0\amp 1\end{bmatrix}\right \rbrace.
</me>


The set <m>\mathcal{B}</m> is linearly independent (see <xref ref="prob-CABlinind"/>) Every element <m>X</m> of <m>C_A</m> can be written as a linear combination of elements of <m>\mathcal{B}</m>.  Thus <m>C_A=\mbox{span}\mathcal{B}</m>.
 Therefore <m>\mathcal{B}</m> is a basis of <m>C_A</m>, and <m>\mbox{dim}(C_A) = 2</m>.
       </p>
    </answer>
</example>

<example xml:id="ex-symmetricmatsubspace">
    <statement>
        <p>
            In <xref ref="prob-symmetricsubspace"/> of you demonstrated that the set of all symmetric <m>n\times n</m> matrices is a subspace of <m>\mathbb{M}_{n,n}</m>.

Let <m>V</m> be a subspace of <m>\mathbb{M}_{2,2}</m> consisting of all <m>2\times 2</m> symmetric matrices.  Find the dimension of <m>V</m>.
       </p>
    </statement>
    <answer>
        <p>
            A matrix <m>A</m> is symmetric if <m>A^{T} = A</m>. In other words, a matrix <m>A</m> is symmetric when entries directly across the main diagonal are equal, so each <m>2 \times 2</m> symmetric matrix has the form

<me>
\begin{bmatrix}
a \amp  c \\
c \amp  b
\end{bmatrix}
= a\begin{bmatrix}
1 \amp  0 \\
0 \amp  0
\end{bmatrix}
+ b\begin{bmatrix}
0 \amp  0 \\
0 \amp  1
\end{bmatrix}
+ c\begin{bmatrix}
0 \amp  1 \\
1 \amp  0
\end{bmatrix}.
</me>

Hence the set 
<me>\mathcal{B} = \left \lbrace
\begin{bmatrix}
1 \amp  0 \\
0 \amp  0
\end{bmatrix}, \begin{bmatrix}
0 \amp  0 \\
0 \amp  1
\end{bmatrix}, \begin{bmatrix}
0 \amp  1 \\
1 \amp  0
\end{bmatrix}
\right \rbrace </me>
 spans <m>V</m>. The reader can verify that <m>\mathcal{B}</m> is linearly independent. Thus <m>\mathcal{B}</m> is a basis of <m>V</m>, so <m>\mbox{dim}(V) = 3</m>.
       </p>
    </answer>
</example>
</subsection>








<subsection xml:id="Subsection-Finite-Dimensional-Vector-Spaces">
    <title>Finite-Dimensional Vector Spaces</title>

    <p>
Our definition of dimension of a vector space depends on the vector space having a basis.  In this section we will establish that any vector space spanned by finitely many vectors has a basis.
    </p>


<definition xml:id="def-findimvectorspace">

    <statement>
        <p>
            A vector space is said to be <term>finite-dimensional</term> if it is spanned by finitely many vectors.
        </p>
    </statement>
</definition>

<p>
Given a finite-dimensional vector space <m>V</m> we will find a basis for <m>V</m> by starting with a linearly independent subset of <m>V</m> and expanding it to a basis.  The following results are more general versions of <xref ref="lemma-atmostnlinindinrn"/> and <xref ref="lemma-expandinglinindset"/>, and <xref ref="th-dimwelldefined"/>.  The proofs are identical and we will omit them.
</p>

<lemma xml:id="lemma-atmostnlinindinabstract">

    <statement>
        <p>
            Let <m>V</m> be a vector space spanned by <m>n</m> vectors.  If a linearly independent subset <m>S</m> of <m>V</m> contains <m>m</m> vectors, then <m>m\leq n</m>.
        </p>
    </statement>
</lemma>

<lemma xml:id="lemma-expandinglinindsetabstract">

    <statement>
        <p>
            Let <m>V</m> be a vector space.  Let <m>\{\mathbf{v}_1,\ldots ,\mathbf{v}_k\}</m> be a linearly independent subset of <m>V</m>.  If <m>\mathbf{u}</m> is not in <m>\mbox{span}(\mathbf{v}_1,\ldots ,\mathbf{v}_k)</m>, then <m>\{\mathbf{u},\mathbf{v}_1,\ldots ,\mathbf{v}_k\}</m> is linearly independent.
        </p>
    </statement>
</lemma>

<theorem xml:id="th-expandtobasisabstract">

    <statement>
        <p>
            Let <m>V</m> be a finite-dimensional vector space.  Any linearly independent subset of <m>V</m> can be expanded to a basis of <m>V</m>.
        </p>
    </statement>
</theorem>
</subsection>




















<subsection xml:id="Subsection-Coordinate-Vectors">
    <title>Coordinate Vectors</title>

<p>
Recall that in the context of <m>\R^n</m> (and subspaces of <m>\R^n</m>) the requirement that elements of a basis be linearly independent guarantees that every element of the vector space has a <em>unique</em> representation in terms of the elements of the basis.  (See Theorem <xref ref="th-linindbasis"/> of <url href="https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0030/main">Introduction to Bases</url>)  We proved the same property for abstract vector spaces in Theorem <xref ref="th-uniquerep"/>.
</p> 

<p>
Uniqueness of representation in terms of the elements of a basis allows us to associate every element of a vector space <m>V</m> with a unique <term>coordinate vector</term> with respect to a given basis.  Coordinate vectors were first introduced in <url href="https://ximera.osu.edu/oerlinalg/LinearAlgebra/VSP-0030/main">Introduction to Bases</url>.  We now give a formal definition.
</p>

<definition xml:id="def-coordvector">

    <statement>
        <p>
            Let <m>V</m> be a vector space, and let <m>\mathcal{B}=\{\mathbf{v}_1, \ldots ,\mathbf{v}_n\}</m> be a basis for <m>V</m>.  If <m>\mathbf{v}=a_1\mathbf{v}_1+\ldots +a_n\mathbf{v}_n</m>, then the vector in <m>\R^n</m> whose components are the coefficients <m>a_1, \ldots ,a_n</m>  is said to be the <term>coordinate vector</term> for <m>\mathbf{v}</m> with respect to <m>\mathcal{B}</m>.  We denote the coordinate vector by <m>[\mathbf{v}]_{\mathcal{B}}</m> and write:

<me>
    [\mathbf{v}]_{\mathcal{B}}=\begin{bmatrix}a_1\\\vdots \\a_n\end{bmatrix}.
</me>
        </p>
    </statement>
</definition>

<remark xml:id="coordVectorOrder">
<statement>
    <p>
The order of in which vectors <m>\mathbf{v}_1, \ldots ,\mathbf{v}_n</m> appear in <m>\mathcal{B}</m> of <xref ref="def-coordvector"/> is important.  Switching the order of these vectors would switch the order of the coordinate vector components.  For this reason, we will often use the term <term>ordered basis</term> to describe <m>\mathcal{B}</m> in the context of coordinate vectors.
    </p>
</statement>
</remark>



<p> 
Coordinate vectors may seem abstract as described above. In examples, however, one can nearly always pinpoint exactly what the coordinates are. Some examples will emphaize this:
</p> 


<example xml:id="ex-coordvectorinpolyvectspace">
    <p>
        The coordinate vector for <m>p(x)=4-3x^2+5x^3</m> in <m>\mathbb{P}^4</m> with respect to the ordered basis <m>\mathcal{B}_1=\{1, x, x^2, x^3, x^4\}</m> is 

<me>
    [p(x)]_{\mathcal{B}_1}=\begin{bmatrix}4\\0\\-3\\5\\0\end{bmatrix}.
</me>

Now let's change the order of the elements in <m>\mathcal{B}_1</m>.  The coordinate vector for <m>p(x)=4-3x^2+5x^3</m> with respect to the ordered basis <m>\mathcal{B}_2=\{x^4, x^3, x^2, x, 1\}</m> is 

<me>
    [p(x)]_{\mathcal{B}_2}=\begin{bmatrix}0\\5\\-3\\0\\4\end{bmatrix}.
</me>
    </p>
</example>

<example xml:id="ex-coordvectorinpolyvectspace2">
    <statement>
        <p>
            Show that the set <m>\mathcal{B}=\{x, 1+x, x+x^2\}</m> is a basis for <m>\mathbb{P}^2</m>. Keep the order of elements in <m>\mathcal{B}</m> and find the coordinate vector for <m>p(x)=4-x+3x^2</m> with respect to the ordered basis <m>\mathcal{B}</m>.
       </p>
    </statement>
    <answer>
        <p>
            We will begin by showing that the elements of <m>\mathcal{B}</m> are linearly independent.  Suppose 

<me>
    ax+b(1+x)+c(x+x^2)=0.
</me>

Then

<me>
    b+(a+b+c)x+cx^2=0.
</me>

This gives us the following system of equations:

<me>
    \begin{array}{ccccccc}
     \amp  \amp b\amp \amp \amp =\amp 0 
     a \amp  +\amp b\amp +\amp c\amp = \amp 0 
	 \amp  \amp \amp \amp c\amp =\amp 0 
     \end{array}  
</me>

The solution <m>a=b=c=0</m> is unique.  We conclude that <m>\mathcal{B}</m> is linearly independent.
</p>

<p>
Next, we need to show that <m>\mathcal{B}</m> spans <m>\mathbb{P}^2</m>.  To this end, we will consider a generic element <m>p(x)=\alpha+\beta x+\gamma x^2</m> of <m>\mathbb{P}^2</m> and attempt to express it as a linear combination of the elements of <m>\mathcal{B}</m>.

<me>
    ax+b(1+x)+c(x+x^2)=\alpha+\beta x+\gamma x^2.
</me>

then

<me>
    b+(a+b+c)x+cx^2=\alpha+\beta x+\gamma x^2. 
</me>

Setting the coefficients of like terms equal to each other gives us

<me>
    \begin{array}{ccccccc}
     \amp  \amp b\amp \amp \amp =\amp \alpha\\
     a \amp  +\amp b\amp +\amp c\amp = \amp \beta \\
	 \amp  \amp \amp \amp c\amp =\amp \gamma
     \end{array}
</me>

Solving this linear system of <m>a</m>, <m>b</m> and <m>c</m> gives us

<me>
    a=\beta-\alpha-\gamma,\quad b=\alpha,\quad c=\gamma .
</me>

(You should verify this.)  This shows that every element of <m>\mathbb{P}^2</m> can be written as a linear combination of elements of <m>\mathcal{B}</m>.  Therefore <m>\mathcal{B}</m> is a basis for <m>\mathbb{P}^2</m>.

To find the coordinate vector for <m>p(x)=4-x+3x^2</m> with respect to <m>\mathcal{B}</m> we need to express <m>p(x)</m> as a linear combination of the elements of <m>\mathcal{B}</m>.  Fortunately, we have already done all the necessary work.  For <m>p(x)</m>, <m>\alpha=4</m>, <m>\beta=-1</m> and <m>\gamma=3</m>.  This gives us the coefficients of the linear combination: <m>a=\beta-\alpha-\gamma=-8</m>, <m>b=\alpha=4</m>, <m>c=\gamma=3</m>.  We now write <m>p(x)</m> as a linear combination

<me>
    p(x)=-8(x)+4(1+x)+3(x+x^2)
</me>

The coordinate vector for <m>p(x)</m> with respect to <m>\mathcal{B}</m> is

<me>
    [p(x)]_{\mathcal{B}}=\begin{bmatrix}-8\\4\\3\end{bmatrix}
</me>
       </p>
    </answer>
</example>

<example xml:id="ex-symmmatsubspace">
    <statement>
        <p>
            Recall that the set <m>V</m> of all symmetric <m>2\times 2</m> matrices is a subspace of <m>\mathbb{M}_{2,2}</m>.  In <xref ref="ex-symmetricmatsubspace"/>, we demonstrated that 
<me>\mathcal{B} = \left \lbrace
\begin{bmatrix}
1 \amp  0 \\
0 \amp  0
\end{bmatrix}, \begin{bmatrix}
0 \amp  0 \\
0 \amp  1
\end{bmatrix}, \begin{bmatrix}
0 \amp  1 \\
1 \amp  0
\end{bmatrix}
\right \rbrace</me> 

is a basis for <m>V</m>.  
Let <m>A=\begin{bmatrix}2\amp -3\\-3\amp 1\end{bmatrix}</m>.  Observe that <m>A</m> is an element of <m>V</m>.

<ol>
<li>
      <p> Find the coordinate vector with respect to the ordered basis <m>\mathcal{B}</m> for <m>A</m>. </p>
</li>

<li>
      <p> Let 
<me>\mathcal{B}'=\left \lbrace 
 \begin{bmatrix}
0 \amp  0 \\
0 \amp  1
\end{bmatrix}, 
\begin{bmatrix}
1 \amp  0 \\
0 \amp  0
\end{bmatrix},
\begin{bmatrix}
0 \amp  1 \\
1 \amp  0
\end{bmatrix}
\right \rbrace </me> 

be another ordered basis for <m>V</m>.  Find the coordinate vector for <m>A</m> with respect to <m>\mathcal{B}'</m>.
</p>
</li>
</ol>
       </p>
    </statement>
    <answer>
        <p>
            We write <m>A</m> as a linear combination of the elements of <m>\mathcal{B}</m>.

<me>
    A=\begin{bmatrix}2\amp -3\\-3\amp 1\end{bmatrix}=2\begin{bmatrix}1\amp 0\\0\amp 0\end{bmatrix}+\begin{bmatrix}
0 \amp  0 \\
0 \amp  1
\end{bmatrix}-3\begin{bmatrix}
0 \amp  1 \\
1 \amp  0
\end{bmatrix}
</me>

Thus, the coordinate vector with respect to <m>\mathcal{B}</m> is

<me>
    [A]_{\mathcal{B}}=\begin{bmatrix}2\\1\\-3\end{bmatrix}
</me>

The coordinate vector with respect to <m>\mathcal{B}'</m> is

<me>
    [A]_{\mathcal{B}'}=\begin{bmatrix}1\\2\\-3\end{bmatrix}.
</me>
       </p>
    </answer>
</example>

<p>
Coordinate vectors will play a vital role in establishing one of the most fundamental results in linear algebra, that all <m>n</m>-dimensional vector spaces have the same structure as <m>\R^n</m>.  In <xref ref="ex-p2isor3"/>, for instance, we will show that <m>\mathbb{P}^2</m> is essentially the same as <m>\R^3</m>.  
</p>
</subsection>





































<exercises>

<exercisegroup>
<introduction>
    <p>
Is the set of all points in <m>\mathbb{R}^2</m> a vector space under the given definitions of addition and scalar multiplication?    In each case be specific about which vector space properties hold and which properties fail.
    </p>
</introduction>

<exercise xml:id="prob-abstractvectspace1">
    <statement>
        <p>
  Addition: <me>(a, b)+(c, d)=(a+d, b+c)</me> and scalar Multiplication: <me>k(a, b)=(ka, kb).</me>
        </p>
    </statement>
</exercise> 

<exercise xml:id="prob-abstractvectspace2">
    <statement>
        <p>
            Addition: <me>(a, b)+(c, d)=(0, b+d)</me> and scalar Multiplication: <me>k(a, b)=(ka, kb).</me>
        </p>
    </statement>
</exercise>

  <exercise xml:id="prob-abstractvectspace3">
    <statement>
        <p>
            Addition: <me>(a, b)+(c, d)=(a+c, b+d)</me> and scalar Multiplication: <me>k(a, b)=(a, kb).</me>
        </p>
    </statement>
</exercise>

  <exercise xml:id="prob-abstractvectspace4">
    <statement>
        <p>
            Addition: <me>(a, b)+(c, d)=(a-c, b-d)</me> and scalar Multiplication: <me>k(a, b)=(ka, kb).</me>
        </p>
    </statement>
</exercise>
</exercisegroup>
   
 <exercise xml:id="prob-abstractvectspace5">
    <statement>
        <p>
            Let <m>\mathcal{F}</m> be the set of all real-valued functions whose domain is all real numbers.  Define addition and scalar multiplication as follows:
 
<me>
    (f+g)(x)=f(x)+g(x)\quad (cf)(x)=cf(x).
</me>

 Verify that <m>\mathcal{F}</m> is a vector space.
        </p>
    </statement>
</exercise>


 <exercise xml:id="prob-abstractvectspacediffeq">
    <statement>
        <p>
            A differential equation is an equation that contains derivatives.  Consider the differential equation:
 <men xml:id="diffeq">
f''+f=0.
</men>

A solution to such an equation is a function.

  <ol>
  <li>
      <p> Verify that <m>f(x)=\sin x</m> is a solution to <xref ref="diffeq"/>. </p>
</li>
  <li>
      <p> Is <m>f(x)=2\sin x</m> a solution? </p>
</li>
  <li>
      <p> Is <m>f(x)=\cos x</m> a solution? </p>
</li>
  <li>
      <p> Is <m>f(x)=\sin x+\cos x</m> a solution? </p>
</li>
  <li>
      <p> Let <m>S</m> be the set of all solutions to <xref ref="diffeq"/>.  Prove that <m>S</m> is a vector space. </p>
</li>
  </ol>
        </p>
    </statement>
</exercise>


<exercise xml:id="prob-abstractvectspacecomplex">
    <statement>
        <p>
            In this problem we will check that the set <m>\mathbb{C}</m> of all complex numbers is in fact a vector space.  Let <m>z_1 = a_1 + b_1 i</m> be a complex number.  Similarly, let <m>z_2 = a_2 + b_2 i</m>, <m>z_3 = a_3 + b_3 i</m> be complex numbers, and let <m>k</m> and <m>p</m> be real number scalars.  Check that complex numbers are closed under addition and multiplication, and that they satisfy each of the vector space properties.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-abstractvectspace6">
    <statement>
        <p>
            Refer to <xref ref="ex-centralizerofA"/> and describe all elements of <m>C_I</m>, where <m>I</m> is a <m>3\times 3</m> identity matrix.
        </p>
    </statement>
</exercise>
  
<exercise xml:id="prob-abstractvectspace7">
    <statement>
        <p>
            Is the subset of all invertible <m>n\times n</m> matrices a subspace of <m>\mathbb{M}_{n,n}</m>?  Prove your claim.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-symmetricsubspace">
    <statement>
        <p>
            Is the subset of all symmetric <m>n\times n</m> matrices a subspace of <m>\mathbb{M}_{n,n}</m>? (eee  <xref ref="def-symmetricandskewsymmetric"/>.)  Prove your claim.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-abstractvectspace8">
    <statement>
        <p>
            Let <m>Z</m> be a subset of <m>\mathbb{M}_{n,n}</m> that consists of <m>n\times n</m> matrices that commute with <em>every</em> matrix in <m>\mathbb{M}_{n,n}</m> under matrix multiplication. In other words,

<me>
    Z= \lbrace B : BY=YB \mbox{ for all } Y \in \mathbb{M}_{n,n} \rbrace.
</me>


Is <m>Z</m> a subspace of <m>\mathbb{M}_{n,n}</m>?
        </p>
    </statement>

<hint>
<p> 
Don't forget to check that <m>Z</m> is not empty!
</p>
</hint>
</exercise>

<exercise xml:id="prob-abstractvectspace9">
    <statement>
        <p>
            List several elements of 
        <me>
        \mbox{span}\left(\begin{bmatrix}1\amp 0\\0\amp 1\end{bmatrix}, \begin{bmatrix}0\amp 1\\1\amp 0\end{bmatrix}\right).
        </me>
        
        Suggest a spanning set for <m>\mathbb{M}_{2,2}</m>.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-abstractvectspace10">
    <statement>
        <p>
            Describe how every element of <m>\mbox{span}(1, x, x^2, x^3)</m> looks liike.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-spanisasubspaceabstract">
    <statement>
        <p>
            Prove <xref ref="th-spanisasubspaceabstract"/>.
        </p>
    </statement>
</exercise>





<!--exercises on Bases start here-->


<exercise xml:id="prob-CABlinind">
    <statement>
        <p>
            Prove that set 
    <me>
    \mathcal{B}=\left\{\begin{bmatrix}1\amp 1\\0\amp 0\end{bmatrix},\begin{bmatrix}1\amp 0\\0\amp 1\end{bmatrix}\right\}
    </me>
    
    of <xref ref="ex-CAbasis"/> is linearly independent.
        </p>
    </statement>
</exercise>



<exercisegroup>
<introduction>
    <p>
        Show that each of the following sets of vectors is linearly independent.
    </p>
</introduction>

<exercise xml:id="prob-linindabstractvsp1">
    <statement>
        <p>
            <me>
    \lbrace 1 + x, 1 - x, x + x^{2} \rbrace \quad \text{in } \mathbb{P}^{2}.
</me>
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-linindabstractvsp2">
    <statement>
        <p>
            <me>
    \lbrace x^{2}, x + 1, 1 - x - x^{2} \rbrace  \quad \text{in } \mathbb{P}^{2}.
</me>
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-linindabstractvsp3">
    <statement>
        <p>
            <me>
\left \lbrace
\begin{bmatrix}
1 \amp  1 \\
0 \amp  0
\end{bmatrix}
, 
\begin{bmatrix}
1 \amp  0 \\
1 \amp  0
\end{bmatrix}
, 
\begin{bmatrix}
0 \amp  0 \\
1 \amp  -1
\end{bmatrix}
,\
\begin{bmatrix}
0 \amp  1 \\
0 \amp  1
\end{bmatrix}
\right \rbrace \quad \text{in } \mathbb{M}_{2,2}.
</me>
 
</p>
    </statement>
</exercise>
</exercisegroup>




<exercise xml:id="prob-linindabstractvsp123">
    <statement>
        <p>
            Show that each set in <xref ref="prob-linindabstractvsp1"/>-<xref ref="prob-linindabstractvsp3"/> is a basis for its respective vector space.
        </p>
    </statement>
</exercise>



<exercisegroup>
<introduction>
    <p>
        Find the coordinate vector for <m>p(x)=6-2x+4x^2</m> with respect to the given ordered basis <m>\mathcal{B}</m> of <m>\mathbb{P}^2</m>.
    </p>
</introduction>

<exercise xml:id="prob-coordvectors1">
    <statement>
        <p>
            <me>
    \mathcal{B}= \lbrace 1 + x, 1 - x, x + x^{2} \rbrace.
</me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    [p(x)]_{\mathcal{B}}=\begin{bmatrix}0\\6\\4\end{bmatrix}.
</me>
        </p>
    </answer>
</exercise>

<exercise xml:id="prob-coordvectors2">
    <statement>
        <p>
            <me>
    \mathcal{B}=\{x^{2}, x + 1, 1 - x - x^{2}\}.
</me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    [p(x)]_{\mathcal{B}}=\begin{bmatrix}8\\2\\4\end{bmatrix}.
</me>
        </p>
    </answer>
</exercise>
</exercisegroup>





<exercise xml:id="prob-coordvectors3">
    <statement>
        <p>
            Find the coordinate vector for
            <me>
            A=\begin{bmatrix}4\amp -3\\1\amp 2\end{bmatrix}
            </me>
            
            with respect to the ordered basis
<me>
    \mathcal{B}=
\left \lbrace
\begin{bmatrix}
1 \amp  1 \\
0 \amp  0
\end{bmatrix}
, 
\begin{bmatrix}
1 \amp  0 \\
1 \amp  0
\end{bmatrix}
, 
\begin{bmatrix}
0 \amp  0 \\
1 \amp  -1
\end{bmatrix}
,\
\begin{bmatrix}
0 \amp  1 \\
0 \amp  1
\end{bmatrix}
\right \rbrace.
</me>
        </p>
    </statement>
    <answer>
        <p>
            <me>
    [A]_{\mathcal{B}}=\begin{bmatrix}-1\\5\\-4\\-2\end{bmatrix}.
</me>
        </p>
    </answer>
</exercise>

<exercise xml:id="prob-basisforabstractvectspace">
    <statement>
        <p>
            Let <m>V</m> be a vector space of dimension <m>3</m>.  Suppose <m>S=\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3\}</m> is linearly independent in <m>V</m>.  Show that <m>S</m> is a basis for <m>V</m>.
        </p>
    </statement>
</exercise>







</exercises> 
</section>