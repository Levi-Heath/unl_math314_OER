<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="ch-linearTransformations-sec-changeOfBasis" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Change of basis</title>

    <p>
        <alert>Abstract.</alert>
        <em>Determine how the matrix representation depends on a choice of basis.</em>
    </p>

    <p>
        Suppose that <m>V</m> is an <m>n</m>-dimensional vector space equipped with two bases 
        <m>S_1 = \{{\bf v}_1,{\bf v}_2,\dots,{\bf v}_n\}</m> and <m>S_2 = \{{\bf w}_1, {\bf w}_2,\dots,{\bf w}_n\}</m> 
        (as indicated above, any two bases for <m>V</m> must have the same number of elements). 
        Taking <m>L=Id</m>, Theorem <xref ref="thm-matrep"/> yields the equation
        <men xml:id="eqn-basechange">
            {}_{S_2}({\bf v}) = {}_{S_2}(Id*{\bf v}) = {}_{S_2}Id_{S_1}*{}_{S_1}{\bf v}
        </men>
        where 
        <men xml:id="eqn-basechangematrix">
            {}_{S_2}Id_{S_1} = [{}_{S_2}{\bf v}_1\ {}_{S_2}{\bf v}_2\ \dots\ {}_{S_2}{\bf v}_n]
        </men>
        The matrix <m>{}_{S_2}Id_{S_1}</m> is referred to as a <term>base transition matrix</term>, and written as <m>{}_{S_2}T_{S_1}</m>. 
        In words, equations (<xref ref="eqn-basechange"/>) and (<xref ref="eqn-basechangematrix"/>) tells us that in order to compute the 
        coordinate vector <m>{}_{S_2}{\bf v}</m> from <m>{}_{S_1}{\bf v}</m>, we multiply <m>{}_{S_1}{\bf v}</m> on the left by 
        the <m>n\times n</m> matrix whose <m>i^{th}</m> column is the coordinate vector of <m>{\bf v}_i</m> with respect to the basis <m>S_2</m>.
    </p>
    
    <theorem>
        <statement>
            <p>
                Suppose <m>S_i,1\le i\le 3</m> are three bases for <m>V</m>. Then one has the following equalities
                <ol>
                    <li>
                        <m>{}_{S_3}T_{S_1} = {}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}</m>
                    </li>
                    <li>
                        <m>{}_{S_i}T_{S_i} = Id</m>
                    </li>
                    <li>
                        <m>{}_{S_i}T_{S_j} = \left({}_{S_j}T_{S_i}\right)^{-1}</m>
                    </li>
                </ol>
            </p>
        </statement>
    </theorem>
    <proof>
        <p>
            To show that (a) is true, it is enough to observe that for any coordinate vector <m>{}_{S_1}{\bf v}</m> there is an equality
            <me>
                {}_{S_3}T_{S_1}*{}_{S_1}{\bf v} = {}_{S_3}{\bf v} = 
                {}_{S_3}T_{S_2}*({}_{S_2}{\bf v}) 
                = {}_{S_3}T_{S_2}*\left({}_{S_2}T_{S_1}*{}_{S_1}{\bf v}\right)
                = \left({}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}\right)*{}_{S_1}{\bf v}
            </me>
            which implies the equality of matrices <m>{}_{S_3}T_{S_1}*{}_{S_1} = {}_{S_3}T_{S_2}*{}_{S_2}T_{S_1}</m>. 
            The second statement is clear; <m>{}_{S_i}T_{S_i}</m> is the <m>n\times n</m> matrix for which left multiplication 
            by it leaves each coordinate vector <m>{}_{S_1}{\bf v}</m> unchanged, and the only matrix satisfying this property 
            is the <m>n\times n</m> identity matrix. Finally (c) follows from (a) and (b) when <m>S_3 = S_1</m>.
        </p>   
    </proof>

    <p>    
        For vector spaces other than <m>\mathbb R^n</m> (such as the function space <m>F[a,b]</m> we looked at earlier) where the vectors do not naturally look like column vectors, we always use the above notation when working with their coordinate representations.
    </p>
    
    <p>
        However, in the case of <m>\mathbb R^n</m>, the vectors were <em>defined</em> as column vectors even before discussing coordinate representations. So what should we do here?
    </p>

    <p>
        The answer is that the vectors in <m>\mathbb R^n</m> are, by convention, identified with their coordinate representations in the <term>standard basis</term> <m>{\bf e} = \{{\bf e}_1,\dots,{\bf e}_n\}</m> for <m>\mathbb R^n</m>. So, for example, in <m>\mathbb R^3</m> when we wrote <m>{\bf v} = \begin{bmatrix} 2\\3\\-1\end{bmatrix}</m> what we really meant was that <m>\bf v</m> is the vector in <m>\mathbb R^3</m> with coordinate representation in the standard basis given by <m>{}_{\bf e}{\bf v} = \begin{bmatrix} 2\\3\\-1\end{bmatrix}</m>.
    </p>

    <p>
        Because it is very important to keep track of bases whenever determining base transition matrices and computing new coordinate representations, when doing so we will <em>always</em> use base-subscript notation when working with coordinate vectors, even when the vectors are in <m>\mathbb R^n</m> and are being represented in the standard basis.
    </p>

    <example>
        <p>
            Suppose <m>S = \{{\bf u}_1, {\bf u}_2, {\bf u}_3\}</m> are three vectors in <m>\mathbb R^3</m> with
            <me>
                {}_{\bf e}{\bf u}_1 = \begin{bmatrix} 1\\0\\2\end{bmatrix},\quad
                {}_{\bf e}{\bf u}_2 = \begin{bmatrix} -2\\3\\1\end{bmatrix},\quad
                {}_{\bf e}{\bf u}_3 = \begin{bmatrix} 0\\1\\-1\end{bmatrix}
            </me>
            Suppose we want to
            <ul>
                <li>
                    Show that the set of vectors <m>S</m> is a basis for <m>\mathbb R^3</m>,
                </li>
                <li>
                    compute the base transition matrix <m>{}_{S}T_{\bf e}</m>,
                </li>
                <li>
                    for <m>\bf v</m> in <m>\mathbb R^3</m> with <m>{}_{\bf e}{\bf v} = \begin{bmatrix} 2\\3\\-1\end{bmatrix}</m>, 
                    compute the coordinate representation of <m>{\bf v}</m> with repsect to the basis <m>S</m>.
                </li>
            </ul>
            To perform step 1, since <m>S</m> has the right number of vectors to be a basis for <m>\mathbb R^3</m>, 
            it suffices to show the vectors are linearly independent. 
            And we know how to do this; we form the matrix <m>A = [{}_{\bf e}{\bf u}_1\ {}_{\bf e}{\bf u}_2\ {}_{\bf e}{\bf u}_3]</m> 
            and show that the columns are linearly independent by showing <m>rref(A) = Id^{3\times 3}</m> 
            (exercise: do this, using MATLAB). This verifies <m>S</m> is a basis.
        </p>
        <p>
            Next, we look at the matrix <m>A</m>. The columns of <m>A</m> are the coordinate representations of the vectors in <m>S</m> 
            with respect to the standard basis <m>\bf e</m>. But <m>S</m> is a basis. So the matrix <m>A</m> identifies as a base-transition matrix. 
            We know it must be either <m>{}_S T_{\bf e}</m> or <m>{}_{\bf e}T_S</m>. But which one?
        </p>
        <p>
            This is where the notation being used helps us. The coordinate vectors of the columns of <m>A</m> have <q><m>\bf e</m></q> in the lower 
            left. The rule is that this must match what appears in the notation of the transition matrix. So the only possibility is
            <me>
                {}_{\bf e}T_S = A
            </me>
            To complete the second step, we then compute
            <me>
                {}_S T_{\bf e} = \left({}_{\bf e} T_S\right)^{-1} 
            </me>
            Finally, we can use this to compute <m>{}_S {\bf v}</m> as
            <me>
                {}_S {\bf v} = {}_S T_{\bf e}* {}_{\bf e}{\bf v}
            </me>
        </p>
    </example>
    
    <p>
        The following theorem combines base-transition in both the domain and range, 
        together with matrix representations of linear transformations. 
        It amounts to a <q>base-transition</q> for matrix representations of linear transformations.
    </p>

    <theorem>
        <statement>
            <p>
                If <m>S_1,S_1'</m> are two bases for <m>V</m>, <m>S_2,S_2'</m> two bases for <m>W</m>, and <m>L:V\to W</m> a linear transformation from <m>V</m> to <m>W</m>, then
                <me>
                    {}_{S_2'}L_{S_1'} = {}_{S_2'}T_{S_2}*{}_{S_2}L_{S_1}*{}_{S_1}T_{S_1'}
                </me>
            </p>
        </statement>
    </theorem>
    
    <exercise>
        <statement>
            <p>
                Let <m>S_1,S_2</m> be two bases for <m>V</m>, and <m>L:V\to V</m> a linear transformation from <m>V</m> to itself. 
                We can consider the representations <m>{}_{S_1}L_{S_1}</m> and <m>{}_{S_2}L_{S_2}</m> of <m>L</m> with respect to the bases <m>S_1</m> and <m>S_2</m>. 
                Using the above identities, show that
                <me>
                    {}_{S_1}L_{S_1} = A*{}_{S_2}L_{S_2}*A^{-1}
                </me>
                where <m>A = {}_{S_1}T_{S_2}</m> (hint: apply the above theorem.)
            </p>
        </statement>
    </exercise>
    
    <p>
        Note: Square matrices <m>B,C</m> which satisfy the equality <m>B = A*C*A^{-1}</m> are called <term>similar</term>. 
        This is an important relation between square matrices, and plays a prominent role in the theory of eigenvalues and eigenvectors as we will see later on.
    </p>

</section>