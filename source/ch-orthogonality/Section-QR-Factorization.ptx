<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-QR-Factorization" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>QR Factorization</title>



 





<p>
One of the main virtues of orthogonal
matrices is that they can be easily inverted---the transpose is the
inverse. This fact, combined with the factorization theorem in this
section, provides a useful way to simplify many matrix calculations.
</p>

<definition xml:id="def-QR-factorization">

    <statement>
        <p>
            Let <m>A</m> be an <m>m \times n</m> matrix with independent columns. A <term>QR-factorization</term> of <m>A</m> expresses it as <m>A = QR</m> where <m>Q</m> is <m>m \times n</m> with orthonormal columns and <m>R</m> is an invertible and upper triangular matrix with positive diagonal entries.
        </p>
    </statement>
</definition>

<p>
The importance of the factorization
lies in the fact that there are computer algorithms that accomplish it
with good control over round-off error, making it particularly useful in
 matrix calculations. 
</p>



<p> The factorization is a matrix version of the Gram-Schmidt process.


Suppose 
<me>
A = \left[ \begin{array}{cccc}
|\amp |\amp  \amp | \\
\mathbf{c}_{1} \amp  \mathbf{c}_{2} \amp  \cdots \amp   \mathbf{c}_{n}\\
|\amp |\amp  \amp |
\end{array}\right]
</me>

is an <m>m \times n</m> matrix with linearly independent columns <m>\mathbf{c}_{1}, \mathbf{c}_{2}, \dots, \mathbf{c}_{n}</m>. The Gram-Schmidt algorithm can be applied to these columns to provide orthogonal columns <m>\mathbf{f}_{1}, \mathbf{f}_{2}, \dots, \mathbf{f}_{n}</m> where <m>\mathbf{f}_{1} = \mathbf{c}_{1}</m> and
<me>
\mathbf{f}_{k} = \mathbf{c}_{k} - \frac{\mathbf{c}_{k} \cdot \mathbf{f}_{1}}{\norm{ \mathbf{f}_{1} }^2}\mathbf{f}_{1} + \frac{\mathbf{c}_{k} \cdot \mathbf{f}_{2}}{\norm{ \mathbf{f}_{2} }^2}\mathbf{f}_{2} - \dots - \frac{\mathbf{c}_{k} \cdot \mathbf{f}_{k-1}}{\norm{ \mathbf{f}_{k-1} }^2}\mathbf{f}_{k-1}
</me>

for each <m>k = 2, 3, \dots, n</m>. Now write <m>\mathbf{q}_{k} = \frac{1}{\norm{ \mathbf{f}_{k} }}\mathbf{f}_{k}</m> for each <m>k</m>. Then <m>\mathbf{q}_{1}, \mathbf{q}_{2}, \dots, \mathbf{q}_{n}</m> are orthonormal columns, and the above equation becomes
<me>
\norm{ \mathbf{f}_{k} } \mathbf{q}_{k} = \mathbf{c}_{k} - (\mathbf{c}_{k} \cdot \mathbf{q}_{1})\mathbf{q}_{1} - (\mathbf{c}_{k} \cdot \mathbf{q}_{2})\mathbf{q}_{2} - \dots - (\mathbf{c}_{k} \cdot \mathbf{q}_{k-1})\mathbf{q}_{k-1}.
</me>

Using these equations, express each <m>\mathbf{c}_{k}</m> as a linear combination of the <m>\mathbf{q}_{i}</m>:
<me>
\begin{array}{ccl}
\mathbf{c}_{1} \amp =\amp  \norm{ \mathbf{f}_{1} } \mathbf{q}_{1} \\
\mathbf{c}_{2} \amp =\amp  (\mathbf{c}_{2} \cdot \mathbf{q}_{1})\mathbf{q}_{1} + \norm{ \mathbf{f}_{2} } \mathbf{q}_{2} \\
\mathbf{c}_{3} \amp =\amp  (\mathbf{c}_{3} \cdot \mathbf{q}_{1})\mathbf{q}_{1} + (\mathbf{c}_{3} \cdot \mathbf{q}_{2})\mathbf{q}_{2} + \norm{ \mathbf{f}_{3} } \mathbf{q}_{3} \\
\vdots \amp \amp  \vdots \\
\mathbf{c}_{n} \amp =\amp  (\mathbf{c}_{n} \cdot \mathbf{q}_{1})\mathbf{q}_{1} + (\mathbf{c}_{n} \cdot \mathbf{q}_{2})\mathbf{q}_{2} + (\mathbf{c}_{n} \cdot \mathbf{q}_{3})\mathbf{q}_{3} + \dots + \norm{ \mathbf{f}_{n} } \mathbf{q}_{n}
\end{array}
</me>
These equations have a matrix form that gives the required factorization:
<mdn>
<mrow  xml:id="matrixFactEq"> A \amp  = \left[ \begin{array}{ccccc}
|\amp |\amp |\amp  \amp | \\
\mathbf{c}_{1} \amp  \mathbf{c}_{2} \amp  \mathbf{c}_{3} \amp \cdots \amp   \mathbf{c}_{n}\\
|\amp |\amp |\amp  \amp |
\end{array}\right]  </mrow>
<mrow number="no"> \amp = \left[ \begin{array}{ccccc}
|\amp |\amp |\amp  \amp | \\
\mathbf{q}_{1} \amp  \mathbf{q}_{2} \amp  \mathbf{q}_{3} \amp  \cdots \amp   \mathbf{q}_{n}\\
|\amp |\amp |\amp  \amp |
\end{array}\right] \left[ \begin{array}{ccccc}
\norm{ \mathbf{f}_{1} } \amp  \mathbf{c}_{2} \cdot \mathbf{q}_{1} \amp  \mathbf{c}_{3} \cdot \mathbf{q}_{1} \amp  \cdots \amp  \mathbf{c}_{n} \cdot \mathbf{q}_{1} \\
0 \amp  \norm{ \mathbf{f}_{2} } \amp  \mathbf{c}_{3} \cdot \mathbf{q}_{2} \amp  \cdots \amp  \mathbf{c}_{n} \cdot \mathbf{q}_{2} \\
0 \amp  0 \amp  \norm{ \mathbf{f}_{3} } \amp  \cdots \amp  \mathbf{c}_{n} \cdot \mathbf{q}_{3} \\
\vdots \amp  \vdots \amp  \vdots \amp  \ddots \amp  \vdots \\
0 \amp  0 \amp  0 \amp  \cdots \amp  \norm{ \mathbf{f}_{n} }
\end{array} \right]. </mrow>
</mdn>

<xref ref="matrixFactEq"/> Here the first factor 
<me>
Q = \left[ \begin{array}{ccccc}
|\amp |\amp |\amp  \amp | \\
\mathbf{q}_{1} \amp  \mathbf{q}_{2} \amp  \mathbf{q}_{3} \amp  \cdots \amp   \mathbf{q}_{n}\\
|\amp |\amp |\amp  \amp |
\end{array}\right]
</me>

has orthonormal columns, and the second factor is an <m>n \times n</m> upper triangular matrix <m>R</m> with positive diagonal entries (and so is invertible). We record this in the following theorem.
</p>


<theorem xml:id="th-QR-025133">
    <title>QR-Factorization</title>
    <statement>
        <p>
            Every <m>m \times n</m> matrix <m>A</m> with linearly independent columns has a QR-factorization <m>A = QR</m> where <m>Q</m> has orthonormal columns and <m>R</m> is upper triangular with positive diagonal entries.
        </p>
    </statement>
</theorem>

<p>
The matrices <m>Q</m> and <m>R</m> in <xref ref="th-QR-025133"/> are uniquely determined by <m>A</m>; we return to this below.
</p> 


<example xml:id="ex-QR4x3-025139">
    <statement>
        <p>
            Find the QR-factorization of <me>A = \left[ \begin{array}{rrr}
1 \amp  1 \amp  0 \\
-1 \amp  0 \amp  1 \\
0 \amp  1 \amp  1 \\
0 \amp  0 \amp  1
\end{array}\right]</me>.
       </p>
    </statement>
    <answer>
        <p>
            Denote the columns of <m>A</m> as <m>\mathbf{c}_{1}</m>, <m>\mathbf{c}_{2}</m>, and <m>\mathbf{c}_{3}</m>, and observe that <m>\{\mathbf{c}_{1}, \mathbf{c}_{2}, \mathbf{c}_{3}\}</m> is independent. If we apply the Gram-Schmidt algorithm to these columns, the result is:
<me>
\mathbf{f}_{1} = \mathbf{c}_{1} = \left[ \begin{array}{r}
1  \\
-1  \\
0  \\
0
\end{array}\right], \quad \mathbf{f}_{2} = \mathbf{c}_{2} - \frac{1}{2}\mathbf{f}_{1} = \left[ \def\arraystretch{1.2} \begin{array}{r}
\frac{1}{2}  \\
\frac{1}{2}  \\
1  \\
0
\end{array}\right], \mbox{ and }
</me>

<me> \mathbf{f}_{3} = \mathbf{c}_{3} + \frac{1}{2}\mathbf{f}_{1} - \mathbf{f}_{2} = \left[ \begin{array}{r}
0  \\
0  \\
0  \\
1
\end{array}\right].
</me>
Write <m>\mathbf{q}_{j} = \frac{1}{\norm{ \mathbf{f}_{j} }^2}\mathbf{f}_{j}</m>
 for each <m>j</m>, so <m>\{\mathbf{q}_{1}, \mathbf{q}_{2}, \mathbf{q}_{3}\}</m> is orthonormal. Then <xref ref="matrixFactEq"/> preceding<xref ref="th-QR-025133"/> gives <m>A = QR</m> where
<md>
<mrow> Q \amp = \left[ \begin{array}{ccc} | \amp  | \amp  | \\
\mathbf{q}_{1} \amp  \mathbf{q}_{2} \amp  \mathbf{q}_{3} \\
| \amp  | \amp  |
\end{array}\right] = \left[ \def\arraystretch{1.3}\begin{array}{ccc}
\frac{1}{\sqrt{2}} \amp  \frac{1}{\sqrt{6}} \amp  0 \\
\frac{-1}{\sqrt{2}} \amp  \frac{1}{\sqrt{6}} \amp  0 \\
0 \amp  \frac{2}{\sqrt{6}} \amp  0 \\
0 \amp  0 \amp  1
\end{array}\right]  = \frac{1}{\sqrt{6}} \left[ \begin{array}{ccc}
\sqrt{3} \amp  1 \amp  0 \\
-\sqrt{3} \amp  1 \amp  0 \\
0 \amp  2 \amp  0 \\
0 \amp  0 \amp  \sqrt{6}
\end{array} \right] </mrow>
<mrow> R \amp = \left[ \begin{array}{ccc}
\norm{ \mathbf{f}_{1} } \amp  \mathbf{c}_{2} \cdot \mathbf{q}_{1} \amp  \mathbf{c}_{3} \cdot \mathbf{q}_{1} \\
0 \amp  \norm{ \mathbf{f}_{2} } \amp  \mathbf{c}_{3} \cdot \mathbf{q}_{2} \\
0 \amp  0 \amp  \norm{ \mathbf{f}_{3} } \\
\end{array} \right] = \left[ \def\arraystretch{1.5} \begin{array}{ccc}
\sqrt{2} \amp  \frac{1}{\sqrt{2}} \amp  \frac{-1}{\sqrt{2}} \\
0 \amp  \frac{\sqrt{3}}{\sqrt{2}} \amp  \frac{\sqrt{3}}{\sqrt{2}} \\
0 \amp  0 \amp  1
\end{array} \right] = \frac{1}{\sqrt{2}}\left[ \begin{array}{ccc}
2 \amp  1 \amp  -1 \\
0 \amp  \sqrt{3} \amp  \sqrt{3} \\
0 \amp  0 \amp  \sqrt{2}
\end{array} \right] </mrow>
</md>

The reader can verify that indeed <m>A = QR</m>.
       </p>
    </answer>
</example>

<p>
If a matrix <m>A</m> has independent rows and we apply QR-factorization to <m>A^{T}</m>, the result is:
</p> 


<corollary xml:id="cor-QR-transpose-025162">

    <statement>
        <p>
            If <m>A</m> has independent rows, then <m>A</m> factors uniquely as <m>A = LP</m> where <m>P</m> has orthonormal rows and <m>L</m> is an invertible lower triangular matrix with positive main diagonal entries.
        </p>
    </statement>
</corollary>

<p>
Since a square matrix with orthonormal columns is orthogonal, we have:
</p>


<theorem xml:id="th-025166">

    <statement>
        <p>
            Every square invertible matrix <m>A</m> has factorizations <m>A = QR</m> and <m>A = LP</m> where <m>Q</m> and <m>P</m> are orthogonal, <m>R</m> is upper triangular with positive diagonal entries, and <m>L</m> is lower triangular with positive diagonal entries.
        </p>
    </statement>
</theorem>

<p>
We now take the time to prove the uniqueness of the QR-factorization.
</p>


<theorem xml:id="th-QR-unique-025187">

    <statement>
        <p>
            Let <m>A</m> be an <m>m \times n</m> matrix with independent columns. If <m>A = QR</m> and <m>A = Q_{1}R_{1}</m> are QR-factorizations of <m>A</m>, then <m>Q_{1} = Q</m> and <m>R_{1} = R</m>.
        </p>
    </statement>


<proof>
    <p>
Write 
<me>Q = \left[ \begin{array}{cccc}
|\amp |\amp  \amp | \\
\mathbf{c}_{1} \amp  \mathbf{c}_{2} \amp  \cdots \amp   \mathbf{c}_{n}\\
|\amp |\amp  \amp |
\end{array}\right] \quad \text{and} \quad Q_{1} =  \left[ \begin{array}{cccc}
|\amp |\amp  \amp | \\
\mathbf{d}_{1} \amp  \mathbf{d}_{2} \amp  \cdots \amp   \mathbf{d}_{n}\\
|\amp |\amp  \amp |
\end{array}\right]</me> in terms of their columns, and observe first that <m>Q^TQ = I_{n} = Q_{1}^TQ_{1}</m> because <m>Q</m> and <m>Q_{1}</m> have orthonormal columns. Hence it suffices to show that <m>Q_{1} = Q</m> (then <m>R_{1} = Q_{1}^TA = Q^TA = R</m>). Since <m>Q_{1}^TQ_{1} = I_{n}</m>, the equation <m>QR = Q_{1}R_{1}</m> gives <m>Q_{1}^TQ = R_{1}R^{-1}</m>; for convenience we write this matrix as
<me>
Q_{1}^TQ = R_{1}R^{-1} = \left[ \begin{array}{c} t_{ij} \end{array}\right]
</me>
This matrix is upper triangular with positive diagonal elements (since this is true for <m>R</m> and <m>R_{1}</m>), so <m>t_{ii} \gt  0</m> for each <m>i</m> and <m>t_{ij} = 0</m> if <m>i \gt  j</m>. On the other hand, the <m>(i, j)</m>-entry of <m>Q_{1}^TQ</m> is <m>\mathbf{d}_{i}^T\mathbf{c}_{j} = \mathbf{d}_{i} \cdot \mathbf{c}_{j}</m>, so we have <m>\mathbf{d}_{i} \cdot \mathbf{c}_{j} = t_{ij}</m> for all <m>i</m> and <m>j</m>. But each <m>\mathbf{c}_{j}</m> is in <m>\mbox{span}\{\mathbf{d}_{1}, \mathbf{d}_{2}, \dots, \mathbf{d}_{n}\}</m> because <m>Q = Q_{1}(R_{1}R^{-1})</m>. We know how to write a vector as a linear combination of an orthonormal basis (using Corollary~<xref ref="cor-orthonormal"/> from <url href="https://ximera.osu.edu/oerlinalg/LinearAlgebra/RTH-0010/main">Orthogonality and Projections</url>):
<md>
<mrow> \mathbf{c}_{j} \amp = (\mathbf{d}_{1} \cdot \mathbf{c}_{j})\mathbf{d}_{1} + (\mathbf{d}_{2} \cdot \mathbf{c}_{j})\mathbf{d}_{2} + \dots + (\mathbf{d}_{n} \cdot \mathbf{c}_{j})\mathbf{d}_{n} </mrow> 
<mrow> \amp = t_{1j}\mathbf{d}_{1} + t_{2j}\mathbf{d}_{2} + \dots + t_{jj}\mathbf{d}_{i}, </mrow>
</md>
because <m>\mathbf{d}_{i} \cdot \mathbf{c}_{j} = t_{ij} = 0</m> if <m>i \gt  j</m>. The first few equations here are
<me>
\begin{array}{ccl}
\mathbf{c}_{1} \amp =\amp  t_{11}\mathbf{d}_{1} \\
\mathbf{c}_{2} \amp =\amp  t_{12}\mathbf{d}_{1} + t_{22}\mathbf{d}_{2} \\
\mathbf{c}_{3} \amp =\amp  t_{13}\mathbf{d}_{1} + t_{23}\mathbf{d}_{2} + t_{33}\mathbf{d}_{3} \\
\mathbf{c}_{4} \amp =\amp  t_{14}\mathbf{d}_{1} + t_{24}\mathbf{d}_{2} + t_{34}\mathbf{d}_{3} + t_{44}\mathbf{d}_{4} \\
\vdots \amp \amp  \vdots
\end{array}
</me>
The first of these equations gives 
<me>
1 = \norm{ \mathbf{c}_{1} } = \norm{ t_{11}\mathbf{d}_{1} } = | t_{11} | \norm{ \mathbf{d}_{1} } = t_{11},
</me>

whence <m>\mathbf{c}_{1} = \mathbf{d}_{1}</m>. But then we have <m>t_{12} = \mathbf{d}_{1} \cdot \mathbf{c}_{2} = \mathbf{c}_{1} \cdot \mathbf{c}_{2} = 0</m>, so the second equation becomes <m>\mathbf{c}_{2} = t_{22}\mathbf{d}_{2}</m>. Now a similar argument gives <m>\mathbf{c}_{2} = \mathbf{d}_{2}</m>, and then <m>t_{13} = 0</m> and <m>t_{23} = 0</m> follows in the same way. Hence <m>\mathbf{c}_{3} = t_{33}\mathbf{d}_{3}</m> and <m>\mathbf{c}_{3} = \mathbf{d}_{3}</m>. Continue in this way to get <m>\mathbf{c}_{i} = \mathbf{d}_{i}</m> for all <m>i</m>. This proves that <m>Q_{1} = Q</m>.
    </p>
</proof>
</theorem>









<subsection xml:id="Subsection-QR-Algorithm-for-approximating-eigenvalues">
    <title>QR-Algorithm for approximating eigenvalues</title>

<p>
We learned about an iterative method for computing eigenvalues in the preceding chapter.  We also mentioned that a better method for approximating the eigenvalues of an invertible matrix <m>A</m> depends on the QR-factorization of <m>A</m>.  While it is beyond the scope of this book to pursue a detailed discussion of this method, we give an example and conclude with some remarks on the QR-algorithm.
</p> 

<p>
The <term>QR-algorithm</term> uses QR-factorization repeatedly to create a sequence of matrices <m>A_{1} =A, A_{2}, A_{3}, \dots,</m> as follows:

<ol>
<li>
      <p> Define <m>A_{1} = A</m> and factor it as <m>A_{1} = Q_{1}R_{1}</m>. </p>
</li>

<li>
      <p> Define <m>A_{2} = R_{1}Q_{1}</m> and factor it as <m>A_{2} = Q_{2}R_{2}</m>. </p>
</li>

<li>
      <p> Define <m>A_{3} = R_{2}Q_{2}</m> and factor it as <m>A_{3} = Q_{3}R_{3}</m>. </p>
</li>

<li>
<p>
<m>\vdots</m>
</p> 
</li>
</ol>

In general, <m>A_{k}</m> is factored as <m>A_{k} = Q_{k}R_{k}</m> and we define <m>A_{k + 1} = R_{k}Q_{k}</m>. Then <m>A_{k + 1}</m> is similar to <m>A_{k}</m> [in fact, <m>A_{k+1} = R_{k}Q_{k} = (Q_{k}^{-1}A_{k})Q_{k}</m>], and hence each <m>A_{k}</m> has the same eigenvalues as <m>A</m>. If the eigenvalues of <m>A</m> are real and have distinct absolute values, the remarkable thing is that the sequence of matrices <m>A_{1}, A_{2}, A_{3}, \dots</m> converges to an upper triangular matrix with these eigenvalues on the main diagonal. [See below for the case of complex eigenvalues.]
</p> 


<example xml:id="QR-algortihm-2x2-025425">
    <statement>
        <p>
            If <me>
                A = \left[ \begin{array}{rr}
1 \amp  1 \\
2 \amp  0
\end{array}\right]</me> as in <xref ref="ex-2x2PowerMethod025326"/>. Use the QR-algorithm to approximate the eigenvalues.
       </p>
    </statement>
    <answer>
        <p>
            The matrices <m>A_{1}</m>, <m>A_{2}</m>, and <m>A_{3}</m> are as follows:
<md>
<mrow> A_{1} = \amp  \left[ \begin{array}{rr}
1 \amp  1 \\
2 \amp  0
\end{array}\right] = Q_{1}R_{1} \quad \mbox{ where } Q_{1} = \frac{1}{\sqrt{5}}\left[ \begin{array}{rr}
1 \amp  2 \\
2 \amp  -1
\end{array}\right] \mbox{ and } R_{1} =  \frac{1}{\sqrt{5}}\left[ \begin{array}{rr}
5 \amp  1 \\
0 \amp  2
\end{array}\right] </mrow>
<mrow> A_{2} = \amp  \frac{1}{5}\left[ \begin{array}{rr}
7 \amp  9 \\
4 \amp  -2
\end{array}\right] = \left[ \begin{array}{rr}
1.4 \amp  -1.8 \\
-0.8 \amp  -0.4
\end{array}\right]= Q_{2}R_{2} \\
\amp \mbox{ where } Q_{2} = \frac{1}{\sqrt{65}}\left[ \begin{array}{rr}
7 \amp  4 \\
4 \amp  -7
\end{array}\right] \mbox{ and } R_{2} =  \frac{1}{\sqrt{65}}\left[ \begin{array}{rr}
13 \amp  11 \\
0 \amp  10
\end{array}\right] </mrow>
<mrow> A_{3} = \amp \frac{1}{13}\left[ \begin{array}{rr}
27 \amp  -5 \\
8 \amp  -14
\end{array}\right] = \left[ \begin{array}{rr}
2.08 \amp  -0.38 \\
0.62 \amp  -1.08
\end{array}\right]. </mrow>
</md>
This is converging to <me>\left[ \begin{array}{rr}
2 \amp  \ast \\
0 \amp  -1
\end{array}\right]</me> and so is approximating the eigenvalues <m>2</m> and <m>-1</m> on the main diagonal.
       </p>
    </answer>
</example>



<!--
<subsection xml:id="Subsubsection-Shifting">
    <title>Shifting</title>

<p>
We here discuss shifting.  Convergence is accelerated if, at stage <m>k</m> of the algorithm, a number <m>\tau_{k}</m> is chosen and <m>A_{k} - \tau_{k}I</m> is factored in the form <m>Q_{k}R_{k}</m> rather than <m>A_{k}</m> itself. Then
<me>
Q_{k}^{-1}A_{k}Q_{k} = Q_{k}^{-1}(Q_{k}R_{k} + \tau_{k}I)Q_{k} = R_{k}Q_{k} + \tau_{k}I
</me>
so we take <m>A_{k+1} = R_{k}Q_{k} + \tau_{k}I</m>. If the shifts <m>\tau_{k}</m> are carefully chosen, convergence can be greatly improved.
</subsubsection>



<subsection xml:id="Subsection-QR-Algorithm-for-approximating-eigenvalues">
    <title>QR-Algorithm for approximating eigenvalues</title>

\subsubsection*{Preliminary Preparation.} A matrix such as
<me>
\left[ \begin{array}{rrrrr}
\ast  \amp  \ast \amp  \ast \amp  \ast \amp  \ast  \\
\ast  \amp  \ast \amp  \ast \amp  \ast \amp  \ast  \\
0  \amp  \ast \amp  \ast \amp  \ast \amp  \ast  \\
0  \amp  0 \amp  \ast \amp  \ast \amp  \ast  \\
0  \amp  0 \amp  0 \amp  \ast \amp  \ast  \\
\end{array}\right]
</me>
is said to be in <term>upper Hessenberg</term> form, and the QR-factorizations of such matrices are greatly simplified. Given an <m>n \times n</m> matrix <m>A</m>, a series of orthogonal matrices <m>H_{1}, H_{2}, \dots, H_{m}</m> (called <term>Householder matrices</term> can be easily constructed such that
<me>
B = H_{m}^T \cdots H_{1}^TAH_{1} \cdots H_{m}
</me>
is in upper Hessenberg form. Then the QR-algorithm can be efficiently applied to <m>B</m> and, because <m>B</m> is similar to <m>A</m>, it produces the eigenvalues of <m>A</m>.

</subsubsection>
-->
</subsection>









<exercises>

<exercisegroup  xml:id="prob-findQR">
<introduction>
    <p>
        In each case find the QR-factorization of <m>A</m>.
    </p>
</introduction>

<exercise>
    <statement>
        <p>
            <me>A = \left[ \begin{array}{rr}
                1 \amp  -1 \\
                -1 \amp  0
                \end{array}\right].</me>     
        </p>
    </statement>

    
    <answer>
        <p>
            <me>Q = \frac{1}{\sqrt{5}}\left[ \begin{array}{rr}
                2 \amp  -1 \\
                1 \amp  2
                \end{array}\right], \quad 
                 R = \frac{1}{\sqrt{5}}\left[ \begin{array}{rr}
                5 \amp  3 \\
                0 \amp  1
                \end{array}\right].
            </me>
        </p>
    </answer>
</exercise>

    <exercise>
        <statement>
            <p>
                <me>A = \left[ \begin{array}{rr}
                    2 \amp  1 \\
                    1 \amp 1
                    \end{array}\right].</me> 
            </p>
        </statement>
        <answer>
            <p>
                <me>Q = \frac{1}{\sqrt{3}}\left[ \begin{array}{rrr}
                    1 \amp  1 \amp  0 \\
                    -1 \amp  0 \amp  1 \\
                    0 \amp  1 \amp  1 \\
                    1 \amp  -1 \amp  1 
                    \end{array}\right], \quad
                    R = \frac{1}{\sqrt{3}}\left[ \begin{array}{rrr}
                    3 \amp  0 \amp  -1 \\
                    0 \amp  3 \amp  1 \\
                    0 \amp  0 \amp  2
                    \end{array}\right].
                </me>
            </p>
        </answer>
    </exercise>

        <exercise>
            <statement>
                <p>
                    <me>A = \left[ \begin{array}{rrr}
                        1 \amp  1 \amp  1 \\
                        1 \amp  1 \amp  0 \\
                        1 \amp  0 \amp  0 \\
                        0 \amp  0 \amp  0 
                        \end{array}\right].</me>   
                </p>
            </statement>
        </exercise>

            <exercise>
                <statement>
                    <p>
                        <me>A = \left[ \begin{array}{rrr}
                            1 \amp  1 \amp  0 \\
                            -1 \amp  0 \amp  1 \\
                            0 \amp  1 \amp  1 \\
                            1 \amp  -1 \amp  0
                            \end{array}\right].</me> 
                    </p>
                </statement>
</exercise>
</exercisegroup>

<exercise xml:id="prob-take-diag-positive">
    <statement>
        <p>
            If <m>R</m> is upper triangular and invertible, show that there exists a diagonal matrix <m>D</m> with diagonal entries <m>\pm 1</m> such that <m>R_{1} = DR</m> is invertible, upper triangular, and has positive diagonal entries.
        </p>
    </statement>
</exercise>


<exercise xml:id="prob-fullQR">
    <statement>
        <p>
            If <m>A</m> has independent columns, let \\ <m>A = QR</m> where <m>Q</m> has orthonormal columns and <m>R</m> is invertible and upper triangular. (Some authors do not require a <m>QR</m>-factorization to have positive diagonal entries.) Show that there is a diagonal matrix <m>D</m> with diagonal entries <m>\pm 1</m> such that <m>A = (QD)(DR)</m> is the QR-factorization of <m>A</m>.
        </p>
    </statement>

<hint>
See <xref ref="prob-take-diag-positive"/>.
</hint>
</exercise>



<exercisegroup>
<introduction>
    <p>
        In each case, find the exact eigenvalues and then approximate them using the QR-algorithm.
    </p>
</introduction>


<exercise>
    <statement>
        <p>
            <me>A = \left[ \begin{array}{rr}
                1 \amp  1 \\
                1 \amp  0
                \end{array}\right].</me>
            </p>
        </statement>
<answer>
    <p>
        <me>A_{1} = \left[ \begin{array}{rr}
            3 \amp  1 \\
            1 \amp  0 
            \end{array}\right], \quad Q_{1} = \frac{1}{\sqrt{10}}\left[ \begin{array}{rr}
            3 \amp  -1 \\
            1 \amp  3
            \end{array}\right],\quad R_{1} = \frac{1}{\sqrt{10}}\left[ \begin{array}{rr}
            10 \amp  3 \\
            0 \amp  -1
            \end{array}\right].</me> 
    </p>
</answer>
</exercise>


<exercise>
<statement>
    <p>
        <me>A = \left[ \begin{array}{rr}
            3 \amp  1 \\
            1 \amp  0 
            \end{array}\right].</me> 
    </p>
</statement>

<answer>
    <p>
        <me>A_{2} = \frac{1}{10}\left[ \begin{array}{rr}
            33 \amp  -1 \\
            -1 \amp  -3
            \end{array}\right], \quad Q_{2} = \frac{1}{\sqrt{1090}}\left[ \begin{array}{rr}
            33 \amp  1 \\
            -1 \amp  33
            \end{array}\right], \quad R_{2} = \frac{1}{\sqrt{1090}}\left[ \begin{array}{rr}
            109 \amp  -3 \\
            0 \amp  -10
            \end{array}\right].</me>     
    </p>
</answer>
</exercise>
</exercisegroup>






<exercise xml:id="prob-QR-symmetric">
    <statement>
        <p>
            If <m>A</m> is symmetric, show that each matrix <m>A_{k}</m> in the QR-algorithm is also symmetric. Deduce that they converge to a diagonal matrix.
        </p>
    </statement>

<answer>
    <p>
Use induction on <m>k</m>. If <m>k = 1</m>, <m>A_{1} = A</m>. In general <m>A_{k+1} = Q_{k}^{-1}A_{k}Q_{k} = Q_{k}^{T}A_{k}Q_{k}</m>, so the fact that <m>A_{k}^{T} = A_{k}</m> implies <m>A_{k+1}^{T} = A_{k+1}</m>. The eigenvalues of <m>A</m> are all real (<xref ref="cor-ews-symmetric-real"/>), so the <m>A_{k}</m> converge to an upper triangular matrix <m>T</m>. But <m>T</m> must also be symmetric (it is the limit of symmetric matrices), so it is diagonal.

    </p>
</answer>
</exercise>

<exercise xml:id="QR-special-2x2">
    <statement>
        <p>
            Apply the QR-algorithm to 
            <me>A = \left[ \begin{array}{rr}
2 \amp  -3 \\
1 \amp  -2
\end{array}\right].</me> Explain.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-analyzeQRalgorithm">
    <statement>
        <p>
            Given a matrix <m>A</m>, let <m>A_{k}</m>, <m>Q_{k}</m>, and <m>R_{k}</m>, <m>k \geq 1</m>, be the matrices constructed in the QR-algorithm. Show that <m>A_{k} = (Q_{1}Q_{2} \cdots Q_{k})(R_{k} \cdots R_{2}R_{1})</m> for each <m>k \geq 1</m> and hence that this is a QR-factorization of <m>A_{k}</m>. 

        </p>
    </statement>

<hint>
    <p>
        Show that <m>Q_{k}R_{k} = R_{k-1}Q_{k-1}</m> for each <m>k \geq 2</m>, and use this equality to compute <m>(Q_{1}Q_{2} \cdots Q_{k})(R_{k} \cdots R_{2}R_{1})</m> ``from the centre out.'' 
        Use the fact that <m>(AB)^{n+1} = A(BA)^{n}B</m> for any square matrices <m>A</m> and <m>B</m>.
    </p>
</hint>
</exercise>

</exercises>
</section>