<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="Section-Similar-Matrices-and-Their-Properties" xmlns:xi="http://www.w3.org/2001/XInclude">
    <title>Similar - and Diagonalizable Matrices</title>



 






<p>
Let <m>A</m> and <m>B</m> be <m>n \times n</m> matrices.  Then the products <m>AB</m> and <m>BA</m> are both <m>n \times n</m> matrices.  In most cases the products <m>AB</m> and <m>BA</m> are not equal.

However, for some pairs of <m>n \times n</m> matrices <m>A</m> and <m>B</m>, we are able to find an invertible matrix <m>P</m> such that <m>PB = AP</m>.  This leads to the following definition.
</p>


<definition xml:id="def-similar">

    <statement>
        <p>
            If <m>A</m> and <m>B</m> are <m>n \times n</m> matrices, we say that <m>A</m> and <m>B</m> are <term>similar</term>, 
            if <m>B = P^{-1}AP</m> for some invertible matrix <m>P</m>.  In this case we write 
            <me>A \sim B.</me>
        </p>
    </statement>
</definition>

<p> 
The following theorem shows that <term>similarity</term> (<m>\sim</m>) satisfies <term>reflexive</term>, <term>symmetric</term>, and <term>transitive</term> properties.
</p>


<theorem xml:id="th-similarityequivalence">

    <statement>
        <p>
            Similarity is an <term>equivalence relation</term>, i.e. for <m>n \times n</m> matrices <m>A,B,</m> and <m>C</m>,
<ol>
<li xml:id="item-reflexive">
  <p> <m>A \sim A</m> (reflexive) </p>
</li>
<li xml:id="item-symmetric">
  <p> If <m>A \sim B</m>, then <m>B \sim A</m> (symmetric) </p>
</li>
<li xml:id="item-transitive">
  <p> If <m>A \sim B</m> and <m>B \sim C</m>, then <m>A \sim C</m> (transitive) </p>
</li>
</ol>
        </p>
    </statement>


<proof>
    <p>
<xref ref="item-reflexive"/>: It is clear that <m>A\sim A</m> (let <m>P=I</m>).

<xref ref="item-symmetric"/>
If <m>A\sim B,</m> then for some invertible matrix <m>P</m>,
<me>
A=P^{-1}BP
</me>
and so
<me>
PAP^{-1}=B.
</me>
But then
<me>
\left( P^{-1}\right) ^{-1}AP^{-1}=B,
</me>
which shows that <m>B\sim A</m>.

<xref ref="item-transitive"/>
Now suppose <m>A\sim B</m> and <m>B\sim C</m>. Then there exist invertible matrices
<m>P,Q</m> such that
<me>
A=P^{-1}BP,\ B=Q^{-1}CQ.
</me>
Then,
<me>
A=P^{-1} \left( Q^{-1}CQ \right)P=\left( QP\right) ^{-1}C\left( QP\right),
</me>
showing that <m>A</m> is similar to <m>C</m>.
    </p>
</proof>
</theorem>



<p>
Any relation satisfying the reflexive, symmetric and transitive properties is called an  <term>equivalence relation</term>. <xref ref="th-similarityequivalence"/> proves that similarity between matrices is an <term>equivalence relation</term>. <xref ref="prob-lessthan"/> gives a good example of a relation that is NOT an equivalence relation.

As we will see later, similar matrices share many properties. 
</p>

<p> 
 Before proceeding to explore these properties, we pause to introduce a simple matrix function that we will continue to use throughout the course.  
 Another important quantity we can compute is called the <term>trace</term> of a matrix.
</p>

<definition xml:id="def-trace">

    <statement>
        <p>
            The <term>trace</term> of an <m>n \times n</m> matrix <m>A</m>, abbreviated <m>\mbox{tr} A</m>, is defined to be the sum of the main diagonal elements of <m>A</m>. 
             In other words, if <m> A = [a_{ij}]</m>, then 
<me>
    \mbox{tr}(A) = a_{11} + a_{22} + \dots + a_{nn}.
</me>
  We may also write <m>\mbox{tr}(A) =\sum_{i=1}^n a_{ii}</m>.
        </p>
    </statement>
</definition>

<remark>
<statement>
<p>
It is easy to see that <m>\mbox{tr}(A + B) = \mbox{tr} A + \mbox{tr} B</m> and that <m>\mbox{tr}(cA) = c \mbox{ tr}(A)</m> holds for all <m>n \times n</m> matrices <m>A</m> and <m>B</m> and all scalars <m>c</m>. The following fact is more surprising.
</p>
</statement>
</remark>


<theorem xml:id="th-trAB-trBA">

    <statement>
        <p>
            Let <m>A</m> and <m>B</m> be <m>n \times n</m> matrices. Then <m>\mbox{tr}(AB) = \mbox{tr}(BA)</m>.
        </p>
    </statement>


<proof>
    <p>
Write <m>A = [a_{ij}]</m> and <m>B = [b_{ij}]</m>. For each <m>i</m>, the <m>(i, i)</m>-entry <m>d_{i}</m> of the matrix <m>AB</m> is given as follows:
<me>d_{i} = a_{i1}b_{1i} + a_{i2}b_{2i} + \dots + a_{in}b_{ni} = \sum_{j}a_{ij}b_{ji}.</me>

Hence
<me>
\mbox{tr}(AB) = d_1 + d_2 + \dots + d_n = \sum_{i}d_i = \sum_{i}\left(\sum_{j}a_{ij}b_{ji}\right).
</me>
Similarly we have
<me>
\mbox{tr}(BA) = \sum_{i}\left(\sum_{j}b_{ij}a_{ji}\right).
</me>

Since these two double sums are the same, we have proven the theorem.
    </p>
</proof>
</theorem>

<p>
The following theorem lists a number of properties shared by similar matrices.
</p>

<theorem xml:id="th-properties-similar">

    <statement>
        <p>
            If <m>A</m> and <m>B</m> are <m>n\times n</m> matrices and <m>A\sim B</m>, then
<ol>
<li xml:id="th-properties-similar-det">
  <p> <m>\det(A) = \det(B)</m>, </p>
</li>
<li xml:id="th-properties-similar-rank">
  <p>  <m>\mbox{rank}(A) = \mbox{rank}(B)</m>, </p>
</li>
<li xml:id="th-properties-similar-trace">
  <p>  <m>\mbox{tr}(A)= \mbox{tr}(B)</m>, </p>
</li>
<li xml:id="th-properties-similar-char-poly">
  <p> <m>A</m> and <m>B</m> have the same characteristic equations, and </p>
</li>
<li xml:id="th-properties-similar-eig">
  <p>  <m>A</m> and <m>B</m> have the same eigenvalues. </p>
</li>
</ol>
        </p>
    </statement>


<proof>
    <p>
Let <m>B = P^{-1}AP</m> for some invertible matrix <m>P</m>. 
    </p>

    <p> 
For <xref ref="th-properties-similar-det"/>, <m>\det B = \det(P^{-1}) \det A \det P = \det A</m> because <m>\det(P^{-1}) = 1/ \det P</m> (see <xref ref="th-detofinverse"/>).
    </p>

    <p>
Similarly, for <xref ref="th-properties-similar-rank"/>, <m>\mbox{rank} B = \mbox{rank}(P^{-1}AP) = \mbox{rank} A</m>, because multiplication by an invertible matrix cannot change the rank.  To see this, note that any invertible matrix is a product of elementary matrices.  Multiplying by elementary matrices is equivalent to performing elementary row (column) operations on <m>A</m>, which does not change the row (column) space, nor the rank.  It follows that similar matrices have the same rank. 
    </p> 

    <p> 
For <xref ref="th-properties-similar-trace"/>, we make use of <xref ref="th-trAB-trBA"/>:
<me>
\mbox{tr} (P^{-1}AP) = \mbox{tr}[P^{-1}(AP)] = \mbox{tr}[(AP)P^{-1}] = \mbox{tr} A.
</me>
As for <xref ref="th-properties-similar-char-poly"/>,
<md>
<mrow> \det(B-\lambda I) \amp = \det \{P^{-1}AP-\lambda(P^{-1}P)\} </mrow>
    <mrow> \amp = \det \{ P^{-1}(A-\lambda I)P\} </mrow>
        <mrow>\amp = \det (A-\lambda I), </mrow>
</md> 

so <m>A</m> and <m>B</m> have the same characteristic equation.
</p>

    <p>
Finally, <xref ref="th-properties-similar-char-poly"/> implies <xref ref="th-properties-similar-eig"/> because the eigenvalues of a matrix are the roots of its characteristic polynomial.
    </p>
</proof>
</theorem>



<remark xml:id="fivePropSim">
<statement> 
<p>
Sharing the five properties in <xref ref="th-properties-similar"/> does not guarantee that two matrices are similar. The matrices
<me>A = \begin{bmatrix}
1 \amp  1 \\
0 \amp  1
\end{bmatrix} \quad \text{and} \quad I = \begin{bmatrix}
1 \amp  0 \\
0 \amp  1
\end{bmatrix}</me> have the same determinant, rank, trace, characteristic polynomial, and eigenvalues, but they are not similar because <m>P^{-1}IP = I</m> for any invertible matrix <m>P</m>.
</p>
</statement>
</remark>

<p> 
Even though the properties in <xref ref="th-properties-similar"/> cannot be used to show two matrices are similar, these properties come in handy for showing that two matrices are NOT similar.
</p>

<p>
With all the abstract business running around, let us turn to a concretete example.  
</p>


<example xml:id="ex-areTheySimilar">
    <statement>
        <p>
            Are the matrices <m>A =
\begin{bmatrix}
2 \amp  1 \\
1 \amp  -1
\end{bmatrix}</m> and <m>B =
\begin{bmatrix}
3 \amp  0 \\
1 \amp  -1
\end{bmatrix}</m> similar?
       </p>
    </statement>
    <answer>
        <p>
            A quick check shows us <m>\det A = \det B</m>, and both matrices are seen to be invertible, so they have the same rank.  However, <m>\mbox{tr} A = 1</m> and <m>\mbox{tr} B = 2</m>, so the matrices are not similar.
       </p>
    </answer>
</example>

<p> 
The next theorem shows that similarity is preserved under inverses, transposes, and powers:
</p>


<theorem xml:id="th-other-properties-similar">

    <statement>
        <p>
            If <m>A</m> and <m>B</m> are <m>n\times n</m> matrices and <m>A\sim B</m>, then
<ol>
<li xml:id="th-properties-similar-inverse">
  <p> <m>A^{-1} \sim B^{-1}</m>, </p>
</li>
<li xml:id="th-properties-similar-transpose">
  <p> <m>A^T \sim B^T</m>, and </p>
</li>
<li xml:id="th-properties-similar-powers">
  <p> <m>A^k \sim B^k</m> for all integers <m>k \geq 1</m>. </p>
</li>
</ol>
        </p>
    </statement>


<proof>
    <p>
See <xref ref="prob-similarproperties"/>.
    </p>
</proof>
</theorem>
































<subsection xml:id="Section-Diagonalizable-Matrices-and-Multiplicity">
    <title>Diagonalizable Matrices and Multiplicity</title>



  
<p>
Recall that a <term>diagonal matrix</term> <m>D</m> is a matrix containing a zero in every entry except those on the main diagonal.  More precisely, if <m>d_{ij}</m> is the <m>ij^{th}</m> entry of a diagonal matrix <m>D</m>, then
<m>d_{ij}=0</m> unless <m>i=j</m>. Such
matrices look like the following.
<me>
D = 
\begin{bmatrix}
* \amp   \amp  0 \\
\amp  \ddots \amp   \\
0 \amp   \amp  *
\end{bmatrix}
</me>
where <m>*</m> is a number which might not be zero.

Diagonal matrices have some nice properties, as we demonstrate below.
</p> 



<exploration xml:id="init-multiplydiag">
    <p>
        Let us warm up with a small computation.
    </p> 

<problem>
<statement>
<p>

Let
<me>
M =\begin{bmatrix}1 \amp  2 \amp  3\\ 4\amp 5\amp 6\\7\amp 8\amp 9\end{bmatrix} \quad \text{and} \quad D =\begin{bmatrix}2 \amp  0 \amp  0\\ 0\amp -5\amp 0\\0\amp 0\amp 10\end{bmatrix}.
</me>

Compute <m>MD</m> and <m>DM</m>.

</p>
</statement>

<answer>
<p>
    <me>
        MD = \begin{bmatrix} 2 \amp  -10 \amp  30\\ 8\amp -25\amp 60\\14\amp -40\amp 90\end{bmatrix}, \quad DM= \begin{bmatrix} 2 \amp  4 \amp  6\\ -20\amp -25\amp -30\\70\amp 80\amp 90\end{bmatrix}.
    </me>
       
</p>
</answer> 
</problem> 

<p> 
Notice the patterns present in the product matrices.  Each row of <m>DM</m> is the same as its corresponding row of <m>M</m> multiplied by the scalar which is the corresponding diagonal element of <m>D</m>.  In the product <m>MD</m>, it is the columns of <m>M</m> that have been multiplied by the diagonal elements. These patterns hold in general for any diagonal matrix, and they are fundamental to understanding diagonalization, the process we discuss below.
</p>
</exploration>



<definition xml:id="def-diagonalizable">

    <statement>
        <p>
            Let <m>A</m> be an <m>n\times n</m> matrix. Then <m>A</m> is said to be <term>diagonalizable</term> if there exists an invertible matrix <m>P</m> such that
<me>
P^{-1}AP=D,
</me>
where <m>D</m> is a diagonal matrix.  In other words, a matrix <m>A</m> is diagonalizable if it is similar to a diagonal matrix, <m>A \sim D</m>.
        </p>
    </statement>
</definition>

<p>
If we are given a matrix <m>A</m> that is diagonalizable, then we can write <m>P^{-1}AP=D</m> for some matrix <m>P</m>, or, equivalently,
<men xml:id="eq-understand-diag">
AP=PD   
</men>
If we pause to examine <xref ref="eq-understand-diag"/>, the work that we did in <xref ref="init-multiplydiag"/> can help us to understand how to find <m>P</m> that will diagonalize <m>A</m>. The product <m>PD</m> is formed by multiplying each column of <m>P</m> by a scalar which is the corresponding element on the diagonal of <m>D</m>.  To restate this, if <m>\mathbf{x}_i</m> is column <m>i</m> in our matrix <m>P</m>, then <xref ref="eq-understand-diag"/> tells us that 
<men xml:id="eq-ev-ew-diag">
A \mathbf{x}_i = \lambda_i \mathbf{x}_i,  
</men>
where <m>\lambda_i</m> is the <m>i</m>th diagonal element of <m>D</m>.  

Of course, <xref ref="eq-ev-ew-diag"/> is very familiar!  We see that if we are able to diagonalize a matrix <m>A</m>, the columns of matrix <m>P</m> will be the eigenvectors of <m>A</m>, and the corresponding diagonal entries of <m>D</m> will be the corresponding eigenvalues of <m>A</m>.  This is summed up in the following theorem.
</p> 




<theorem xml:id="th-eigenvectorsanddiagonalizable">

    <statement>
        <p>
            An <m>n\times n</m> matrix <m>A</m> is diagonalizable if and only if there is an
invertible matrix <m>P</m> given by

<me>
    P=\begin{bmatrix}
| \amp  | \amp    \amp  | \\
\mathbf{x}_1 \amp  \mathbf{x}_2  \amp  \cdots \amp  \mathbf{x}_n \\
| \amp  | \amp    \amp  |
\end{bmatrix},
</me>

where the columns <m>\mathbf{x}_i</m> are eigenvectors of <m>A</m>.

Moreover, if <m>A</m> is diagonalizable, the corresponding eigenvalues of <m>A</m> are the
diagonal entries of the diagonal matrix <m>D</m>.
        </p>
    </statement>


<proof>
    <p>
Suppose <m>P</m> is given as above as an invertible matrix whose columns are eigenvectors of <m>A</m>. To show that <m>A</m> is diagonalizable, we will show 

<me>
    AP=PD,
</me>

which is equivalent to <m>P^{-1}AP=D</m>.  We have

<me>
    AP=\begin{bmatrix}
| \amp  | \amp    \amp  | \\
A\mathbf{x}_1 \amp  A\mathbf{x}_2  \amp  \cdots \amp  A\mathbf{x}_n \\
| \amp  | \amp    \amp  |
\end{bmatrix},
</me>

while
<md>
<mrow> PD \amp =\begin{bmatrix}
| \amp  | \amp    \amp  | \\
\mathbf{x}_1 \amp  \mathbf{x}_2  \amp  \cdots \amp  \mathbf{x}_n \\
| \amp  | \amp    \amp  |
\end{bmatrix} 
\begin{bmatrix}
\lambda _{1} \amp   \amp  0 \\
\amp  \ddots \amp   \\
0 \amp   \amp  \lambda _{n}
\end{bmatrix} </mrow>
<mrow> \amp =\begin{bmatrix}
| \amp  | \amp    \amp  | \\
\lambda _{1}\mathbf{x}_1 \amp  \lambda _{2}\mathbf{x}_2 \amp  \cdots \amp  \lambda_{n}\mathbf{x}_n \\
| \amp  | \amp    \amp  | 
\end{bmatrix}. </mrow> 
</md>
We can complete this half of the proof by comparing columns, and noting that 
<men>
A \mathbf{x}_i = \lambda_i \mathbf{x}_i 
</men>
for <m>i=1,\ldots,n</m> since the <m> \mathbf{x}_i</m> are eigenvectors of <m>A</m> and the <m>\lambda_i</m> are corresponding eigenvalues of <m>A</m>.
</p> 

<p>
Conversely, suppose <m>A</m> is diagonalizable so that <m>P^{-1}AP=D.</m> Let
<me>
P=\begin{bmatrix}
| \amp  | \amp    \amp  | \\
\mathbf{x}_1 \amp  \mathbf{x}_2  \amp  \cdots \amp  \mathbf{x}_n \\
| \amp  | \amp    \amp  |
\end{bmatrix}
</me>
 where the columns are the vectors <m>\mathbf{x}_i</m> and
<me>
D=\begin{bmatrix}
\lambda _{1} \amp   \amp  0 \\
\amp  \ddots \amp   \\
0 \amp   \amp  \lambda _{n}
\end{bmatrix}.
</me>
Then
<me>
AP=PD=\begin{bmatrix}
| \amp  | \amp    \amp  | \\
\mathbf{x}_1 \amp  \mathbf{x}_2  \amp  \cdots \amp  \mathbf{x}_n \\
| \amp  | \amp    \amp  |
\end{bmatrix} \begin{bmatrix}
\lambda _{1} \amp   \amp  0 \\
\amp  \ddots \amp   \\
0 \amp   \amp  \lambda _{n}
\end{bmatrix}
</me>
and so
<me>
\begin{bmatrix}
| \amp  | \amp    \amp  | \\
A\mathbf{x}_1 \amp  A\mathbf{x}_2  \amp  \cdots \amp  A\mathbf{x}_n \\
| \amp  | \amp    \amp  |
\end{bmatrix} =\begin{bmatrix}
| \amp  | \amp    \amp  | \\
\lambda _{1}\mathbf{x}_1 \amp  \lambda _{2}\mathbf{x}_2 \amp  \cdots \amp  \lambda_{n}\mathbf{x}_n \\
| \amp  | \amp    \amp  | 
\end{bmatrix},
</me>
showing the <m>\mathbf{x}_i</m> are eigenvectors of <m>A</m> and the <m>\lambda _{i}</m>
are eigenvalues.
    </p>
</proof>
</theorem>


<p>
Notice that because the matrix <m>P</m> defined above is invertible it follows that the set of eigenvectors of <m>A</m>, <m>\left\{ \mathbf{x}_1 , \mathbf{x}_2 , \ldots, , \mathbf{x}_n  \right\}</m>, is a basis of <m>\mathbb{R}^n</m>.
</p>

<p>
We demonstrate the concept given in the above theorem in the next example. Note that not only
are the columns of the matrix <m>P</m> formed by eigenvectors, but <m>P</m> must
be invertible, and therefore must consist of a linearly independent set of eigenvectors. 
</p> 


<example xml:id="ex-diagonalizematrix">
    <statement>
        <p>
            Let
<me>
A=\begin{bmatrix}
2 \amp  0 \amp  0 \\
1 \amp  4 \amp  -1 \\
-2 \amp  -4 \amp  4
\end{bmatrix}
</me>
 Find an invertible matrix <m>P</m> and a diagonal matrix <m>D</m> such that <m>P^{-1}AP=D</m>.
       </p>
    </statement>
    <answer>
        <p>
            We will use eigenvectors of <m>A</m> as the columns of <m>P</m>, and
the corresponding eigenvalues of <m>A</m> as the diagonal entries of <m>D</m>. The eigenvalues of <m>A</m> are <m>\lambda_1 =2,\lambda_2 = 2</m>, and <m>\lambda_3 = 6</m>.   We leave these computations as exercises, as well as the computations to find a basis for each eigenspace.  

One possible basis for <m>\mathcal{S}_2</m>, the eigenspace corresponding to <m>2</m>, is 
<me>\left \lbrace 
\begin{bmatrix}
-2 \\
1 \\
0
\end{bmatrix},
\begin{bmatrix}
1 \\
0 \\
1
\end{bmatrix}
\right \rbrace,
</me>

while a basis for <m>\mathcal{S}_6</m> is given by 
<me>\left \lbrace \begin{bmatrix}
0 \\
1 \\
-2
\end{bmatrix}\right \rbrace </me>.

We construct the matrix <m>P</m> by using these basis elements as columns.
<me>
P=\begin{bmatrix}
-2 \amp  1 \amp  0 \\
1 \amp  0 \amp  1 \\
0 \amp  1 \amp  -2
\end{bmatrix}
</me>
You can verify (and will do so during exercise) that
<me>
P^{-1}=\begin{bmatrix}
-1/4 \amp  1/2 \amp  1/4 \\
1/2 \amp  1 \amp  1/2 \\
1/4 \amp  1/2 \amp  -1/4
\end{bmatrix}
</me>
Thus,
<md>
<mrow> P^{-1}AP \amp = \begin{bmatrix}
-1/4 \amp  1/2 \amp  1/4 \\
1/2 \amp  1 \amp  1/2 \\
1/4 \amp  1/2 \amp  -1/4
\end{bmatrix} \begin{bmatrix}
2 \amp  0 \amp  0 \\
1 \amp  4 \amp  -1 \\
-2 \amp  -4 \amp  4
\end{bmatrix} \begin{bmatrix}
-2 \amp  1 \amp  0 \\
1 \amp  0 \amp  1 \\
0 \amp  1 \amp  -2
\end{bmatrix} </mrow>
<mrow> \amp =\begin{bmatrix}
2 \amp  0 \amp  0 \\
0 \amp  2 \amp  0 \\
0 \amp  0 \amp  6
\end{bmatrix} </mrow> 
</md>
You can see that the result here is a diagonal matrix where the entries on the main diagonal are the eigenvalues of <m>A</m>. Notice that eigenvalues on the main diagonal <em>must</em> be in the same order as the corresponding eigenvectors in <m>P</m>.
       </p>
    </answer>
</example>

<p> 
It is often easier to work with matrices that are diagonalizable, as the next Exploration demonstrates.  
</p> 


<exploration xml:id="exp-motivate-diagonalization">
    <p>
        Let 
<me>A=\begin{bmatrix}
2 \amp  0 \amp  0 \\
1 \amp  4 \amp  -1 \\
-2 \amp  -4 \amp  4
\end{bmatrix} \quad \text{and} \quad D=\begin{bmatrix}
2 \amp  0 \amp  0 \\
0 \amp  2 \amp  0 \\
0 \amp  0 \amp  6
\end{bmatrix}.</me>  

Would it be easier to compute <m>A^5</m> or <m>D^5</m> if you had to do so by hand, without a computer?  Certainly <m>D^5</m> is easier, due to the number of zero entries!
</p> 

<problem>
<statement>
    <p>
Do compute <m>A^5</m> together with <m>D^5</m>. Feel free to use a program or online calculator tool for <m>A^5 </m>, but do <m> D^5 </m> by hand.
    </p>
</statement>
</problem>

<p> 
We see that raising a diagonal matrix to a power amounts to simply raising each entry to that same power, whereas computing <m>A^5</m> requires many more calculations.  However, we learned in <xref ref="ex-diagonalizematrix"/> that <m>A</m> is similar to <m>D</m>, and we can use this to make our computation easier.  This is because
<md>
    <mrow> A^5\amp =\left(PDP^{-1}\right)^5 </mrow>
        <mrow>   \amp =(PDP^{-1})(PDP^{-1})(PDP^{-1})(PDP^{-1})(PDP^{-1}) </mrow>
            <mrow>  \amp =PD(P^{-1}P)D(P^{-1}P)D(P^{-1}P)D(P^{-1}P)DP^{-1} </mrow>
                <mrow> \amp =PD(I)D(I)D(I)D(I)DP^{-1} </mrow>
                    <mrow> \amp =PD^5P^{-1}. </mrow>
</md>
With this in mind, it is not as daunting to calculate <m>A^5</m> by hand.  We can compute the product <m>PD^5</m> quite easily since <m>D^5</m> is diagonal, as we learned in <xref ref="init-multiplydiag"/>.  That leaves just one product of <m>3 \times 3</m> matrices to compute by hand to compute <m>A^5</m>.  And the savings in work would certainly be more pronounced for larger matrices or for powers larger that 5.

    </p>
</exploration>  

<p>
In <xref ref="exp-motivate-diagonalization"/>, because matrix <m>A</m> was diagonalizable, we were able to cut down on computations.  
When we chose to work with <m>D</m> and <m>P</m> instead of <m>A</m> we worked with the eigenvalues and eigenvectors of <m>A</m>.  
Each column of <m>P</m> is an eigenvector of <m>A</m>, and so we repeatedly made use of the following theorem (with <m>m=5</m>).
</p>

<theorem xml:id="th-eigpowers">

    <statement>
        <p>
            Let <m>A</m> be an <m>n \times n</m> matrix and suppose <m>A\mathbf{x}=\lambda \mathbf{x}</m>.  Then 
            <me>
                A^m \mathbf{x} = \lambda^m \mathbf{x}.
            </me>
        </p>
    </statement>


<proof>
    <p>
We prove this theorem by induction on <m>m</m>.  Clearly <m>A^m \mathbf{x} = \lambda^m \mathbf{x}</m> holds when <m>m=1</m>, as that was given.  For the inductive step, suppose that we know <m>A^{m-1} \mathbf{x} = \lambda^{m-1} \mathbf{x}</m>.  Then
<md>
   <mrow> A^m \mathbf{x} \amp = (A A^{m-1}) \mathbf{x} </mrow>
    <mrow>       \amp = A (A^{m-1} \mathbf{x}) </mrow>
        <mrow>      \amp = A (\lambda^{m-1} \mathbf{x}) </mrow>
            <mrow>  \amp = \lambda^{m-1} A\mathbf{x} </mrow>
                <mrow> \amp = \lambda^{m-1} \lambda \mathbf{x} </mrow>
                    <mrow> \amp = \lambda^m \mathbf{x}. </mrow>
</md>
as desired.
    </p>
</proof>
</theorem>

<p>
Matrix <m>A</m> from the <xref ref="ex-diagonalizematrix"/> and <xref ref="exp-motivate-diagonalization"/> had a repeated eigenvalue of 2.  The next theorem and corollary show that matrices which have <em>distinct</em> eigenvalues (where none are repeated) have desirable properties.
</p> 


<theorem xml:id="th-linindepeigenvectors">

    <statement>
        <p>
            Let <m>A</m> be an <m>n\times n</m> matrix, and suppose that <m>A</m>
has distinct eigenvalues <m>\lambda_1, \lambda_2, \ldots, \lambda_m</m>.
For each <m>i</m>, let <m>\mathbf{x}_i</m> be a <m>\lambda_i</m>-eigenvector of <m>A</m>.
Then <m>\{ \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m\}</m> is
linearly independent.
        </p>
    </statement>


<proof>
    <p>
We prove this by induction on <m>m</m>, the number of vectors in the set. If <m>m = 1</m>, then <m>\{\mathbf{x}_{1}\}</m> is a linearly independent set because <m>\mathbf{x}_{1} \neq \mathbf{0}</m>. In general, suppose we have established that the theorem is true for some <m>m \geq 1</m>. Given eigenvectors <m>\{\mathbf{x}_{1}, \mathbf{x}_{2}, \dots, \mathbf{x}_{m+1}\}</m>, suppose
<men xml:id="eq-thm-proof-5-5-4-1">
c_1\mathbf{x}_1 + c_2\mathbf{x}_2 + \dots + c_{m+1}\mathbf{x}_{m+1} = \mathbf{0}.
</men>
We must show that each <m>c_{i} = 0</m>. Multiply both sides of <xref ref="eq-thm-proof-5-5-4-1"/> on the left by <m>A</m> and use the fact that <m>A\mathbf{x}_{i} = \lambda_{i}\mathbf{x}_{i}</m> to get
<men xml:id="eq-thm-proof-5-5-4-2">
c_1\lambda_1\mathbf{x}_1 + c_2\lambda_2\mathbf{x}_2 + \dots + c_{m+1}\lambda_{m+1}\mathbf{x}_{m+1} = \mathbf{0}.
</men>
If we multiply <xref ref="eq-thm-proof-5-5-4-1"/> by <m>\lambda_{1}</m> and subtract the result from <xref ref="eq-thm-proof-5-5-4-2"/>, the first terms cancel and we obtain
<me>
c_2(\lambda_2 - \lambda_1)\mathbf{x}_2 + c_3(\lambda_3 - \lambda_1)\mathbf{x}_3 + \dots + c_{m+1}(\lambda_{m+1} - \lambda_1)\mathbf{x}_{m+1} = \mathbf{0}.
</me>
Since <m>\mathbf{x}_{2}, \mathbf{x}_{3}, \dots, \mathbf{x}_{m+1}</m>
correspond to distinct eigenvalues <m>\lambda_{2}, \lambda_{3}, \dots, \lambda_{m+1}</m>, the set <m>\{\mathbf{x}_{2}, \mathbf{x}_{3}, \dots, \mathbf{x}_{m+1}\}</m> is linearly independent by the induction hypothesis. Hence,
<me>
c_2(\lambda_2 - \lambda_1) = 0, \quad c_3(\lambda_3 - \lambda_1) = 0, \quad \dots, \quad c_{m+1}(\lambda_{m+1} - \lambda_1) = 0
</me>
and so <m>c_{2} = c_{3} = \dots = c_{m+1} = 0</m> because the <m>\lambda_{i}</m> are distinct. It follows that <xref ref="eq-thm-proof-5-5-4-1"/> becomes <m>c_{1}\mathbf{x}_{1} = \mathbf{0}</m>, which implies that <m>c_{1} = 0</m> because <m>\mathbf{x}_{1} \neq \mathbf{0}</m>, and the proof is complete. 
    </p>
</proof>
</theorem>

<p>
The corollary that follows from this theorem gives a useful tool in determining if <m>A</m> is diagonalizable.
</p>


<corollary xml:id="th-distincteigenvalues">

    <statement>
        <p>
            Let <m>A</m> be an <m>n \times n</m> matrix and suppose it has <m>n</m> distinct eigenvalues. Then it follows that <m>A</m> is diagonalizable.
        </p>
    </statement>
</corollary>


<remark>
<statement>
<p>
    Note that <xref ref="th-distincteigenvalues"/> is NOT an ``if and only if statement".  This means that if <m>A</m> has repeated eigenvalues it is still sometimes possible to diagonalize <m>A</m>, as seen in <xref ref="ex-diagonalizematrix"/>.
</p>
</statement>
</remark> 

<definition xml:id="def-eigdecomposition">

    <statement>
        <p>
            If we are able to diagonalize <m>A</m>, say <m>A=PDP^{-1}</m>, we say that <m>PDP^{-1}</m> is an <term>eigenvalue decomposition</term> of <m>A</m>.
        </p>
    </statement>
</definition>

<p>
Not every matrix has an eigenvalue decomposition. Sometimes we cannot find an invertible matrix <m>P</m> such that <m>P^{-1}AP=D</m>.  Consider the following example.
</p> 


<example xml:id="ex-impossiblediagonalize">
    <statement>
        <p>
            Let
<me>
A =
\begin{bmatrix}
1 \amp  1 \\
0 \amp  1
\end{bmatrix}.
</me>

If possible, find an invertible matrix <m>P</m> and a diagonal matrix <m>D</m> so that <m>P^{-1}AP=D</m>.
       </p>
    </statement>

    <answer>
        <p>
            We see immediately (how?) that the eigenvalues of <m>A</m> are <m>\lambda_1 =1</m> and  <m>\lambda_2=1</m>.
To find <m>P</m>, the next step would be to find a basis for the corresponding eigenspace <m>\mathcal{S}_1</m>.  We solve the equation <m>\left( A - \lambda I \right) \mathbf{x} = \mathbf{0}</m>.
Writing this equation as an augmented matrix, we already have a matrix in row echelon form:
<me>
\left[\begin{array}{cc|c}  
0 \amp  -1 \amp  0 \\
0 \amp  0 \amp  0
 \end{array}\right]
</me>
We see that the eigenvectors in <m>\mathcal{S}_1</m> are of the form

<me>
    
\begin{bmatrix}
t \\
0
\end{bmatrix}
=t\begin{bmatrix}
1 \\
0
\end{bmatrix},

</me>

so a basis for the eigenspace <m>\mathcal{S}_1</m> is given by <m>[1,0]</m>. It is easy to see that we cannot form an invertible matrix <m>P</m>, because any two eigenvectors will be of the form 
<m>[t,0]</m>, and so the second row of <m>P</m> would have a row of zeros, and <m>P</m> could not be invertible.  Hence <m>A</m> cannot be diagonalized.
       </p>
    </answer>
</example>

<p>
We saw earlier in <xref ref="th-distincteigenvalues"/> that an <m>n \times n</m> matrix with <m>n</m> distinct eigenvalues is diagonalizable.  It turns out that there are other useful diagonalizability tests.
</p>

<p> 
Recall that the <term>algebraic multiplicity</term> of an eigenvalue <m>\lambda</m> is the number of times that it occurs as a root of the characteristic polynomial.
</p> 


<definition xml:id="def-geommulteig">

    <statement>
        <p>
            The <term>geometric multiplicity</term> of an eigenvalue <m>\lambda</m> is the dimension of the corresponding eigenspace <m>\mathcal{S}_\lambda</m>.
        </p>
    </statement>
</definition>


<p>
Consider now the following lemma.
</p> 


<lemma xml:id="lemma-dimeigenspace">

    <statement>
        <p>
            Let <m>A</m> be an <m>n\times n</m> matrix, and let <m>\mathcal{S}_{\lambda_1}</m> be the eigenspace corresponding to the eigenvalue <m>\lambda_1</m> which has algebraic multiplicity <m>m</m>.  Then

<me>
    \mbox{dim}(\mathcal{S}_{\lambda_1})\leq m.
</me>


In other words, the geometric multiplicity of an eigenvalue is less than or equal to the algebraic multiplicity of that same eigenvalue.

        </p>
    </statement>


<proof>
    <p>
Let <m>k</m> be the geometric multiplicity of <m>\lambda_1</m>, i.e., <m>k=\mbox{dim}(\mathcal{S}_{\lambda_1})</m>.  Suppose <m>\left\{\mathbf{x}_1, \mathbf{x}_2, \ldots ,\mathbf{x}_k\right\}</m> is a basis for the eigenspace <m>\mathcal{S}_{\lambda_1}</m>.  Let <m>P</m> be any invertible matrix having <m>\mathbf{x}_1,  \mathbf{x}_2, \ldots ,\mathbf{x}_k</m> as its first <m>k</m> columns, say 

<me>
    P=\begin{bmatrix}
| \amp  | \amp   \amp  | \amp  | \amp  \amp  | \\
\mathbf{x}_1 \amp  \mathbf{x}_2 \amp  \cdots \amp  \mathbf{x}_k \amp  \mathbf{x}_{k+1} \amp  \cdots \amp  \mathbf{x}_n \\
| \amp  | \amp   \amp  | \amp  | \amp  \amp  |
\end{bmatrix}.
</me>
  
In block form we may write 

<me>
    P=\begin{bmatrix}
B\amp C
\end{bmatrix} \quad \text{and} \quad P^{-1}=\begin{bmatrix}
D \\
E
\end{bmatrix},
</me>

where <m>B</m> is <m>n \times k</m>, <m>C</m> is  <m>n \times (n-k)</m>, <m>D</m> is <m>k \times n</m>, and <m>E</m> is <m>(n-k) \times n</m>.  We observe
<me>I_n = P^{-1}P = \left[\begin{array}{c|c}  
DB \amp  DC \\ \hline
EB \amp  EC
 \end{array}\right]. </me>.

This implies

<me>
    DB = I_k,\quad DC=O_{k\,\,n-k},\quad EB = O_{n-k\,\,k} \quad\text{ and }\quad EC = I_{n-k}.
</me>

Therefore,
<md>
<mrow> P^{-1}AP  \amp =\begin{bmatrix}
D \\
E
\end{bmatrix} 
A
\begin{bmatrix}
B\amp C
\end{bmatrix} =
\left[\begin{array}{c|c}  
DAB \amp  DAC \\ \hline
EAB \amp  EAC
 \end{array}\right ] </mrow>
 <mrow> \amp =  \left[\begin{array}{c|c}  
\lambda_1 DB \amp  DAC \\ \hline
\lambda_1 EB \amp  EAC
 \end{array}\right]  
  =  \left[\begin{array}{c|c}  
\lambda_1 I_k \amp  DAC \\ \hline
O \amp  EAC
 \end{array}\right].  </mrow>
</md>
We finish the proof by comparing the characteristic polynomials on both sides of this equation, and making use of the fact that similar matrices have the same characteristic polynomials.  

<me>
    \det(A-\lambda I) = \det(P^{-1}AP-\lambda I)=(\lambda_1 - \lambda)^k \det(EAC).
</me>

We see that the characteristic polynomial of <m>A</m> has <m>(\lambda_1 - \lambda)^k</m> as a factor.  
This tells us that algebraic multiplicity of <m>\lambda_1</m> is at least <m>k</m>, proving the desired inequality. 
    </p>
</proof>
</lemma>


<p>
This result tells us that if <m>\lambda</m> is an eigenvalue of <m>A</m>, then
the number of linearly independent <m>\lambda</m>-eigenvectors
is never more than the multiplicity of <m>\lambda</m>. We now use this fact to provide a useful diagonalizability condition.
</p>

<theorem xml:id="th-diagonalizability">

    <statement>
        <p>
            Let <m>A</m> be an <m>n \times n</m> matrix <m>A</m>. Then <m>A</m> is diagonalizable if and only if for each eigenvalue <m>\lambda</m> of <m>A</m>, 
            the algebraic multiplicity of <m>\lambda</m> is equal to the geometric multiplicity of <m>\lambda</m>.
        </p>
    </statement>


<proof>
    <p>
Suppose <m>A</m> is diagonalizable and let <m>\lambda_1, \ldots, \lambda_t</m> be the distinct eigenvalues of <m>A</m>, with algebraic multiplicities <m>m_1, \ldots, m_t</m>, respectively and geometric multiplicities <m>k_1, \ldots, k_t</m>, respectively.  Since <m>A</m> is diagonalizable, <xref ref="th-eigenvectorsanddiagonalizable"/> implies that <m> k_1+\cdots+k_t=n</m>.  By applying <xref ref="lemma-dimeigenspace"/> <m>t</m> times, we have

<me>
    n = k_1+\cdots+k_t \le m_1+\cdots+m_t = n,
</me>

which is only possible if <m>k_i=m_i</m> for <m>i=1,\ldots,t</m>.

Conversely, if the geometric multiplicity equals the algebraic multiplicity of each eigenvalue, then obtaining a basis for each eigenspace yields <m>n</m> eigenvectors.  Applying <xref ref="th-linindepeigenvectors"/>, we know that these <m>n</m> eigenvectors are linearly independent, so <xref ref="th-eigenvectorsanddiagonalizable"/> implies that <m>A</m> is diagonalizable.
    </p>
</proof>
</theorem>
</subsection>























<exercises>

<exercise xml:id="prob-lessthan">
    <statement>
        <p>
            At the beginning of this section we mentioned that similarity of <m>n \times n</m> matrices is an <term>equivalence relation</term>.

An equivalence relation is a binary relation <m>R</m> on elements of a set <m>S</m> that has the following properties:
<ul>
<li>
      <p> The reflexive property:  <m>x R x</m> for every <m>x \in S</m> </p>
</li>
<li>
      <p> The symmetric property:  If <m>x R y</m>, then <m>y R x</m> for every <m>x,y \in S</m> </p>
</li>
<li>
      <p> The transitive property:  If <m>x R y</m> and <m>y R z</m>, then <m>x R z</m> for every <m>x,y,z \in S</m> </p>
</li>
</ul>

Let <m>S</m> be the set of all positive integers.  We can show that the relation ``less than'' (symbolized by <m>\lt </m>) is NOT an equivalence relation on this set.  To see this, note that ``less than'' is not reflexive, because <m>s\lt s</m> is not true for any positive integer <m>s</m>.
<ol>
    <li>
          <p> Is the relation ``less than'' symmetric? </p>
    </li>
    <li>
          <p> Is the relation ``less than'' transitive? </p>
    </li>
    </ol>

</p>
</statement>

<answer>
<p>
The relation "less than" not symmetric, but it is transitive.
</p>
</answer>     
</exercise>



<exercise xml:id="prob-lessthan3">
    <statement>
        <p>
            Another relation between matrices we have studied in this course is that two matrices can be ``row equivalent''.  Is the relation ``row equivalent'' 
<ol>
    <li>
      <p> reflexive? </p>
</li>
    <li>
      <p> symmetric?  </p>
</li>
    <li>
      <p> transitive?  </p>
</li>
</ol>
        </p>
    </statement>

<answer>
    <p>
The relation is reflexive, symmetric and transitive.
    </p>
</answer>
</exercise>

<exercisegroup>
<introduction>
    <p>
 By computing the trace, determinant, and rank, show that <m>A</m> and <m>B</m> are <em>not</em> similar in each case. 
    </p>
</introduction>
   
<exercise xml:id="notsimilar-a">      
<statement>
    <p>
<me>A = \begin{bmatrix}
	1 \amp  2 \\
	2 \amp  1
\end{bmatrix}, \quad B =
\begin{bmatrix}
	1 \amp  1\\
	-1 \amp  1
\end{bmatrix}.</me>
        </p>
    </statement>
</exercise>

<exercise xml:id="notsimilar-b">
    <statement>
        <p>
            <me> A =
\begin{bmatrix}
3 \amp  1 \\
2 \amp  -1
\end{bmatrix}, \quad B =
\begin{bmatrix}
1 \amp  1 \\
2 \amp  1
\end{bmatrix}.</me>
        </p>
    </statement>
</exercise>

<exercise xml:id="notsimilar-c">
    <statement>
        <p>
            <me>A =
\begin{bmatrix}
3 \amp  1 \\
-1 \amp  2
\end{bmatrix}, \quad B =
\begin{bmatrix}
2 \amp  -1 \\
3 \amp  2
\end{bmatrix}</me>
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-notsimilar-d">
    <statement>
        <p>
            <me>A =
\begin{bmatrix}
2 \amp  1 \amp  1 \\
1 \amp  0 \amp  1 \\
1 \amp  1 \amp  0
\end{bmatrix}, \quad B =
\begin{bmatrix}
 1 \amp  -2 \amp  1 \\
-2 \amp  4 \amp  -2 \\
-3 \amp  6 \amp  -3
\end{bmatrix}.</me>
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-notsimilar-e">
    <statement>
        <p>
            <me>A =
\begin{bmatrix}
1 \amp  2 \amp  -3 \\
1 \amp  -1 \amp  2 \\
0 \amp  3 \amp  -5
\end{bmatrix}, \quad B =
\begin{bmatrix}
-2 \amp  1 \amp  3 \\
 6 \amp  -3 \amp  -9 \\
 0 \amp  0 \amp  0
\end{bmatrix}.</me>
        </p>
    </statement>
</exercise>
</exercisegroup>



<exercise xml:id="prob-notsimilar-4x4">
    <statement>
        <p>
            Show that <me>\begin{bmatrix}
1 \amp  2 \amp  -1 \amp   0 \\
2 \amp  0 \amp   1 \amp   1 \\
1 \amp  1 \amp   0 \amp  -1 \\
4 \amp  3 \amp  0 \amp  0
\end{bmatrix} \quad \text{and} \quad 
\begin{bmatrix}
  1 \amp  -1 \amp   3 \amp   0 \\
 -1 \amp   0 \amp   1 \amp   1 \\
  0 \amp  -1 \amp   4 \amp   1 \\
  5 \amp  -1 \amp  -1 \amp  -4
\end{bmatrix}</me> 

are <em>not</em> similar.
        </p>
    </statement>
</exercise>










<exercise xml:id="prob-similarproperties">
    <statement>
        <p>
            Prove  <xref ref="th-other-properties-similar"/>.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-similarproperties-invertible">
    <statement>
        <p>
            If <m>A</m> is invertible, show that <m>AB</m> is similar to <m>BA</m> for all <m>B</m>.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-similarproperties-rI">
    <statement>
        <p>
            Show that the only matrix similar to a scalar matrix <m>A = rI</m>, where <m>r</m> is any number, is <m>A</m> itself.
        </p>
    </statement>
</exercise>

<exercise xml:id="prob-similarproperties-ev">
    <statement>
        <p>
            Let <m>\lambda</m> be an eigenvalue of <m>A</m> with corresponding eigenvector <m>\mathbf{x}</m>. If <m>B = P^{-1}AP</m> is similar to <m>A</m>, show that <m>P^{-1}\mathbf{x}</m> is an eigenvector of <m>B</m> corresponding to <m>\lambda</m>.
        </p>
    </statement>
</exercise>






















<!--Exercises of 2nd combined half-->

<exercisegroup>
    <introduction>
        <p>
    In this exercise you will "fill in the details" of <xref ref="ex-diagonalizematrix"/>.
        </p>
    </introduction>
    
    
    <exercise xml:id="prob-ex-diagonalizematrix1">
        <statement>
            <p>
             
    Find the eigenvalues of matrix 
    <me>
    A=\begin{bmatrix}
    2 \amp  0 \amp  0 \\
    1 \amp  4 \amp  -1 \\
    -2 \amp  -4 \amp  4
    \end{bmatrix}
    </me>
            </p>
        </statement>
    </exercise> 
    
    <exercise xml:id="prob-ex-diagonalizematrix2">
        <statement>
            <p>
                Find a basis for each eigenspace of the matrix <m>A</m>.
            </p>
        </statement>
    </exercise>
    
    <exercise xml:id="prob-ex-diagonalizematrix3">
        <statement>
            <p>
                Compute the inverse of             
    <me>
    P=
    \begin{bmatrix}
    -2 \amp  1 \amp  0 \\
    1 \amp  0 \amp  1 \\
    0 \amp  1 \amp  -2
    \end{bmatrix}
    </me>
            </p>
        </statement>
    </exercise>
    
    
    <exercise xml:id="prob-ex-diagonalizematrix5">
        <statement>
            <p>
                Compute <m>D=P^{-1}AP</m>.
            </p>
        </statement>
    </exercise>
    
    
    <exercise xml:id="prob-ex-diagonalizematrix4">
        <statement>
            <p>
                Show that computing the inverse of <m>P</m> is not really necessary by comparing the products  <m>PD</m> and <m>AP</m>.
            </p>
        </statement>
    </exercise>
    </exercisegroup>
    
    
    <exercise xml:id="prb-diagonalizable">
        <statement>
            <p>
                In each case, decide whether the matrix <m>A</m> is diagonalizable. If so, find <m>P</m> such that <m>P^{-1}AP</m> is diagonal.
    
    <ol>
    
    <li>
          <p> 
            <me>\begin{bmatrix}
    1 \amp  0 \amp  0 \\
    1 \amp  2 \amp  1 \\
    0 \amp  0 \amp  1
    \end{bmatrix},</me>
          </p>
    </li>
    <li>
          <p> <me>\begin{bmatrix}
    3 \amp   0 \amp  6 \\
    0 \amp  -3 \amp  0 \\
    5 \amp   0 \amp  2 
    \end{bmatrix},</me>
    </p>
    </li>
    <li>
          <p> <me>\begin{bmatrix}
     3 \amp   1 \amp   6 \\
     2 \amp   1 \amp   0 \\
    -1 \amp   0 \amp  -3 
    \end{bmatrix},</me>
    </p>
    </li>
    <li>
          <p> <me>\begin{bmatrix}
    4 \amp  0 \amp  0 \\
    0 \amp  2 \amp  2 \\
    2 \amp  3 \amp  1 
    \end{bmatrix}.</me>
    </p>
    </li>
    
    </ol>
            </p>
        </statement>
    
    <answer>
        <p>
    <ol>
        <li>
            <p>
        Diagonalizable.  
            </p>
        </li>
    
        <li>
            <p>
        Diagonalizable.  
            </p>
        </li>
    
        <li>
            <p>
        Diagonalizable.   
            </p>
        </li>
    
        <li>
            <p>
        Not diagonalizable.  
            </p>
        </li>
    </ol>
        </p>
    </answer>
    </exercise>
    
    
    
    <exercise xml:id="prb-upper-triangular-case">
        <statement>
            <p>
                Let <m>A</m> denote an <m>n \times n</m> upper triangular matrix.
    <ol>
    
    <li>
          <p> If all the main diagonal entries of <m>A</m> are distinct, show that <m>A</m> is diagonalizable. </p>
    </li>
    
    <li>
          <p> If all the main diagonal entries of <m>A</m> are equal, show that <m>A</m> is diagonalizable only if it is <em>already</em> diagonal.</p>
    </li>
    
    
    <li>
          <p> Show that <me>\begin{bmatrix}
    1 \amp  0 \amp  1 \\
    0 \amp  1 \amp  0 \\
    0 \amp  0 \amp  2 
    \end{bmatrix}</me>
    
    is diagonalizable while <me>\begin{bmatrix}
     1 \amp  1 \amp  0 \\
     0 \amp  1 \amp  0 \\
     0 \amp  0 \amp  2
     \end{bmatrix}</me>
     
     is not diagonalizable.
    </p>
    </li>
    </ol>
            </p>
        </statement>
    
    <answer>
        <p>
    For part (b): The eigenvalues of <m>A</m> are all equal (they are the diagonal elements), so if <m>P^{-1}AP = D</m> is diagonal, then <m>D = \lambda I</m>. Hence <m>A = P^{-1}(\lambda I)P = \lambda I</m>.
        </p>
    </answer>
    </exercise>
    
    <exercise xml:id="prb-det-and-tr-diagonalizable">
        <statement>
            <p>
                Let <m>A</m> be a diagonalizable <m>n \times n</m> matrix with eigenvalues <m>\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}</m> (including multiplicities). Show that:
    
    <ol>
    <li>
          <p> <m>\det A = \lambda_{1}\lambda_{2}\cdots \lambda_{n}</m> </p>
    </li>
    <li>
          <p> <m>\mbox{tr} A = \lambda_{1} + \lambda_{2} + \cdots + \lambda_{n}</m> </p>
    </li>
    </ol>
            </p>
        </statement>
    </exercise>
    
    <exercise xml:id="prb-A-sim-A-T-diagonalizable">
        <statement>
            <p>
                <ol>
    
    <li>
          <p> Show that two diagonalizable matrices are similar if and only if they have the same eigenvalues with the same multiplicities. </p>
    </li>
    
    <li>
          <p> If <m>A</m> is diagonalizable, show that <m>A \sim A^{T}</m>. </p>
    </li>
    
    <li>
          <p> Show that <m>A \sim A^{T}</m> if
     <me>A = \begin{bmatrix}
     1 \amp  1 \\
     0 \amp  1
     \end{bmatrix}</me>
    </p>
    </li>
    </ol>
            </p>
        </statement>
    </exercise>
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
</exercises>

</section>